{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836b26ce",
   "metadata": {},
   "source": [
    "# conda 25.5.1\n",
    "# Python 3.10.18\n",
    "# torch 2.2.2（pytorch）\n",
    "# pip install transformers==4.38.2\n",
    "# pip install accelerate==0.30.0\n",
    "# pip install peft==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a7ba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 103,248,384 || trainable%: 1.1425341049405675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lichao/miniconda3/envs/LLMs/lib/python3.10/site-packages/peft/tuners/lora/model.py:347: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 我们选择一个在中文语料上预训练过的GPT-2模型\n",
    "model_name = \"uer/gpt2-chinese-cluecorpussmall\" \n",
    "\n",
    "# AutoTokenizer 会自动加载速度更快的 FastTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # 低秩维度，越小越轻量\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"mlp.c_fc\", \"mlp.c_proj\"],  # GPT-2 的注意力层\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# 使用 PEFT 库将 LoRA 配置应用到模型上\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48e2d97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测到MPS可用，将使用MPS进行训练。\n"
     ]
    }
   ],
   "source": [
    "#查看你目前机子所支持的GPU类型\n",
    "import torch\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"检测到MPS可用，将使用MPS进行训练。\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"检测到CUDA可用，将使用GPU进行训练。\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"未检测到MPS，将使用CPU进行训练。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb8033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 461\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 定义每个文本块的大小\n",
    "BLOCK_SIZE = 128\n",
    "\n",
    "# 训练数据集文件路径\n",
    "TRAIN_FILE=\"cleaned_huagaiji.txt\"\n",
    "\n",
    "# 加载数据集\n",
    "# 假设你的数据集是一个文本文件，每行是一个样本\n",
    "raw_datasets = load_dataset('text', data_files={'train': TRAIN_FILE})\n",
    "\n",
    "# 定义一个函数来对文本进行分词\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples[\"text\"]) \n",
    "\n",
    "# 对数据集进行分词\n",
    "# 使用 batched=True 以批处理的方式进行分词，这样可以提高效率\n",
    "# remove_columns=[\"text\"] 用于移除原始文本列，只保留分词的结果\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# 定义一个函数来将文本分块\n",
    "# 这里的 BLOCK_SIZE 是每个块的大小\n",
    "# 例如，如果 BLOCK_SIZE=128，那么每个块将包含 128 个 token\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()} \n",
    "    #examples{\n",
    "    # 'input_ids':[[101, 5678, ...], [102, 1314...],...],\n",
    "    # 'attention_mask':[[1,1,...], [1,1...]...],\n",
    "    # ...\n",
    "    #}\n",
    "    #concatenated_examples {\n",
    "    # 'input_ids':[101, 5678, ...,102, 1314...]\n",
    "    # 'attention_mask':[1,1....1,1...]\n",
    "    #}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]]) \n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE \n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)] \n",
    "        for k, t in concatenated_examples.items() \n",
    "    }\n",
    "    # result {\n",
    "    #   'input_ids':[[...BLOCK_SIZE个ID...], [...BLOCK_SIZE个ID...] ...],\n",
    "    #   'attention_mask':[[...BLOCK_SIZE个1...], [...BLOCK_SIZE个1...] ...],\n",
    "    #   ...\n",
    "    #}\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy() \n",
    "    # result {\n",
    "    #   'input_ids':[[...BLOCK_SIZE个ID...], [...BLOCK_SIZE个ID...] ...],\n",
    "    #   'attention_mask':[[...BLOCK_SIZE个1...], [...BLOCK_SIZE个1...] ...],\n",
    "    #   ...\n",
    "    #   'labels':[[...BLOCK_SIZE个ID...], [...BLOCK_SIZE个ID...] ...]\n",
    "    #}\n",
    "    return result\n",
    "\n",
    "# 对分词后的数据集进行分块\n",
    "# 使用 batched=True 以批处理的方式进行分块，这样可以提高效率\n",
    "# batch_size=1000 用于指定每次处理的样本数量\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "    group_texts, batched=True, batch_size=1000\n",
    ")\n",
    "\n",
    "print(lm_datasets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e32a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"./gpt2-luxun-finetuned-mps\"\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "SAVE_STEPS = 500\n",
    "OVERWRITE_OUTPUT_DIR = True\n",
    "\n",
    "# 设置训练参数,这些参数可以根据你的需求进行调整\n",
    "# 例如，num_train_epochs 可以设置为你希望的训练轮数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    learning_rate=1e-5, #LEARNING_RATE,\n",
    "    logging_steps=50,  # 每50步记录一次日志\n",
    "    evaluation_strategy=\"steps\",  # 每500步评估一次\n",
    "    eval_steps=500,  # 每500步评估一次\n",
    "    save_total_limit=2,  # 最多保存2个checkpoint\n",
    "    warmup_ratio=0.03,  # 预热比例\n",
    "    use_mps_device=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8146ca1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lichao/miniconda3/envs/LLMs/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始训练 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a438a3e50ac74be29b861d0d9d4c008c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/348 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.456, 'grad_norm': 1.3484209775924683, 'learning_rate': 8.842729970326411e-06, 'epoch': 0.43}\n",
      "{'loss': 4.3656, 'grad_norm': 1.470889925956726, 'learning_rate': 7.359050445103858e-06, 'epoch': 0.86}\n",
      "{'loss': 4.2514, 'grad_norm': 1.7284166812896729, 'learning_rate': 5.875370919881306e-06, 'epoch': 1.29}\n",
      "{'loss': 4.146, 'grad_norm': 1.8330525159835815, 'learning_rate': 4.391691394658754e-06, 'epoch': 1.72}\n",
      "{'loss': 4.1199, 'grad_norm': 1.7699863910675049, 'learning_rate': 2.9080118694362018e-06, 'epoch': 2.16}\n",
      "{'loss': 4.0692, 'grad_norm': 1.455802321434021, 'learning_rate': 1.42433234421365e-06, 'epoch': 2.59}\n",
      "{'train_runtime': 181.6624, 'train_samples_per_second': 7.613, 'train_steps_per_second': 1.916, 'train_loss': 4.20465280817843, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=348, training_loss=4.20465280817843, metrics={'train_runtime': 181.6624, 'train_samples_per_second': 7.613, 'train_steps_per_second': 1.916, 'train_loss': 4.20465280817843, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer,DataCollatorForLanguageModeling\n",
    "\n",
    "# 使用 DataCollatorForLanguageModeling 来处理数据\n",
    "# 在构建每个 batch 时自动进行动态填充（padding）、掩码生成（masking）、以及标签对齐。\n",
    "# 这里的 mlm=False 表示我们不使用掩码语言模型（Masked Language Modeling），因为我们是在做自回归语言模型（Causal Language Modeling）\n",
    "# 如果你使用的是 BERT 或其他需要掩码的模型，可以将 mlm 设置为 True\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# 创建 Trainer 实例\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_datasets[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(f\"--- 开始训练 ---\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67434b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 训练完成，保存最终模型到 ./gpt2-luxun-finetuned-mps ---\n",
      "--- 脚本执行完毕 ---\n"
     ]
    }
   ],
   "source": [
    "# 8. 保存最终模型 (与之前完全相同)\n",
    "print(f\"--- 训练完成，保存最终模型到 {OUTPUT_DIR} ---\")\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(\"--- 脚本执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1221fa8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在使用设备: MPS (Apple Silicon GPU)\n",
      "正在加载分词器...\n",
      "正在加载训练好的模型...\n",
      "模型加载并移动到设备完成！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_PATH = \"./gpt2-luxun-finetuned-mps\" \n",
    "\n",
    "# 自动检测并设置设备 (对于您的Mac，这将优先选择MPS)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(f\"正在使用设备: MPS (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(f\"正在使用设备: CPU\")\n",
    "\n",
    "print(\"正在加载分词器...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"正在加载训练好的模型...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "\n",
    "model.to(device)\n",
    "print(\"模型加载并移动到设备完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "905101c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在为以下prompt生成文本 ---\n",
      "'秋天的后半夜，月亮下去了，太阳还没有出，只剩下一片乌蓝的天空。'\n",
      "\n",
      "--- 生成结果 ---\n",
      "秋 天 的 后 半 夜 ， 月 亮 下 去 了 ， 太 阳 还 没 有 出 ， 只 剩 下 一 片 乌 蓝 的 天 空 。 书 上 写 着 ： 从 前 看 过 ， 今 日 之 故 人 也 ！ 那 是 我 们 这 个 世 界 最 大 、 最 古 老 和 最 残 酷 的 朝 代 时 期 但 它 却 在 今 年 来 临 ， 终 于 要 离 开 了.. 又 何 妨 呢 ？ - - - - 《 中 国 文 史 》 里 面 的 语 句 ， 不 知 道 你 听 懂 多 少 ？ 我 先 问 清 楚 ， 如 果 说 得 通 透 点 儿 就 好 了 ！ 这 样 子 算 么 ？ 还 真 难 讲 了 ， 作 为 历 史 家 ， 咱 要 想 办 法 让 读 者 明 白 该 怎 么 回 答 或 者 应 用 哪 种 方 式 ， 而 且 还 必 须 记 住 这 两 点 ：\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt_text, max_len=150):\n",
    "    print(f\"\\n--- 正在为以下prompt生成文本 ---\\n'{prompt_text}'\")\n",
    "    \n",
    "    # 编码：将您的起始句子转换为模型可理解的ID，并移动到设备上\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # 生成：调用模型的 .generate() 方法\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=200,\n",
    "        num_return_sequences=1,  # 生成几个不同的结果\n",
    "        do_sample=True,          # 开启采样，让文本更具创造性\n",
    "        top_k=50,                # 从概率最高的50个词中选择\n",
    "        top_p=0.95,              # 从概率总和为95%的词汇中选择\n",
    "        temperature=0.8,         # 控制生成的随机性，值越小越保守\n",
    "        repetition_penalty=1.5\n",
    "    )\n",
    "\n",
    "    # 解码：将生成的ID转换回人类可读的文本\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n--- 生成结果 ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "prompt1 = \"秋天的后半夜，月亮下去了，太阳还没有出，只剩下一片乌蓝的天空。\"\n",
    "generate_text(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b42eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# list=[1,2,3,4, 5]\n",
    "# print(*list)\n",
    "\n",
    "dict = {\n",
    "    \"key1\":[1,1,1,1,1],\n",
    "    \"key2\":[2,2,2,2,2]\n",
    "}\n",
    "\n",
    "def process_data(key1, key2):\n",
    "    print(key1)\n",
    "    print(key2)\n",
    "\n",
    "process_data(**dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
