{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cddb19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "最终回答：\n",
      "WebRTC（Web Real-Time Communication）三部曲通常指的是实现一个基本的点对点通信应用所需要的三个关键步骤或技术组件。这三个步骤分别是：\n",
      "\n",
      "1. **信令(Signaling)**：这是建立连接前的第一步，主要目的是让两个客户端能够交换必要的信息以开始直接通信。这些信息包括但不限于网络地址、端口等。信令本身并不属于WebRTC标准的一部分，而是需要开发者自行选择合适的传输方式来实现，比如使用WebSocket、HTTP请求或者其他任何可以用来传递消息的方法。\n",
      "\n",
      "2. **媒体协商(Media Negotiation)**：一旦通过信令通道建立了初步联系，接下来就需要进行媒体协商了。这一步骤涉及到双方就将要使用的音频/视频编解码器类型、分辨率以及其他相关参数达成一致。在WebRTC中，这个过程是通过SDP（Session Description Protocol）来完成的，它允许参与者描述他们的多媒体会话能力，并且最终确定共同支持的最佳配置。\n",
      "\n",
      "3. **ICE/STUN/TURN (NAT Traversal)**：最后一个阶段是为了确保即使在网络地址转换(NAT)环境下也能成功建立连接。这里涉及到的技术有：\n",
      "   - **ICE (Interactive Connectivity Establishment)**: 一种框架，用于发现和测试候选者之间的连通性。\n",
      "   - **STUN (Session Traversal Utilities for NAT)**: 协议帮助设备获取其公网IP地址及端口号，从而绕过简单的NAT。\n",
      "   - **TURN (Traversal Using Relays around NAT)**: 当直接P2P连接不可行时，作为备用方案提供中继服务，使得数据可以通过第三方服务器转发给对方。\n",
      "\n",
      "这三个步骤共同作用，使得WebRTC能够在各种复杂的网络环境中实现高质量的实时音视频交流。\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# 配置阿里云 Qwen 的 Key\n",
    "DASHSCOPE_API_KEY = \"sk-你自己的Key\"\n",
    "\n",
    "# --- Step 1: 初始化 OpenAI 客户端 (连接 Qwen) ---\n",
    "# 这就是你想要的：用 OpenAI 原生客户端\n",
    "client = OpenAI(\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "prompt = \"webrtc三部曲是？\"\n",
    "\n",
    "# 原生 ChatCompletion 调用\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen-max\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个乐于助人的AI专家。\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"-\" * 30)\n",
    "print(\"最终回答：\")\n",
    "print(answer)\n",
    "print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb86f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/dk4btz8500gfxc94n2218h140000gn/T/ipykernel_13372/3837800524.py:70: DeprecationWarning: The class QwenEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=QwenEmbeddingFunction()\n",
      "/var/folders/wc/dk4btz8500gfxc94n2218h140000gn/T/ipykernel_13372/3837800524.py:144: DeprecationWarning: The class QwenEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  collection = chroma_client.get_or_create_collection(\"markdown_knowledge_base\", embedding_function=QwenEmbeddingFunction())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发现 102 个Markdown文件，准备处理...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files: 100%|██████████| 102/102 [01:57<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "全部处理完成！数据已存入 ./chroma_pure_db\n",
      "当前数据库总数: 1484\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import uuid\n",
    "import time # 引入时间库，用于失败重试等待\n",
    "from tqdm import tqdm\n",
    "import chromadb\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "# ================= 配置区域 =================\n",
    "DOCS_DIR = \"./my_docs\"\n",
    "CHROMA_PATH = \"./chroma_pure_db\"\n",
    "\n",
    "# *** 修正点 1: 降低入库批次大小，适应阿里云限制 ***\n",
    "BATCH_SIZE = 10 \n",
    "\n",
    "# ================= 1. 初始化 =================\n",
    "client = OpenAI(\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "# *** Embedding 适配器 ***\n",
    "class QwenEmbeddingFunction(chromadb.EmbeddingFunction):\n",
    "    def __init__(self, api_key):\n",
    "        super.__init__()\n",
    "\n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        # 阿里云限制每次最多 10-25 条，我们保守一点，设为 10\n",
    "        # 这样无论外面传进来多少，这里都会切成小块安全发送\n",
    "        safe_batch_size = 10\n",
    "        all_embeddings = []\n",
    "        \n",
    "        # 将 input 列表切片处理\n",
    "        for i in range(0, len(input), safe_batch_size):\n",
    "            batch = input[i : i + safe_batch_size]\n",
    "            \n",
    "            # 简单的重试机制 (防止网络抖动)\n",
    "            max_retries = 3\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    # 必须把换行符去掉，这也是embedding的一个常见坑，防止语义干扰\n",
    "                    clean_batch = [text.replace(\"\\n\", \" \") for text in batch]\n",
    "                    \n",
    "                    response = client.embeddings.create(\n",
    "                        model=\"text-embedding-v3\",\n",
    "                        input=clean_batch,\n",
    "                        encoding_format=\"float\"\n",
    "                    )\n",
    "                    # 收集结果\n",
    "                    batch_embeddings = [data.embedding for data in response.data]\n",
    "                    \n",
    "                    #append是将参数作为一个整体添加到list中，输入【hello】[\"world\", \"hello\"]\n",
    "                    #extend是将参数看作是一组数据，把里边的每一项添加到list中【hello】['a','b','h','e',...]\n",
    "                    all_embeddings.extend(batch_embeddings)\n",
    "                    break # 成功则跳出重试循环\n",
    "                except Exception as e:\n",
    "                    if attempt < max_retries - 1:\n",
    "                        time.sleep(1) # 等1秒再试\n",
    "                        continue\n",
    "                    else:\n",
    "                        # 如果3次都失败，打印严重错误，但不要让整个程序崩溃\n",
    "                        # 注意：这里如果返回空，Chroma 还是会报错，但至少你能看到日志\n",
    "                        print(f\"\\n[Error] Embedding API Failed after retries: {e}\")\n",
    "                        # 填充零向量占位，防止程序崩溃 (可选，或者直接抛出异常)\n",
    "                        # 这里我们选择抛出异常让上层知道\n",
    "                        raise e\n",
    "\n",
    "        return all_embeddings\n",
    "\n",
    "# 初始化数据库\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\n",
    "collection = chroma_client.get_or_create_collection( #创建一个表\n",
    "    name=\"markdown_knowledge_base\",\n",
    "    embedding_function=QwenEmbeddingFunction()\n",
    ")\n",
    "\n",
    "# ================= 2. 核心处理逻辑 (保持不变，微调参数) =================\n",
    "\n",
    "def process_and_index_files():\n",
    "    md_files = glob.glob(os.path.join(DOCS_DIR, \"*.md\"))\n",
    "    print(f\"发现 {len(md_files)} 个Markdown文件，准备处理...\")\n",
    "\n",
    "    # 这是一个映射列表。它告诉程序：“当你看到 # 时，把它当作一级标题（H1）；看到 ## 时，当作二级标题（H2）...”。\n",
    "    # 目的：不仅仅是切分，还会将标题内容作为**元数据（Metadata）**保留下来。\n",
    "    #   比如，切分出的片段会带有一个标签 {'H1': '用户手册', 'H2': '安装指南'}，这样模型检索时就知道这段文字属于哪个章节。\n",
    "    headers_to_split_on = [(\"#\", \"H1\"), (\"##\", \"H2\"), (\"###\", \"H3\")]\n",
    "    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    \n",
    "    # 作用：它会按照上面定义的标题层级，将 Markdown 文本“物理”切断。\n",
    "    # 结果：原本的一个长文档，会根据标题被拆分成多个逻辑段落。\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "\n",
    "    batch_documents = []\n",
    "    batch_metadatas = []\n",
    "    batch_ids = []\n",
    "\n",
    "    for file_path in tqdm(md_files, desc=\"Processing Files\"):\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            filename = os.path.basename(file_path)\n",
    "            header_splits = md_splitter.split_text(content)\n",
    "            final_splits = text_splitter.split_documents(header_splits)\n",
    "\n",
    "            for split in final_splits:\n",
    "                meta = split.metadata.copy()\n",
    "                meta[\"source\"] = filename \n",
    "                \n",
    "                # 过滤掉空内容\n",
    "                if not split.page_content.strip():\n",
    "                    continue\n",
    "\n",
    "                batch_documents.append(split.page_content)\n",
    "                batch_metadatas.append(meta)\n",
    "                batch_ids.append(f\"{filename}_{uuid.uuid4().hex[:8]}\")\n",
    "\n",
    "                # 提交判断\n",
    "                if len(batch_documents) >= BATCH_SIZE:\n",
    "                    collection.upsert(\n",
    "                        documents=batch_documents,\n",
    "                        metadatas=batch_metadatas,\n",
    "                        ids=batch_ids\n",
    "                    )\n",
    "                    batch_documents = []\n",
    "                    batch_metadatas = []\n",
    "                    batch_ids = []\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n跳过文件 {file_path}: {e}\")\n",
    "\n",
    "    # 处理尾部数据\n",
    "    if batch_documents:\n",
    "        collection.upsert(\n",
    "            documents=batch_documents,\n",
    "            metadatas=batch_metadatas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n全部处理完成！数据已存入 {CHROMA_PATH}\")\n",
    "    print(f\"当前数据库总数: {collection.count()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(DOCS_DIR):\n",
    "        os.makedirs(DOCS_DIR)\n",
    "        print(f\"请将你的 Markdown 文件放入 {DOCS_DIR} 文件夹中。\")\n",
    "    else:\n",
    "        # 清理旧数据 (可选：如果你想每次重新跑都清空库的话，取消注释下面这行)\n",
    "        chroma_client.delete_collection(\"markdown_knowledge_base\")\n",
    "        collection = chroma_client.get_or_create_collection(\"markdown_knowledge_base\", embedding_function=QwenEmbeddingFunction())\n",
    "        \n",
    "        process_and_index_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcb7266c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据库共有 1484 条数据。\n",
      "数据样本: {'ids': ['OpenCV课程资料.md_6fde3ea4'], 'embeddings': array([[-0.05851695, -0.01614801, -0.06831551, ..., -0.0196167 ,\n",
      "         0.05310815, -0.01203262]]), 'documents': ['---\\ntitle: OpenCV课程资料\\ntags:\\n- OpenCV\\n- 人工智能\\n- AI\\ncategories: AI\\nabbrlink: 65de593f\\ndate: 2020-05-31 00:19:12\\n---  \\n这里是我OpenCV课程的相关资料，后面还会不断补充...\\nAI\\n<!-- more -->'], 'uris': None, 'included': ['metadatas', 'documents', 'embeddings'], 'data': None, 'metadatas': [{'source': 'OpenCV课程资料.md'}]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_pure_db\")\n",
    "collection = chroma_client.get_collection(\"markdown_knowledge_base\") # 注意：这里不需要传 embedding_function，因为只读不写\n",
    "\n",
    "print(f\"当前数据库共有 {collection.count()} 条数据。\")\n",
    "\n",
    "# 随便拿一条来看看元数据对不对\n",
    "peek = collection.peek(1)\n",
    "print(\"数据样本:\", peek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ec8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/dk4btz8500gfxc94n2218h140000gn/T/ipykernel_13372/3532827569.py:9: DeprecationWarning: The class QwenEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=QwenEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "检索到的背景信息: **WebRTC三部曲**，这个计划渐渐的在我的脑海中浮现出来，于是2017年我出来创业了。  \n",
      "WebRTC三部曲的第一部是[《WebRTC入门与实战》](https://coding.imooc.com/class/329.html)，这门课从WebRTC的应用角度来讲，主要讲WebRTC都能做什么，该怎么使用它，包括各个终端的互联互通。这门课已于2019年上线；第二部[《WebRTC流媒体服器》](https://coding.imooc.com/class/387.html)讲的是如何设计、实现一个可以高负载、大并发，并且能与WebRTC（浏览器）互通的流媒体服务器，这门课同样于2019年上线；第三部就是刚刚更新完的[《WebRTC源码深入剖析》](https://coding.imooc.com/class/532.html)。\n",
      "\n",
      "正在请求 Qwen 模型...\n",
      "------------------------------\n",
      "最终回答：\n",
      "WebRTC三部曲是一系列关于WebRTC技术的学习课程，旨在帮助开发者从入门到深入理解WebRTC。这三部曲包括：\n",
      "\n",
      "1. **《WebRTC入门与实战》**：此课程专注于WebRTC的应用层面，介绍WebRTC能够实现的功能以及如何使用它来开发应用，同时涵盖了不同终端之间的互联互通问题。该课程于2019年上线。\n",
      "\n",
      "2. **《WebRTC流媒体服务器》**：本课程侧重于教授如何设计和构建一个高性能、高并发的流媒体服务器，并确保该服务器能够与基于WebRTC技术的浏览器端良好地协同工作。同样地，这门课也在2019年推出。\n",
      "\n",
      "3. **《WebRTC源码深入剖析》**：作为三部曲的最后一部分，这门课程提供了对WebRTC源代码的深度解析，适合那些希望深入了解WebRTC内部工作机制的技术爱好者或专业人士。这门课程是最新更新完成的。\n",
      "\n",
      "通过这三个阶段的学习，学习者可以从基础概念逐步过渡到高级实践乃至底层原理的理解，全面掌握WebRTC相关知识和技术。\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_pure_db\")\n",
    "collection = chroma_client.get_collection(\n",
    "    name=\"markdown_knowledge_base\",\n",
    "    embedding_function=QwenEmbeddingFunction() \n",
    ")\n",
    "\n",
    "# --- Step 4: 检索 (Retrieval) ---\n",
    "user_query = \"webrtc三部曲是什么？\"\n",
    "\n",
    "# 执行相似度检索\n",
    "# 它找出距离最近（最相似）的 n 个文档返回给你。\n",
    "results = collection.query(\n",
    "    query_texts=[user_query],\n",
    "    n_results=1\n",
    ")\n",
    "\n",
    "# 提取检索到的文本 (Chroma 返回的是个复杂的字典)\n",
    "retrieved_context = results['documents'][0][0]\n",
    "print(f\"\\n检索到的背景信息: {retrieved_context}\")\n",
    "\n",
    "# --- Step 5: 生成 (Generation) ---\n",
    "# 手写 Prompt，没有任何黑盒\n",
    "prompt = f\"\"\"\n",
    "你是一个助手。请根据下面的【参考信息】回答用户问题。\n",
    "\n",
    "【参考信息】：\n",
    "{retrieved_context}\n",
    "\n",
    "【用户问题】：\n",
    "{user_query}\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n正在请求 Qwen 模型...\")\n",
    "\n",
    "# 原生 ChatCompletion 调用\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen-max\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"你是一个乐于助人的AI专家。\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "answer = response.choices[0].message.content\n",
    "print(\"-\" * 30)\n",
    "print(\"最终回答：\")\n",
    "print(answer)\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
