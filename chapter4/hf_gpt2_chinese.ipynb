{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e56f0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda 25.5.1\n",
    "# python 3.10.10\n",
    "# torch 2.2.2\n",
    "# transformers 4.38.2\n",
    "# accelerate 0.30.0\n",
    "# peft 0.10.0 (‰ΩøÁî®ÊóßÁâàÊú¨Ôºå‰∏é transformers 4.38.2 ÂÖºÂÆπ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca462579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaronghe/Desktop/projects/light-weight-private-llm/chapter4/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/jiaronghe/Desktop/projects/light-weight-private-llm/chapter4/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:949: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,179,648 || all params: 103,248,384 || trainable%: 1.1425341049405675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaronghe/Desktop/projects/light-weight-private-llm/chapter4/.venv/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# select a model\n",
    "BASE_MODEL_NAME = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "# AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "# AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,    # lower rank for less memory usage\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        \"c_attn\", \"c_proj\",\n",
    "        \"mlp.c_fc\", \"mlp.c_proj\",\n",
    "    ],  # Âè™ÂØπÊ≥®ÊÑèÂäõÂ±ÇÂíå MLP Â±ÇËøõË°å LoRA ÂæÆË∞É\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# use peft to configure LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1034c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# check device\n",
    "import torch\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a807389a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 1139 examples [00:00, 231548.67 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1139/1139 [00:00<00:00, 30326.36 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1139/1139 [00:00<00:00, 3902.13 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 461\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "BLOCK_SIZE = 128\n",
    "TRAIN_FILE = \"cleaned_huagaiji.txt\"\n",
    "\n",
    "# load raw dataset\n",
    "raw_dataset = load_dataset(\"text\", data_files={\"train\": TRAIN_FILE})\n",
    "\n",
    "# define function to tokenize the dataset\n",
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples[\"text\"])\n",
    "\n",
    "# tokenize the dataset\n",
    "# batched=True ‰ª•ÊâπÂ§ÑÁêÜÊñπÂºèËøõË°åÂàÜËØçÔºåÂä†Âø´Â§ÑÁêÜÈÄüÂ∫¶\n",
    "# remove_columns=[\"text\"] ÁßªÈô§ÂéüÂßãÊñáÊú¨Âàó, Âè™‰øùÁïô tokenized ÂêéÁöÑÊï∞ÊçÆ\n",
    "tokenized_dataset = raw_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "# ÂÆö‰πâ‰∏Ä‰∏™ÂáΩÊï∞Â∞ÜÊñáÊú¨ÂàÜÂùó\n",
    "# ‰æãÂ¶ÇÔºö BLOCK_SIZE = 128, ÈÇ£‰πàÊØèÂùóÂ∞ÜÂåÖÂê´128‰∏™token\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // BLOCK_SIZE) * BLOCK_SIZE\n",
    "    # Split by chunks of max_len\n",
    "    result = {\n",
    "        k: [t[i : i + BLOCK_SIZE] for i in range(0, total_length, BLOCK_SIZE)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# ‰ΩøÁî® group_texts ÂáΩÊï∞Â∞ÜÊñáÊú¨ÂàÜÂùó\n",
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "print(lm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a002e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaronghe/Desktop/projects/light-weight-private-llm/chapter4/.venv/lib/python3.10/site-packages/transformers/training_args.py:1951: UserWarning: `use_mps_device` is deprecated and will be removed in version 5.0 of ü§ó Transformers. `mps` device will be used by default if available similar to the way `cuda` device is used.Therefore, no action from user is required. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "OUTPUT_DIR = \"./gpt2-luxun-finetuned-mps\"\n",
    "NUM_TRAIN_EPOCHS = 3\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 5e-5\n",
    "SAVE_STEPS = 500\n",
    "OVERWRITE_OUTPUT_DIR = True\n",
    "\n",
    "# ËÆæÁΩÆËÆ≠ÁªÉÂèÇÊï∞\n",
    "# ‰æãÂ¶ÇÔºö num_train_epochs ÂèØ‰ª•ËÆæÁΩÆ‰∏∫‰Ω†Â∏åÊúõÁöÑËÆ≠ÁªÉËΩÆÊï∞\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=OVERWRITE_OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    logging_steps=50, # ÊØè50Ê≠•ÊâìÂç∞‰∏ÄÊ¨°Êó•Âøó\n",
    "    evaluation_strategy=\"steps\", # ÊØè500Ê≠•ËØÑ‰º∞‰∏ÄÊ¨°\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2, # ÊúÄÂ§ö‰øùÂ≠ò2‰∏™checkpoint\n",
    "    warmup_ratio=0.03, # È¢ÑÁÉ≠ÊØîÁéá\n",
    "    use_mps_device=True, # ‰ΩøÁî®MPSËÆæÂ§á\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2aea8d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jiaronghe/Desktop/projects/light-weight-private-llm/chapter4/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ÂºÄÂßãËÆ≠ÁªÉ ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|‚ñà‚ñç        | 51/348 [00:09<00:31,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.1968, 'grad_norm': 1.2414604425430298, 'learning_rate': 4.421364985163205e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|‚ñà‚ñà‚ñâ       | 101/348 [00:14<00:25,  9.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8949, 'grad_norm': 1.1374152898788452, 'learning_rate': 3.679525222551929e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 151/348 [00:20<00:20,  9.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8084, 'grad_norm': 1.5812174081802368, 'learning_rate': 2.937685459940653e-05, 'epoch': 1.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 201/348 [00:26<00:15,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7896, 'grad_norm': 1.6326978206634521, 'learning_rate': 2.195845697329377e-05, 'epoch': 1.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 251/348 [00:31<00:10,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7739, 'grad_norm': 1.4555388689041138, 'learning_rate': 1.454005934718101e-05, 'epoch': 2.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 301/348 [00:37<00:05,  9.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.7541, 'grad_norm': 1.8545022010803223, 'learning_rate': 7.12166172106825e-06, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 348/348 [00:42<00:00,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 42.4751, 'train_samples_per_second': 32.56, 'train_steps_per_second': 8.193, 'train_loss': 3.8502285836756913, 'epoch': 3.0}\n",
      "--- ËÆ≠ÁªÉÂÆåÊàê ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# ‰ΩøÁî® DataCollatorForLanguageModeling ‰Ωú‰∏∫Êï∞ÊçÆÊï¥ÁêÜÂô®\n",
    "# Âú®ÊûÑÂª∫ÊØè‰∏™batchÊó∂ÔºåËá™Âä®ËøõË°åÂä®ÊÄÅÂ°´ÂÖÖ padding, Êé©Á†ÅÁîüÊàê masking, ‰ª•ÂèäÊ†áÁ≠æÂØπÈΩê\n",
    "# ËøôÈáåÁöÑmlm=False, Ë°®Á§∫Êàë‰ª¨‰∏ç‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÊ®°Âûã (masked language modeling) ËøõË°åËÆ≠ÁªÉ\n",
    "# Âõ†‰∏∫Êàë‰ª¨Âú®ÂÅöËá™ÂõûÂΩíËØ≠Ë®ÄÊ®°Âûã (causal language modeling) ËÆ≠ÁªÉ\n",
    "# Â¶ÇÊûú‰Ω†‰ΩøÁî®ÁöÑÊòØ BERT Ê®°Âûã, ÈÇ£‰πà mlm=True, Ë°®Á§∫‰ΩøÁî®Êé©Á†ÅËØ≠Ë®ÄÊ®°ÂûãËøõË°åËÆ≠ÁªÉ\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "# ÂàõÂª∫ Trainer ÂØπË±°\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"--- ÂºÄÂßãËÆ≠ÁªÉ ---\")\n",
    "trainer.train()\n",
    "print(\"--- ËÆ≠ÁªÉÂÆåÊàê ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dc24a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ‰øùÂ≠òÊ®°Âûã: ./gpt2-luxun-finetuned-mps ---\n",
      "--- ÂÆåÊàê ---\n"
     ]
    }
   ],
   "source": [
    "# ‰øùÂ≠òÊ®°Âûã\n",
    "print(f\"--- ‰øùÂ≠òÊ®°Âûã: {OUTPUT_DIR} ---\")\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "# ‰øùÂ≠ò tokenizer\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(\"--- ÂÆåÊàê ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f420abab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "--- Âä†ËΩΩÂàÜËØçÂô® ---\n",
      "--- Âä†ËΩΩÂü∫Á°ÄÊ®°Âûã ---\n",
      "--- Âä†ËΩΩ LoRA adapter ---\n",
      "--- Ê®°ÂûãÂä†ËΩΩÂÆåÊàê ---\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "MODEL_PATH = \"./gpt2-luxun-finetuned-mps\"\n",
    "BASE_MODEL_NAME = \"uer/gpt2-chinese-cluecorpussmall\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"--- Âä†ËΩΩÂàÜËØçÂô® ---\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(\"--- Âä†ËΩΩÂü∫Á°ÄÊ®°Âûã ---\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "print(\"--- Âä†ËΩΩ LoRA adapter ---\")\n",
    "model = PeftModel.from_pretrained(base_model, MODEL_PATH)\n",
    "model.to(device)\n",
    "print(\"--- Ê®°ÂûãÂä†ËΩΩÂÆåÊàê ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c21db124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ÁîüÊàêÊñáÊú¨: ÁßãÂ§©ÁöÑÂêéÂçäÂ§úÔºåÊúà‰∫Æ‰∏ãÂéª‰∫ÜÔºåÂ§™Èò≥ËøòÊ≤°ÊúâÂá∫ÔºåÂè™Ââ©‰∏ã‰∏ÄÁâá‰πåËìùÁöÑÂ§©Ôºõ ---\n",
      "--- ÁîüÊàêÁªìÊûú ---\n",
      "Áßã Â§© ÁöÑ Âêé Âçä Â§ú Ôºå Êúà ‰∫Æ ‰∏ã Âéª ‰∫Ü Ôºå Â§™ Èò≥ Ëøò Ê≤° Êúâ Âá∫ Ôºå Âè™ Ââ© ‰∏ã ‰∏Ä Áâá ‰πå Ëìù ÁöÑ Â§© Ôºõ Áßã\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt_text, max_length=150):\n",
    "\n",
    "    print(f\"--- ÁîüÊàêÊñáÊú¨: {prompt_text} ---\")\n",
    "    # ÁºñÁ†ÅÔºöÂ∞ÜËµ∑ÂßãÁöÑÂè•Â≠êËΩ¨Êç¢‰∏∫Ê®°ÂûãÂèØ‰ª•ÁêÜËß£ÁöÑidÔºåÂπ∂ÁßªÂä®Âà∞ËÆæÂ§á\n",
    "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # ÁîüÊàêÊñáÊú¨ÔºåË∞ÉÁî®Ê®°ÂûãÁöÑgenerateÊñπÊ≥ï\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=max_length,\n",
    "        num_return_sequences=1, # ÁîüÊàêÂá†‰∏™‰∏çÂêåÁöÑÁªìÊûú\n",
    "        do_sample=True, # ÊòØÂê¶‰ΩøÁî®ÈááÊ†∑, ËÆ©ÊñáÊú¨Êõ¥ÂÖ∑ÊúâÂàõÈÄ†ÊÄß\n",
    "        top_k=50, # ‰ªéÊ¶ÇÁéáÊúÄÈ´òÁöÑ50‰∏™ËØç‰∏≠ÈÄâÊã©\n",
    "        top_p=0.95, # ‰ªéÊ¶ÇÁéáÊÄªÂíå‰∏∫95%ÁöÑËØçÊ±á‰∏≠ÈÄâÊã©\n",
    "        temperature=0.9, # ÊéßÂà∂ÁîüÊàêÈöèÊú∫ÊÄßÔºåÂÄºË∂äÂ§ßÔºåÈöèÊú∫ÊÄßË∂äÈ´ò\n",
    "        repetition_penalty=1.5, # ÊéßÂà∂ÈáçÂ§çËØçÁöÑÂá∫Áé∞ÔºåÂÄºË∂äÂ§ßÔºåÈáçÂ§çËØçÂá∫Áé∞ÁöÑÊ¶ÇÁéáË∂ä‰Ωé\n",
    "    )\n",
    "\n",
    "    # Ëß£Á†ÅÔºöÂ∞ÜÊ®°ÂûãÁîüÊàêÁöÑidËΩ¨Êç¢‰∏∫ÊñáÊú¨\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"--- ÁîüÊàêÁªìÊûú ---\")\n",
    "    print(generated_text)\n",
    "\n",
    "prompt_text = \"ÁßãÂ§©ÁöÑÂêéÂçäÂ§úÔºåÊúà‰∫Æ‰∏ãÂéª‰∫ÜÔºåÂ§™Èò≥ËøòÊ≤°ÊúâÂá∫ÔºåÂè™Ââ©‰∏ã‰∏ÄÁâá‰πåËìùÁöÑÂ§©Ôºõ\"\n",
    "generate_text(prompt_text, max_length=300)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (chapter4)",
   "language": "python",
   "name": "chapter4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
