{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "783eb2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ qwen3:latest è¿›è¡Œè’¸é¦...\n",
      "\n",
      "æ­£åœ¨å¤„ç†: å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ . ..\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: é¦–å…ˆï¼Œåˆå§‹æœ‰3ä¸ªè‹¹æœã€‚åƒæ‰1ä¸ªåï¼Œå‰©ä¸‹çš„è‹¹æœæ•°é‡æ˜¯3å‡å»1ï¼Œç­‰äº2ä¸ªã€‚æ¥ç€ï¼Œåˆä¹°æ¥äº†5ä¸ªè‹¹æœï¼Œæ‰€ä»¥éœ€. ..\n",
      "æ­£åœ¨å¤„ç†: ä¸€ä¸ªç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œå¤´æœ‰10ä¸ªï¼Œè„šæœ‰28åªï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ . ..\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: é¦–å…ˆï¼Œé¢˜ç›®ä¸­æåˆ°ç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œæ€»å…±æœ‰10ä¸ªå¤´å’Œ28åªè„šã€‚é¸¡å’Œå…”éƒ½æœ‰1ä¸ªå¤´ï¼Œä½†è„šçš„æ•°é‡ä¸åŒï¼Œé¸¡æœ‰2åª. ..\n",
      "æ­£åœ¨å¤„ç†: ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œåˆ¤æ–­ä¸€ä¸ªæ•°å­—æ˜¯ä¸æ˜¯ç´ æ•°ã€‚ . ..\n",
      "âœ… ç”ŸæˆæˆåŠŸï¼\n",
      "  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: é¦–å…ˆï¼Œéœ€è¦æ˜ç¡®ç´ æ•°çš„å®šä¹‰ï¼šç´ æ•°æ˜¯å¤§äº1çš„è‡ªç„¶æ•°ï¼Œé™¤äº†1å’Œå®ƒæœ¬èº«å¤–æ²¡æœ‰å…¶ä»–å› æ•°ã€‚å› æ­¤ï¼Œå‡½æ•°éœ€è¦å¤„ç†è¾“å…¥. ..\n",
      "\n",
      "ğŸ‰ ä»»åŠ¡å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸º local_distill_data.jsonlï¼Œå…± 3 æ¡ã€‚\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# 1. æŒ‡å‘æœ¬åœ° Ollama æ¥å£\n",
    "OLLAMA_BASE_URL = \"http://192.168.1.10:11434/api/generate\"\n",
    "OLLAMA_MODEL_NAME = \"qwen3:latest\"  # ç¡®ä¿ä½ çš„Ollamaä¸­å­˜åœ¨è¿™ä¸ªæ¨¡å‹\n",
    "\n",
    "# ===========================================\n",
    "\n",
    "def distill_knowledge_local(question):\n",
    "    # é’ˆå¯¹ Qwen ä¼˜åŒ–çš„ Promptï¼Œå®ƒå¯¹ä¸­æ–‡æŒ‡ä»¤è·Ÿéšå¾ˆå¥½\n",
    "    prompt = f\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä½é€»è¾‘ä¸¥å¯†çš„è€å¸ˆã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆç”¨äºæ•™å­¦çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰ã€‚\n",
    "    \n",
    "    é—®é¢˜ï¼š{question}\n",
    "    \n",
    "    è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç›´æ¥è¿”å›ï¼Œä¸è¦åŒ…å«Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```json ï¼‰ï¼š\n",
    "    {{\n",
    "        \"instruction\": \"{question}\",\n",
    "        \"rationale\": \"è¿™é‡Œå†™ä¸‹è¯¦ç»†çš„ã€åˆ†æ­¥éª¤çš„æ¨ç†è¿‡ç¨‹ï¼Œå…ˆåˆ†æå†è®¡ç®—ï¼Œé€»è¾‘è¦æ¸…æ™°ã€‚\",\n",
    "        \"output\": \"æœ€ç»ˆçš„ç®€çŸ­ç­”æ¡ˆ\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # æ„é€ Ollama APIè¯·æ±‚å‚æ•°\n",
    "        payload = {\n",
    "            \"model\": OLLAMA_MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"format\": \"json\"  # å‘Šè¯‰Ollamaè¿”å›JSONæ ¼å¼å“åº”\n",
    "        }\n",
    "        \n",
    "        # å‘é€POSTè¯·æ±‚åˆ°Ollama API\n",
    "        response = requests.post(OLLAMA_BASE_URL, json=payload, timeout=30)\n",
    "        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯\n",
    "        \n",
    "        # è§£æå“åº”å†…å®¹\n",
    "        content = response.json()\n",
    "        \n",
    "        if isinstance(content, dict) and \"response\" in content:\n",
    "            clean_content = content[\"response\"].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            try:\n",
    "                data_json = json.loads(clean_content)\n",
    "                return data_json\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ JSONè§£æå¤±è´¥ï¼Œæ¨¡å‹è¾“å‡ºäº†éæ ‡å‡†æ ¼å¼:\\n{clean_content}\\n\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"âŒ å“åº”æ ¼å¼å¼‚å¸¸: {content}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ç½‘ç»œè¯·æ±‚å‡ºé”™: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"APIè°ƒç”¨å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œï¼šæˆ‘ä»¬ç”¨å‡ ä¸ªæ•°å­¦é€»è¾‘é¢˜æ¥æµ‹è¯• Qwen çš„æ¨ç†ç”Ÿæˆèƒ½åŠ›\n",
    "seed_questions = [\n",
    "    \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\",\n",
    "    \"ä¸€ä¸ªç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œå¤´æœ‰10ä¸ªï¼Œè„šæœ‰28åªï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ\",\n",
    "    \"ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œåˆ¤æ–­ä¸€ä¸ªæ•°å­—æ˜¯ä¸æ˜¯ç´ æ•°ã€‚\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "print(f\"æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ {OLLAMA_MODEL_NAME} è¿›è¡Œè’¸é¦...\\n\")\n",
    "\n",
    "for q in seed_questions:\n",
    "    print(f\"æ­£åœ¨å¤„ç†: {q} . ..\")\n",
    "    content = distill_knowledge_local(q)\n",
    "    \n",
    "    if content:\n",
    "        # æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™æ¨¡å‹ä¼šå¿ä¸ä½åŠ  ```json ... ```ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰\n",
    "        try:\n",
    "            data_json = content\n",
    "            dataset.append(data_json)\n",
    "            print(\"âœ… ç”ŸæˆæˆåŠŸï¼\")\n",
    "            print(f\"  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: {data_json['rationale'][:50]}. ..\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæ— è¿”å›å†…å®¹ã€‚\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "if dataset:\n",
    "    filename = \"local_distill_data.jsonl\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in dataset:\n",
    "            final_entry = {\n",
    "                \"instruction\": entry[\"instruction\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": f\"ã€æ€è€ƒè¿‡ç¨‹ã€‘\\n{entry['rationale']}\\n\\nã€æœ€ç»ˆç­”æ¡ˆã€‘\\n{entry['output']}\"\n",
    "            }\n",
    "            json.dump(final_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"\\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸º {filename}ï¼Œå…± {len(dataset)} æ¡ã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87fb83cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0501b9ac-00d4-4bb5-95d7-341b683a15e3)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b206df2bb8ec44dabb713f89766532dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b601de3e0fd741b1848acc72cf1d3e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3be1dca9ed44ea28140337e220490ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad0590779a747b78a8dabd72ed10897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹é—®é¢˜: å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\n",
      "Token æ€»é•¿åº¦: 112\n",
      "Labels å‰10ä¸ª (åº”è¯¥æ˜¯-100): tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100])\n",
      "Labels æœ€å10ä¸ª (åº”è¯¥æœ‰å€¼): tensor([ 10904, 103941, 102349,  10958,    198,     22,  18947, 151645,    198,\n",
      "        151645])\n",
      "\n",
      "=== æ¨¡å‹å°†å­¦ä¹ çš„å†…å®¹ (Teacher çš„ CoT) ===\n",
      "ã€æ€è€ƒè¿‡ç¨‹ã€‘\n",
      "é¦–å…ˆï¼Œåˆå§‹æœ‰3ä¸ªè‹¹æœã€‚åƒæ‰1ä¸ªåï¼Œå‰©ä¸‹çš„è‹¹æœæ•°é‡æ˜¯3å‡å»1ï¼Œç­‰äº2ä¸ªã€‚æ¥ç€ï¼Œåˆä¹°æ¥äº†5ä¸ªè‹¹æœï¼Œæ‰€ä»¥éœ€è¦å°†å‰©ä¸‹çš„2ä¸ªåŠ ä¸Šæ–°ä¹°çš„5ä¸ªï¼Œæ€»å…±æ˜¯2åŠ 5ï¼Œç­‰äº7ä¸ªã€‚\n",
      "\n",
      "ã€æœ€ç»ˆç­”æ¡ˆã€‘\n",
      "7ä¸ª<|im_end|>\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# 1. åŠ è½½å­¦ç”Ÿæ¨¡å‹çš„ Tokenizer (å‡è®¾ä½ ç”¨çš„æ˜¯ Qwen2.5-0.5B)\n",
    "# å¦‚æœæœ¬åœ°è¿˜æ²¡ä¸‹è½½ï¼Œå®ƒä¼šè‡ªåŠ¨ä¸‹è½½\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ç¡®ä¿ pad_token å­˜åœ¨\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    å°†ç”Ÿæˆçš„ CoT æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ ¼å¼\n",
    "    Example æ ¼å¼: {\"instruction\": \"...\", \"input\": \"\", \"output\": \"ã€æ€è€ƒè¿‡ç¨‹ã€‘...\"}\n",
    "    \"\"\"\n",
    "    MAX_LENGTH = 512 # æ¼”ç¤ºç”¨ï¼Œå®é™…å¯è°ƒå¤§\n",
    "    \n",
    "    instruction = example[\"instruction\"]\n",
    "    output = example[\"output\"] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "\n",
    "    # Qwen çš„æ ‡å‡†å¯¹è¯æ¨¡æ¿æ„å»º\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": output}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ apply_chat_template è‡ªåŠ¨å¤„ç† <|im_start|> ç­‰ç‰¹æ®Š token\n",
    "    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼Œä¸åŒæ¨¡å‹æ¨¡æ¿ä¸åŒ\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # è½¬æ¢ä¸º ID\n",
    "    input_ids = tokenizer(text + tokenizer.eos_token, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    # === å…³é”®ç‚¹ï¼šåˆ¶ä½œ Labels (Masking) ===\n",
    "    # æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹å­¦ä¹  \"User\" è¯´çš„è¯ï¼Œåªå­¦ä¹  \"Assistant\" è¯´çš„è¯\n",
    "    # æ‰€ä»¥è¦æŠŠ User éƒ¨åˆ†çš„ token åœ¨ label ä¸­è®¾ä¸º -100 (PyTorch ä¼šå¿½ç•¥)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # æ‰¾åˆ° \"assistant\" å›å¤å¼€å§‹çš„ä½ç½®\n",
    "    # Qwen çš„æ¨¡æ¿é€šå¸¸æ˜¯: ... <|im_start|>assistant\\n\n",
    "    # æˆ‘ä»¬ç®€å•ç²—æš´åœ°é‡æ–° tokenize \"user\" éƒ¨åˆ†æ¥æ‰¾é•¿åº¦ï¼ˆä¸¥è°¨çš„åšæ³•éœ€æ›´å¤æ‚åŒ¹é…ï¼‰\n",
    "    \n",
    "    user_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    user_text = tokenizer.apply_chat_template(user_messages, tokenize=False, add_generation_prompt=True)\n",
    "    user_ids = tokenizer(user_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    user_len = len(user_ids)\n",
    "    \n",
    "    # å°† User éƒ¨åˆ†çš„ Label è®¾ä¸º -100\n",
    "    labels[:user_len] = -100\n",
    "    \n",
    "    # æˆªæ–­æˆ–å¡«å……\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": torch.ones_like(input_ids) # ç®€å•å¤„ç†\n",
    "    }\n",
    "\n",
    "# === æµ‹è¯•è¿è¡Œ ===\n",
    "# è¯»å–æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ local_distill_data.jsonl çš„ç¬¬ä¸€æ¡\n",
    "with open(\"local_distill_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "\n",
    "processed = process_func(first_line)\n",
    "\n",
    "print(f\"åŸå§‹é—®é¢˜: {first_line['instruction']}\")\n",
    "print(f\"Token æ€»é•¿åº¦: {len(processed['input_ids'])}\")\n",
    "print(f\"Labels å‰10ä¸ª (åº”è¯¥æ˜¯-100): {processed['labels'][:10]}\")\n",
    "print(f\"Labels æœ€å10ä¸ª (åº”è¯¥æœ‰å€¼): {processed['labels'][-10:]}\")\n",
    "\n",
    "# è§£ç çœ‹çœ‹æ¨¡å‹åˆ°åº•è¦åœ¨å“ªä¸€éƒ¨åˆ†è®¡ç®— Loss\n",
    "valid_labels = processed['labels'][processed['labels'] != -100]\n",
    "print(\"\\n=== æ¨¡å‹å°†å­¦ä¹ çš„å†…å®¹ (Teacher çš„ CoT) ===\")\n",
    "print(tokenizer.decode(valid_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e7ec43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: datasets in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (4.4.1)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.9.3)\n",
      "Requirement already satisfied: anyio in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: trl in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (0.25.1)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from trl) (4.4.1)\n",
      "Requirement already satisfied: transformers>=4.56.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from trl) (4.57.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (25.0)\n",
      "Requirement already satisfied: psutil in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (2.2.0+cu121)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets>=3.0.0->trl) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets>=3.0.0->trl) (2.2.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.9.3)\n",
      "Requirement already satisfied: anyio in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.11.0)\n",
      "Requirement already satisfied: certifi in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: idna in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (4.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.2.0)\n",
      "Requirement already satisfied: sympy in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.3)\n",
      "Requirement already satisfied: colorama in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from transformers>=4.56.1->trl) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from transformers>=4.56.1->trl) (0.22.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from sympy->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting tf-keras\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/85/6b/d9a8202bfe5c9e3b078cf550bafab962aa9d6b1a1f1180f0065399d4c9b2/tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "     ------------ --------------------------- 0.5/1.7 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.7 MB/s  0:00:00\n",
      "Collecting tensorflow<2.21,>=2.20 (from tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/74/b5/d40e1e389e07de9d113cf8e5d294c04d06124441d57606febfd0fb2cf5a6/tensorflow-2.20.0-cp310-cp310-win_amd64.whl (331.7 MB)\n",
      "     ---------------------------------------- 0.0/331.7 MB ? eta -:--:--\n",
      "     --------------------------------------- 2.9/331.7 MB 15.2 MB/s eta 0:00:22\n",
      "      -------------------------------------- 7.6/331.7 MB 20.4 MB/s eta 0:00:16\n",
      "     - ------------------------------------ 13.4/331.7 MB 23.3 MB/s eta 0:00:14\n",
      "     - ------------------------------------ 16.0/331.7 MB 21.9 MB/s eta 0:00:15\n",
      "     -- ----------------------------------- 18.6/331.7 MB 18.1 MB/s eta 0:00:18\n",
      "     --- ---------------------------------- 29.9/331.7 MB 24.9 MB/s eta 0:00:13\n",
      "     ---- --------------------------------- 35.4/331.7 MB 24.7 MB/s eta 0:00:12\n",
      "     ---- --------------------------------- 41.4/331.7 MB 26.1 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 45.1/331.7 MB 24.7 MB/s eta 0:00:12\n",
      "     ----- -------------------------------- 47.7/331.7 MB 23.2 MB/s eta 0:00:13\n",
      "     ----- -------------------------------- 50.6/331.7 MB 22.7 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 53.5/331.7 MB 21.6 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 56.6/331.7 MB 21.2 MB/s eta 0:00:13\n",
      "     ------ ------------------------------- 59.5/331.7 MB 20.8 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 62.4/331.7 MB 20.3 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 66.3/331.7 MB 19.9 MB/s eta 0:00:14\n",
      "     ------- ------------------------------ 68.9/331.7 MB 19.7 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 72.4/331.7 MB 19.3 MB/s eta 0:00:14\n",
      "     -------- ----------------------------- 75.2/331.7 MB 19.0 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 78.6/331.7 MB 18.9 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 81.3/331.7 MB 18.6 MB/s eta 0:00:14\n",
      "     --------- ---------------------------- 84.7/331.7 MB 18.6 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 87.6/331.7 MB 18.4 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 91.0/331.7 MB 18.3 MB/s eta 0:00:14\n",
      "     ---------- --------------------------- 93.8/331.7 MB 18.1 MB/s eta 0:00:14\n",
      "     ----------- -------------------------- 97.0/331.7 MB 17.9 MB/s eta 0:00:14\n",
      "     ----------- -------------------------- 99.1/331.7 MB 17.8 MB/s eta 0:00:14\n",
      "     ----------- ------------------------- 102.8/331.7 MB 17.7 MB/s eta 0:00:13\n",
      "     ----------- ------------------------- 105.4/331.7 MB 17.7 MB/s eta 0:00:13\n",
      "     ------------ ------------------------ 108.5/331.7 MB 17.5 MB/s eta 0:00:13\n",
      "     ------------ ------------------------ 111.7/331.7 MB 17.5 MB/s eta 0:00:13\n",
      "     ------------ ------------------------ 114.8/331.7 MB 17.3 MB/s eta 0:00:13\n",
      "     ------------- ----------------------- 118.5/331.7 MB 17.3 MB/s eta 0:00:13\n",
      "     ------------- ----------------------- 121.1/331.7 MB 17.3 MB/s eta 0:00:13\n",
      "     ------------- ----------------------- 124.3/331.7 MB 17.1 MB/s eta 0:00:13\n",
      "     -------------- ---------------------- 127.4/331.7 MB 17.1 MB/s eta 0:00:12\n",
      "     -------------- ---------------------- 130.0/331.7 MB 17.1 MB/s eta 0:00:12\n",
      "     -------------- ---------------------- 133.4/331.7 MB 16.9 MB/s eta 0:00:12\n",
      "     --------------- --------------------- 136.3/331.7 MB 16.9 MB/s eta 0:00:12\n",
      "     --------------- --------------------- 139.2/331.7 MB 16.9 MB/s eta 0:00:12\n",
      "     --------------- --------------------- 142.3/331.7 MB 16.8 MB/s eta 0:00:12\n",
      "     ---------------- -------------------- 145.5/331.7 MB 16.7 MB/s eta 0:00:12\n",
      "     ---------------- -------------------- 148.4/331.7 MB 16.7 MB/s eta 0:00:11\n",
      "     ---------------- -------------------- 151.8/331.7 MB 16.6 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 154.4/331.7 MB 16.6 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 156.2/331.7 MB 16.6 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 156.2/331.7 MB 16.6 MB/s eta 0:00:11\n",
      "     ----------------- ------------------- 156.5/331.7 MB 16.0 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 166.2/331.7 MB 16.4 MB/s eta 0:00:11\n",
      "     ------------------ ------------------ 168.6/331.7 MB 16.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 172.5/331.7 MB 16.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 175.9/331.7 MB 16.3 MB/s eta 0:00:10\n",
      "     ------------------- ----------------- 178.5/331.7 MB 16.3 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 181.9/331.7 MB 16.2 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 184.8/331.7 MB 16.2 MB/s eta 0:00:10\n",
      "     -------------------- ---------------- 187.7/331.7 MB 16.2 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 191.1/331.7 MB 16.2 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 193.7/331.7 MB 16.1 MB/s eta 0:00:09\n",
      "     --------------------- --------------- 197.1/331.7 MB 16.1 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 199.8/331.7 MB 16.1 MB/s eta 0:00:09\n",
      "     ---------------------- -------------- 202.9/331.7 MB 16.1 MB/s eta 0:00:09\n",
      "     ----------------------- ------------- 206.3/331.7 MB 16.1 MB/s eta 0:00:08\n",
      "     ----------------------- ------------- 208.9/331.7 MB 16.1 MB/s eta 0:00:08\n",
      "     ----------------------- ------------- 212.6/331.7 MB 16.0 MB/s eta 0:00:08\n",
      "     ------------------------ ------------ 215.2/331.7 MB 16.0 MB/s eta 0:00:08\n",
      "     ------------------------ ------------ 218.6/331.7 MB 16.0 MB/s eta 0:00:08\n",
      "     ------------------------ ------------ 221.2/331.7 MB 16.0 MB/s eta 0:00:07\n",
      "     ------------------------ ------------ 223.6/331.7 MB 15.9 MB/s eta 0:00:07\n",
      "     ------------------------- ----------- 227.3/331.7 MB 15.9 MB/s eta 0:00:07\n",
      "     ------------------------- ----------- 230.9/331.7 MB 15.9 MB/s eta 0:00:07\n",
      "     ------------------------- ----------- 233.0/331.7 MB 15.9 MB/s eta 0:00:07\n",
      "     -------------------------- ---------- 236.7/331.7 MB 15.9 MB/s eta 0:00:06\n",
      "     -------------------------- ---------- 239.6/331.7 MB 15.9 MB/s eta 0:00:06\n",
      "     --------------------------- --------- 242.7/331.7 MB 15.9 MB/s eta 0:00:06\n",
      "     --------------------------- --------- 246.2/331.7 MB 15.8 MB/s eta 0:00:06\n",
      "     --------------------------- --------- 249.3/331.7 MB 15.8 MB/s eta 0:00:06\n",
      "     ---------------------------- -------- 251.9/331.7 MB 15.8 MB/s eta 0:00:06\n",
      "     ---------------------------- -------- 255.3/331.7 MB 15.8 MB/s eta 0:00:05\n",
      "     ---------------------------- -------- 258.2/331.7 MB 15.8 MB/s eta 0:00:05\n",
      "     ----------------------------- ------- 261.1/331.7 MB 15.7 MB/s eta 0:00:05\n",
      "     ----------------------------- ------- 264.2/331.7 MB 15.7 MB/s eta 0:00:05\n",
      "     ----------------------------- ------- 267.6/331.7 MB 15.7 MB/s eta 0:00:05\n",
      "     ------------------------------ ------ 270.0/331.7 MB 15.6 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 273.4/331.7 MB 15.5 MB/s eta 0:00:04\n",
      "     ------------------------------ ------ 275.8/331.7 MB 15.5 MB/s eta 0:00:04\n",
      "     ------------------------------- ----- 279.2/331.7 MB 15.6 MB/s eta 0:00:04\n",
      "     ------------------------------- ----- 282.1/331.7 MB 15.4 MB/s eta 0:00:04\n",
      "     ------------------------------- ----- 285.5/331.7 MB 15.3 MB/s eta 0:00:04\n",
      "     -------------------------------- ---- 288.6/331.7 MB 15.2 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 291.2/331.7 MB 15.0 MB/s eta 0:00:03\n",
      "     -------------------------------- ---- 293.9/331.7 MB 15.0 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 296.7/331.7 MB 14.9 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 300.2/331.7 MB 14.8 MB/s eta 0:00:03\n",
      "     --------------------------------- --- 303.3/331.7 MB 14.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 305.7/331.7 MB 14.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 309.6/331.7 MB 14.7 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 312.5/331.7 MB 14.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 315.9/331.7 MB 14.7 MB/s eta 0:00:02\n",
      "     ----------------------------------- - 318.5/331.7 MB 14.7 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 322.2/331.7 MB 14.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  324.5/331.7 MB 14.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  328.2/331.7 MB 14.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  331.6/331.7 MB 14.7 MB/s eta 0:00:01\n",
      "     ------------------------------------  331.6/331.7 MB 14.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 331.7/331.7 MB 14.5 MB/s  0:00:21\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.4.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.0)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/64/20/4d50191997e917ae13ad0a235c8b42d8c1ab9c3e6fd455ca16d416944355/protobuf-6.33.2-cp310-abi3-win_amd64.whl (436 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\administrator\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.5)\n",
      "Requirement already satisfied: setuptools in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (69.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.62.1)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/9c/d9/a5db55f88f258ac669a92858b70a714bbbd5acd993820b41ec4a96a4d77f/tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "     ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 5.5/5.5 MB 28.1 MB/s  0:00:00\n",
      "Collecting keras>=3.10.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/61/cc8be27bd65082440754be443b17b6f7c185dec5e00dfdaeab4f8662e4a8/keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.5/1.5 MB 25.8 MB/s  0:00:00\n",
      "Collecting numpy>=1.26.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/a3/dd/4b822569d6b96c39d1215dbae0582fd99954dcbcf0c1a13c61783feaca3f/numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 5.2/12.9 MB 31.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.9 MB 30.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.9/12.9 MB 21.9 MB/s  0:00:00\n",
      "Collecting h5py>=3.11.0 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c3/28/dc08de359c2f43a67baa529cb70d7f9599848750031975eed92d6ae78e1d/h5py-3.15.1-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "     ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "     ------------------------------------ --- 2.6/2.9 MB 37.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.9/2.9 MB 11.9 MB/s  0:00:00\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow<2.21,>=2.20->tf-keras)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c7/a3/51886727bd16e2f47587997b802dd56398692ce8c6c03c2e5bb32ecafe26/ml_dtypes-0.5.4-cp310-cp310-win_amd64.whl (210 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2025.11.12)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.6)\n",
      "Requirement already satisfied: pillow in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (10.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.41.2)\n",
      "Requirement already satisfied: rich in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Installing collected packages: protobuf, numpy, tensorboard, ml_dtypes, h5py, keras, tensorflow, tf-keras\n",
      "\n",
      "   ---------------------------------------- 0/8 [protobuf]\n",
      "   ---------------------------------------- 0/8 [protobuf]\n",
      "   ---------------------------------------- 0/8 [protobuf]\n",
      "   ---------------------------------------- 0/8 [protobuf]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ----- ---------------------------------- 1/8 [numpy]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   ---------- ----------------------------- 2/8 [tensorboard]\n",
      "   -------------------- ------------------- 4/8 [h5py]\n",
      "   -------------------- ------------------- 4/8 [h5py]\n",
      "   -------------------- ------------------- 4/8 [h5py]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------- -------------- 5/8 [keras]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ------------------------------ --------- 6/8 [tensorflow]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ----------------------------------- ---- 7/8 [tf-keras]\n",
      "   ---------------------------------------- 8/8 [tf-keras]\n",
      "\n",
      "Successfully installed h5py-3.15.1 keras-3.12.0 ml_dtypes-0.5.4 numpy-2.2.6 protobuf-6.33.2 tensorboard-2.20.0 tensorflow-2.20.0 tf-keras-2.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ltralytics (d:\\programdata\\miniconda3\\envs\\yolov8\\lib\\site-packages)\n",
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\\Users\\Administrator\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain 0.1.10 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "langchain-community 0.0.25 requires numpy<2,>=1, but you have numpy 2.2.6 which is incompatible.\n",
      "matplotlib 3.8.2 requires numpy<2,>=1.21, but you have numpy 2.2.6 which is incompatible.\n",
      "pandas 2.2.0 requires numpy<2,>=1.22.4; python_version < \"3.11\", but you have numpy 2.2.6 which is incompatible.\n",
      "scipy 1.12.0 requires numpy<1.29.0,>=1.22.4, but you have numpy 2.2.6 which is incompatible.\n",
      "statsmodels 0.14.1 requires numpy<2,>=1.18, but you have numpy 2.2.6 which is incompatible.\n",
      "statsmodels 0.14.1 requires numpy<2,>=1.22.3; python_version == \"3.10\" and platform_system == \"Windows\" and platform_python_implementation != \"PyPy\", but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires ml-dtypes~=0.3.1, but you have ml-dtypes 0.5.4 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires numpy<2.0.0,>=1.23.5; python_version <= \"3.11\", but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 6.33.2 which is incompatible.\n",
      "tensorflow-intel 2.16.1 requires tensorboard<2.17,>=2.16, but you have tensorboard 2.20.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cbd20e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9108\\2890340605.py\", line 1, in <module>\n",
      "    import transformers\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\utils\\__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\utils\\auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\utils\\generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\__init__.py\", line 1471, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.57.3\n",
      "0.19.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import trl\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(trl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å¼•å…¥ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "# ä½œç”¨ï¼šè¿™æ˜¯åº•å±‚åŸºç¡€æ¡†æ¶ï¼Œç”¨äºå¼ é‡è®¡ç®—ã€è‡ªåŠ¨æ±‚å¯¼ä»¥åŠç®¡ç† GPU/CPU èµ„æºã€‚\n",
    "import torch\n",
    "# 2. ä» datasets åº“ä¸­å¼•å…¥ load_dataset å‡½æ•°\n",
    "# ä½œç”¨ï¼šç”¨äºåŠ è½½è®­ç»ƒæ•°æ®ã€‚æ”¯æŒä» Hugging Face Hub åœ¨çº¿åŠ è½½ï¼Œä¹Ÿæ”¯æŒåŠ è½½æœ¬åœ°çš„ JSON/CSV/Parquet æ–‡ä»¶ã€‚\n",
    "from datasets import load_dataset\n",
    "# 3. ä» transformers åº“ä¸­å¼•å…¥ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶\n",
    "# AutoTokenizer: è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½è¯»æ‡‚çš„æ•°å­—ï¼ˆToken IDï¼‰ã€‚\n",
    "# AutoModelForCausalLM: è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Llama, Qwen, GPT ç­‰è‡ªå›å½’æ¨¡å‹ï¼‰çš„ç»“æ„å’Œæƒé‡ã€‚\n",
    "# TrainingArguments: ç”¨äºå®šä¹‰è®­ç»ƒæ—¶çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€Batch Sizeã€Epochæ•°ã€ä¿å­˜è·¯å¾„ç­‰ï¼‰ã€‚\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "# 4. ä» trl (Transformer Reinforcement Learning) åº“ä¸­å¼•å…¥ SFT è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨\n",
    "# SFTTrainer: ä¸“é—¨ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–äº†æµç¨‹ã€‚\n",
    "# DataCollatorForCompletionOnlyLM: ä¸€ç§ç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦ä¹ â€œå›ç­”â€éƒ¨åˆ†ï¼Œ\n",
    "# è€Œä¸è®¡ç®—â€œæé—®â€éƒ¨åˆ†çš„ Lossã€‚è¿™ä¸ªå·¥å…·å¯ä»¥è‡ªåŠ¨æŠŠâ€œæé—®â€éƒ¨åˆ†æ©ç›–ï¼ˆMaskï¼‰æ‰ã€‚\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# 5. ä» peft åº“ä¸­å¼•å…¥ LoraConfig\n",
    "# ä½œç”¨ï¼šPEFT (Parameter-Efficient Fine-Tuning) æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ã€‚\n",
    "# LoraConfig ç”¨äºé…ç½® LoRA (Low-Rank Adaptation) ç®—æ³•çš„å‚æ•°ï¼ˆå¦‚ç§© rank, alpha ç­‰ï¼‰ï¼Œ\n",
    "# è®©æˆ‘ä»¬ä¸éœ€è¦å…¨é‡å¾®è°ƒæ¨¡å‹ï¼Œåªéœ€å¾®è°ƒæå°‘é‡çš„å‚æ•°ï¼Œå¤§å¤§é™ä½æ˜¾å­˜éœ€æ±‚ã€‚\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cb708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e2665b7dfa45d6acf8b9b047f6de72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x21e4c460eb0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 0b0e7eb8-907e-4eff-a647-131488dccde9)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x21e4c461870>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: de853889-6e2e-4a3c-ad27-13ed07cce1ad)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05fd37d59d846659ab85036e2de3a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd89817db54431e869dfb77ec2f121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3a482ca474437bada2b3be10ee924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ================= é…ç½® =================\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "DATA_FILE = \"math_dataset_with_answers.jsonl\" \n",
    "\n",
    "# ç”¨äºæŒ‡å®šè®­ç»ƒå®Œæˆåæ¨¡å‹ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "# åç»­åœ¨å®šä¹‰ TrainingArguments æ—¶ä¼šç”¨åˆ°è¿™ä¸ªå˜é‡ã€‚å¦‚æœè¯¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè®­ç»ƒè„šæœ¬é€šå¸¸ä¼šè‡ªåŠ¨åˆ›å»ºå®ƒã€‚\n",
    "OUTPUT_DIR = \"./qwen2.5_distilled_checkpoint\"\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "# ä½¿ç”¨ HuggingFace dataset åº“ç›´æ¥è¯»å– jsonl\n",
    "\n",
    "# json, æŒ‡å®šæ•°æ®çš„è§£ææ ¼å¼ã€‚è¿™æ„å‘³ç€ datasets åº“ä¼šä½¿ç”¨å†…ç½®çš„ JSON åŠ è½½è„šæœ¬å»è¯»å–æ–‡ä»¶ã€‚é™¤äº† \"json\"ï¼Œå¸¸ç”¨çš„è¿˜æœ‰ \"csv\", \"parquet\", \"text\" ç­‰ã€‚\n",
    "# data_files, è¿™ä¸ªå‚æ•°å‘Šè¯‰å‡½æ•°å»å“ªé‡Œæ‰¾æ•°æ®ã€‚æ”¯æŒä¼ å…¥å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿæ”¯æŒä¼ å…¥æ–‡ä»¶åˆ—è¡¨ã€‚\n",
    "# split, æŒ‡å®šåŠ è½½æ•°æ®åçš„åˆ’åˆ†æ ‡ç­¾ã€‚load_dataset é»˜è®¤ä¼šè¿”å›ä¸€ä¸ª DatasetDictï¼ˆåŒ…å« train/test/valid ç­‰ï¼‰ã€‚\n",
    "# è®¾ç½® split=\"train\" åï¼Œå®ƒä¼šç›´æ¥è¿”å›ä¸€ä¸ªå•çº¯çš„ Dataset å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«å…¨é‡æ•°æ®ã€‚\n",
    "# å¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œä½ åç»­è°ƒç”¨æ•°æ®æ—¶éœ€è¦ç”¨ dataset[\"train\"] æ‰èƒ½å–åˆ°æ•°æ®ã€‚\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "\n",
    "# 2. åŠ è½½æ¨¡å‹ä¸Tokenizer\n",
    "# æ ¹æ®æ¨¡å‹ ID åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨ã€‚åˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬åˆ‡åˆ†å¹¶è½¬æ¢ä¸ºæ•°å­— IDã€‚\n",
    "\n",
    "# MODEL_ID, æ¨¡å‹åœ¨ Hugging Face Hub ä¸Šçš„ IDï¼ˆå¦‚ \"Qwen/Qwen2.5-7B\"ï¼‰æˆ–è€…æœ¬åœ°æ¨¡å‹çš„å­˜å‚¨è·¯å¾„ã€‚\n",
    "# trust_remote_code=True, éå¸¸é‡è¦ã€‚è®¸å¤šæ–°æ¨¡å‹ï¼ˆå¦‚ Qwen, ChatGLM ç­‰ï¼‰çš„åˆ†è¯é€»è¾‘å¹¶æ²¡æœ‰é›†æˆåœ¨å®˜æ–¹çš„ transformers åº“ä¸­ï¼Œ\n",
    "# è€Œæ˜¯æ”¾åœ¨äº†æ¨¡å‹ä»“åº“é‡Œçš„ Python æ–‡ä»¶ï¼ˆå¦‚ tokenization_qwen.pyï¼‰ä¸­ã€‚\n",
    "# è®¾ç½®ä¸º True, è¡¨ç¤ºä½ å…è®¸ä»è¯¥æ¨¡å‹ä»“åº“ä¸‹è½½å¹¶æ‰§è¡Œè¿™äº›è‡ªå®šä¹‰çš„ Python ä»£ç ã€‚å¦‚æœä¸è®¾ç½®ï¼ŒåŠ è½½æ–°æ¨¡å‹é€šå¸¸ä¼šæŠ¥é”™ã€‚\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦æœ‰â€œå¡«å……ç¬¦â€ï¼ˆpad_tokenï¼‰ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±ç”¨â€œç»“æŸç¬¦â€ï¼ˆeos_tokenï¼‰ä»£æ›¿ã€‚\n",
    "# åœ¨æ‰¹é‡è®­ç»ƒï¼ˆBatch Trainingï¼‰æ—¶ï¼Œå¿…é¡»æŠŠé•¿çŸ­ä¸ä¸€çš„å¥å­è¡¥é½æˆåŒæ ·çš„é•¿åº¦ï¼ˆçŸ©é˜µè¦æ±‚ï¼‰ï¼Œè¿™å°±éœ€è¦ç”¨åˆ° pad_tokenã€‚\n",
    "# ç°åœ¨ï¼Œå¾ˆå¤šç°ä»£å¤§æ¨¡å‹ï¼ˆå¦‚ Llama 2, Llama 3, Qwen ç­‰ï¼‰ä¸ºäº†èŠ‚çœè¯è¡¨ç©ºé—´ï¼Œé»˜è®¤æ²¡æœ‰å®šä¹‰ pad_tokenã€‚\n",
    "# å¦‚æœä¸æ‰‹åŠ¨æŒ‡å®šï¼Œè®­ç»ƒæ—¶ä¼šæŠ¥é”™ã€‚å°† eos_tokenï¼ˆEnd of Sentenceï¼‰ä½œä¸ºå¡«å……ç¬¦æ˜¯ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ (è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæ²¡æœ‰ç”¨é‡åŒ–ï¼Œ0.5B å¾ˆå°ï¼Œæ˜¾å­˜å¤Ÿç”¨)\n",
    "# MODEL_IDï¼ŒæŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„æˆ– IDã€‚\n",
    "# device_map=\"auto\"ï¼Œæ™ºèƒ½æ˜¾å­˜åˆ†é…ã€‚\n",
    "#   è¿™ä¸ªå‚æ•°ä¼šè®© accelerate åº“è‡ªåŠ¨è®¡ç®—å¦‚ä½•åŠ è½½æ¨¡å‹ã€‚\n",
    "#   å¦‚æœæ˜¯ä¸€å¼ å¡ï¼Œå®ƒä¼šæŠŠæ¨¡å‹å…¨æ”¾è¿›å»ã€‚\n",
    "#   å¦‚æœæ˜¯å¤šå¼ å¡ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ‡åˆ†æ¨¡å‹å±‚ï¼Œå‡åŒ€åˆ†é…åˆ°ä¸åŒ GPU ä¸Šã€‚\n",
    "#   å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå®ƒç”šè‡³ä¼šå°†éƒ¨åˆ†å±‚å¸è½½ï¼ˆOffloadï¼‰åˆ° CPU å†…å­˜ç”šè‡³ç¡¬ç›˜ä¸Šã€‚\n",
    "# torch_dtype=torch.float16ï¼Œç²¾åº¦è®¾ç½®ã€‚\n",
    "#   é»˜è®¤æƒ…å†µï¼Œæ¨¡å‹æƒé‡é€šå¸¸æ˜¯ FP32ï¼ˆ32ä½æµ®ç‚¹æ•°ï¼‰ï¼Œå ç”¨æ˜¾å­˜å¤§ã€‚\n",
    "#   torch.float16 (FP16)ï¼ŒåŠç²¾åº¦æµ®ç‚¹æ•°ã€‚è¿™å¯ä»¥å°†æ˜¾å­˜å ç”¨ç›´æ¥å‡åŠï¼Œä¸”å¯¹æ¨ç†å’Œå¾®è°ƒçš„ç²¾åº¦å½±å“å¾ˆå°ã€‚\n",
    "#   æ³¨ï¼šå¦‚æœæ˜¯æ¯”è¾ƒæ–°çš„æ˜¾å¡ï¼ˆå¦‚ RTX 30/40ç³»åˆ—ï¼ŒA100/H100ï¼‰ï¼Œè¿™é‡Œæ¨èç”¨ torch.bfloat16ï¼Œå®ƒçš„æ•°å€¼ç¨³å®šæ€§æ¯” float16 æ›´å¥½ï¼Œè®­ç»ƒä¸å®¹æ˜“å‘æ•£ã€‚\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcf0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a55be25e-c36e-41c7-a789-b7b183ddd5c8)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ec526b18c4c7a8eb67a8a14c3bc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1de6af8064d6bb3c52c2206086028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb008143501433fa1de04a87faced11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03255000e9b42f9a12bf3ae7dbd2eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5123ac4d7e264130a5c2d948c225858e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.355600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ è®­ç»ƒå®Œæˆï¼ä¿å­˜é€‚é…å™¨åˆ° ./qwen2.5_distilled_checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. è®¾ç½® LoRA (å¯é€‰ï¼Œä½†æ¨è)\n",
    "# å…¨å‚æ•°å¾®è°ƒ0.5Bä¹Ÿå¯ä»¥ï¼Œä½†LoRAæ›´å¿«æ›´ç¨³\n",
    "# LoRAä¸æ›´æ–°æ¨¡å‹åŸæœ¬çš„å‡ åäº¿å‚æ•°ï¼Œè€Œæ˜¯ç»™æ¨¡å‹ä¸­æŒ‡å®šçš„å±‚æ’å…¥ä¸¤ä¸ªä½ç§©å°çŸ©é˜µæ¥è®­ç»ƒã€‚è¿™æ ·æ˜¾å­˜å ç”¨æä½ï¼Œä¸”æ•ˆæœé€¼è¿‘å…¨é‡å¾®è°ƒã€‚\n",
    "# r=8ï¼Œç§© (Rank)ã€‚\n",
    "#   è¿™æ˜¯ LoRA ä¸­æœ€é‡è¦çš„å‚æ•°ã€‚å®ƒå†³å®šäº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„ä¿¡æ¯è¶Šå¤šï¼Œä½†æ˜¾å­˜å ç”¨å’Œè®¡ç®—é‡ä¹Ÿè¶Šå¤§ã€‚\n",
    "#   é€šå¸¸è®¾ä¸º 8, 16, 32 æˆ– 64ã€‚å¯¹äºä¸€èˆ¬å¾®è°ƒï¼Œ8 æˆ– 16 æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©ã€‚\n",
    "# lora_alpha=16ï¼Œç¼©æ”¾ç³»æ•° (Scaling Factor)ã€‚\n",
    "#   ç±»ä¼¼äºå­¦ä¹ ç‡æ”¾å¤§å™¨ã€‚LoRA çš„æƒé‡æ›´æ–°ä¼šè¢«ä¹˜ä»¥ alpha/rã€‚\n",
    "#   é€šå¸¸è®¾ç½®ä¸º r çš„ 2å€ï¼ˆå³ alpha=2rï¼‰æ˜¯ç»éªŒä¸Šçš„æœ€ä½³å®è·µã€‚\n",
    "# target_modules=[...]ï¼Œç›®æ ‡æ¨¡å—ã€‚æŒ‡å®šè¦å¯¹æ¨¡å‹çš„å“ªäº›å±‚åº”ç”¨ LoRAã€‚\n",
    "#   åˆ—è¡¨ä¸­çš„åå­—ï¼ˆå¦‚ q_projï¼‰å¯¹åº” Transformer å†…éƒ¨çš„çº¿æ€§å±‚åç§°ã€‚\n",
    "#   ç‰¹åˆ«è¯´æ˜ï¼Œå¯¹äº Qwenã€Llama ç­‰æ¨¡å‹ï¼Œå®˜æ–¹å»ºè®®å¯¹æ‰€æœ‰çº¿æ€§å±‚ï¼ˆAttention å±‚çš„ Q,K,V,O å’Œ MLP å±‚çš„ Gate,Up,Downï¼‰éƒ½è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœè¿œå¥½äºåªå¾®è°ƒ Attention å±‚ã€‚\n",
    "# lora_dropout=0.05ï¼ŒDropout ç‡ã€‚\n",
    "#   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒ 5% çš„ç¥ç»å…ƒè¿æ¥ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆæ­»è®°ç¡¬èƒŒè®­ç»ƒæ•°æ®ï¼‰ã€‚\n",
    "# bias=\"none\"ï¼Œåç½®é¡¹è®¾ç½®ã€‚\n",
    "#   \"none\" è¡¨ç¤ºä¸è®­ç»ƒåŸæ¨¡å‹ä¸­çš„ Bias å‚æ•°ã€‚è¿™æ˜¯ä¸ºäº†æœ€å¤§ç¨‹åº¦èŠ‚çœæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥é€‰ \"all\" æˆ– \"lora_only\"ï¼Œä½†é€šå¸¸ \"none\" å°±å¤Ÿäº†ã€‚\n",
    "# task_type=\"CAUSAL_LM\"ï¼Œä»»åŠ¡ç±»å‹ã€‚\n",
    "#   å‘Šè¯‰ LoRA æˆ‘ä»¬åœ¨åšä¸€ä¸ªæ ‡å‡†çš„â€œå› æœè¯­è¨€æ¨¡å‹â€ä»»åŠ¡ï¼ˆå³ GPT å¼çš„æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen å…¨çº¿æ€§å±‚å¾®è°ƒæ•ˆæœæœ€å¥½\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4. æ ¼å¼åŒ–å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œä¼šè¢« SFTTrainer è‡ªåŠ¨è°ƒç”¨ã€‚\n",
    "# å®ƒçš„ç›®çš„æ˜¯æŠŠæ•°æ®é›†ä¸­çš„åŸå§‹åˆ—ï¼ˆinstruction, thinking, outputï¼‰è½¬æ¢æˆæ¨¡å‹èƒ½çœ‹æ‡‚çš„ä¸€ä¸ªé•¿å­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "# æ€ç»´é“¾èåˆï¼Œä»£ç å°† thinkingï¼ˆæ€ç»´é“¾/CoTï¼‰å’Œ outputï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰æ‹¼åœ¨äº†ä¸€èµ·ã€‚\n",
    "# è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨æ•™æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰å…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚è¿™æ˜¯ç›®å‰ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒè®­ç»ƒæ–¹å¼ã€‚\n",
    "\n",
    "# example å‚æ•°ï¼šè¿™æ˜¯ä¼ å…¥çš„ä¸€æ‰¹æ•°æ®ï¼ˆBatchï¼‰ï¼ŒåŒ…å«å¤šæ¡æ ·æœ¬ã€‚\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        # æ‹¼æ¥ \"æ€è€ƒè¿‡ç¨‹\" å’Œ \"æœ€ç»ˆç­”æ¡ˆ\"\n",
    "        output = \"[æ€è€ƒè¿‡ç¨‹]\\n\"+example[\"thinking\"][i]+\"\\n\\n[æœ€ç»ˆç­”æ¡ˆ]\\n\"+example[\"output\"][i] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "        # æ„å»º Qwen æ ¼å¼æ–‡æœ¬\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": example['instruction'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": output} # è¿™é‡Œçš„ output åŒ…å«æ€ç»´é“¾\n",
    "        ]\n",
    "        # åº”ç”¨åˆ†è¯å™¨çš„èŠå¤©æ¨¡æ¿\n",
    "        # æ ¹æ® Qwen æ¨¡å‹çš„è¦æ±‚ï¼Œè‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰ã€‚\n",
    "        #   ä¾‹å¦‚ï¼šå®ƒä¼šæŠŠ messages å˜æˆç±»ä¼¼è¿™æ ·çš„å­—ç¬¦ä¸²ï¼š\n",
    "        #   <|im_start|>system\\nYou are...<|im_end|><|im_start|>user\\né—®é¢˜...<|im_end|><|im_start|>assistant\\n[æ€è€ƒè¿‡ç¨‹]...\n",
    "        #   tokenize=Falseï¼šéå¸¸å…³é”®ã€‚æˆ‘ä»¬è¿™é‡Œåªç”Ÿæˆå­—ç¬¦ä¸²ï¼Œä¸è½¬æˆæ•°å­— IDã€‚å› ä¸º SFTTrainer å†…éƒ¨ä¼šç»Ÿä¸€åš Tokenizationã€‚\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# 5. å…³é”®ï¼Œè¿™æ˜¯æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ä¸­æœ€é‡è¦çš„ä¸€ä¸ªç»„ä»¶ã€‚å®ƒçš„ä½œç”¨æ˜¯Maskï¼ˆæ©ç›–ï¼‰æ‰ç”¨æˆ·æé—®éƒ¨åˆ†çš„ Lossã€‚\n",
    "# è¿™ä¸ªç¥å™¨ä¼šè‡ªåŠ¨å¸®ä½ æŠŠ User çš„éƒ¨åˆ† mask æ‰ (è®¾ä¸º -100)ï¼Œåªè®¡ç®— Assistant å›å¤çš„ Loss\n",
    "\n",
    "# ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ\n",
    "#   å¦‚æœä½ ä¸åŠ è¿™ä¸ªï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåŒæ—¶è®¡ç®—â€œç”¨æˆ·é—®é¢˜â€å’Œâ€œAIå›ç­”â€çš„æŸå¤±ã€‚\n",
    "#   è¿™ä¼šå¯¼è‡´æ¨¡å‹é€šè¿‡èƒŒè¯µç”¨æˆ·çš„é—®é¢˜æ¥é™ä½ Lossï¼Œè€Œä¸æ˜¯å­¦ä¹ å¦‚ä½•å›ç­”ã€‚\n",
    "#   ä½¿ç”¨äº†è¿™ä¸ª Collator åï¼Œæ¨¡å‹åªä¼šè¢«æƒ©ç½šå›ç­”å¾—ä¸å¯¹çš„åœ°æ–¹ï¼Œè€Œä¸ä¼šè¢«æƒ©ç½šå¤è¿°é—®é¢˜çš„åœ°æ–¹ã€‚\n",
    "\n",
    "# å‚æ•°è¯¦ç»†è¯´æ˜ï¼š\n",
    "#   response_templateï¼Œåˆ†éš”ç¬¦ã€‚å‘Šè¯‰ Collator ä»å“ªé‡Œå¼€å§‹æ˜¯â€œAI çš„å›ç­”â€ã€‚\n",
    "#   <|im_start|>assistant\\nï¼šè¿™æ˜¯ Qwen æ¨¡å‹ç‰¹æœ‰çš„ç‰¹æ®Š Token åºåˆ—ï¼Œæ ‡å¿—ç€ Assistant å¼€å§‹è¯´è¯ã€‚\n",
    "#   Collator ä¼šåœ¨æ•´æ®µæ–‡æœ¬ä¸­å¯»æ‰¾è¿™ä¸ªå­—ç¬¦ä¸²ï¼ŒæŠŠè¿™ä¸ªå­—ç¬¦ä¸²ä¹‹å‰çš„æ‰€æœ‰ Tokenï¼ˆSystem Prompt + User Promptï¼‰çš„ Label è®¾ä¸º -100ï¼ˆPyTorch ä¸­å¿½ç•¥è®¡ç®— Loss çš„æ ‡è®°ï¼‰ã€‚\n",
    "# tokenizer=tokenizerï¼ŒCollator éœ€è¦åˆ†è¯å™¨æ¥æŠŠ response_template å­—ç¬¦ä¸²è½¬æˆ Token IDï¼Œä»¥ä¾¿åœ¨è½¬æ¢åçš„æ•°æ®ä¸­è¿›è¡ŒåŒ¹é…æŸ¥æ‰¾ã€‚\n",
    "response_template = \"<|im_start|>assistant\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# 6. é…ç½®è®­ç»ƒå‚æ•°\n",
    "# ä½œç”¨ï¼ŒTrainingArguments æ˜¯ Hugging Face transformers åº“ä¸­ç”¨äºå®šä¹‰è®­ç»ƒè¿‡ç¨‹æ‰€æœ‰è¶…å‚æ•°ï¼ˆHyperparametersï¼‰çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒè´Ÿè´£å‘Šè¯‰è®­ç»ƒå™¨ï¼ˆTrainerï¼‰â€œè¯¥æ€ä¹ˆç»ƒâ€â€”â€”æ¯”å¦‚ç»ƒå¤šå¿«ï¼ˆå­¦ä¹ ç‡ï¼‰ã€ç»ƒå¤šå°‘æ¬¡ï¼ˆEpochï¼‰ã€å å¤šå°‘æ˜¾å­˜ï¼ˆBatch Sizeï¼‰ã€å¾€å“ªé‡Œå­˜ï¼ˆOutput Dirï¼‰ç­‰ã€‚\n",
    "\n",
    "# output_dirï¼Œ æŒ‡å®šè¾“å‡ºç›®å½•ã€‚\n",
    "#   è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶éƒ½ä¼šå­˜æ”¾åœ¨è¿™é‡Œã€‚åŒ…æ‹¬ï¼š\n",
    "#       Checkpointsï¼ˆæ£€æŸ¥ç‚¹ï¼‰ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹æƒé‡çš„ä¿å­˜ã€‚\n",
    "#       Logsï¼ˆæ—¥å¿—ï¼‰ï¼šLoss çš„å˜åŒ–è®°å½•ç­‰ã€‚\n",
    "#       Configï¼ˆé…ç½®ï¼‰ï¼šæœ€ç»ˆæ¨¡å‹çš„é…ç½®æ–‡ä»¶ã€‚\n",
    "\n",
    "# per_device_train_batch_sizeï¼Œå•å¼ æ˜¾å¡ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚\n",
    "#   å®ƒå†³å®šäº†æ¯æ¬¡å‚æ•°æ›´æ–°å‰ï¼Œæ˜¾å¡ä¸€æ¬¡æ€§è¯»å…¥å¤šå°‘æ¡æ•°æ®è¿›è¡Œè®¡ç®—ã€‚\n",
    "#   æ•°å€¼å«ä¹‰ï¼šè¿™é‡Œè®¾ä¸º 8ï¼Œæ„å‘³ç€å¦‚æœä½ çš„æ˜¾å­˜å¤Ÿå¤§ï¼Œæ¯æ¬¡ä¼šåŒæ—¶æŠŠ 8 æ¡æ•°æ®å¡è¿›æ˜¾å¡è®¡ç®— Lossã€‚\n",
    "#   æ˜¾å­˜å½±å“ï¼šè¿™ä¸ªæ•°å€¼è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ã€‚å¦‚æœæŠ¥ OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰é”™è¯¯ï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªè¦æ”¹å°çš„å°±æ˜¯è¿™ä¸ªå‚æ•°ã€‚\n",
    "\n",
    "# gradient_accumulation_stepsï¼Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚\n",
    "#   è¿™æ˜¯å°æ˜¾å­˜è·‘å¤§ Batch Size çš„æ ¸å¿ƒæŠ€å·§ã€‚\n",
    "#   åŸç†ï¼šæ¨¡å‹ä¼šè¿›è¡Œ 4 æ¬¡å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰ï¼Œæ¯æ¬¡ç®—å®Œ Loss åä¸ç«‹å³æ›´æ–°æƒé‡ï¼Œè€Œæ˜¯æŠŠæ¢¯åº¦æ”’èµ·æ¥ã€‚ç­‰å‡‘å¤Ÿäº† 4 æ¬¡ï¼Œå†ä¸€æ¬¡æ€§æ›´æ–°æƒé‡ã€‚\n",
    "#   å®é™…/ç­‰æ•ˆ Batch Sizeï¼šTotal Batch Size=per_device_batch_sizeÃ—gradient_accumulation_stepsÃ—GPUæ•°é‡\n",
    "#   åœ¨æœ¬ä¾‹ä¸­ï¼š8Ã—4Ã—1=32\n",
    "#   è¿™æ„å‘³ç€è™½ç„¶å•æ¬¡åªè¯» 8 æ¡æ•°æ®ï¼Œä½†åœ¨æ•°å­¦ä¸Šç­‰åŒäºä¸€æ¬¡è¯»äº† 32 æ¡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\n",
    "\n",
    "# learning_rateï¼Œ å­¦ä¹ ç‡ï¼ˆOptimizer çš„æ­¥é•¿ï¼‰ã€‚\n",
    "# å†³å®šäº†æ¨¡å‹å‚æ•°æ¯æ¬¡æ›´æ–°å˜åŒ–çš„å¹…åº¦ã€‚\n",
    "# æ•°å€¼èƒŒæ™¯ï¼š2e-4ï¼ˆå³ 0.0002ï¼‰æ˜¯ LoRA å¾®è°ƒçš„ç»éªŒå€¼ã€‚\n",
    "# å¯¹æ¯”ï¼šå¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ï¼Œå­¦ä¹ ç‡é€šå¸¸å¾ˆå°ï¼ˆå¦‚ 1e-5 æˆ– 2e-5ï¼‰ã€‚ä½†å› ä¸º LoRA è®­ç»ƒå‚æ•°å¾ˆå°‘ï¼Œé€šå¸¸éœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡æ‰èƒ½æ”¶æ•›ã€‚\n",
    "\n",
    "# logging_stepsï¼Œæ—¥å¿—æ‰“å°é¢‘ç‡ã€‚\n",
    "# è®¾ç½®æ¯è®­ç»ƒå¤šå°‘æ­¥ï¼ˆStepï¼‰å°±åœ¨æ§åˆ¶å°æ‰“å°ä¸€æ¬¡å½“å‰çš„ Loss å’Œå­¦ä¹ ç‡ã€‚\n",
    "# è®¾ä¸º 1 æ˜¯å› ä¸ºæ•°æ®é›†æå°åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ï¼Œé€šå¸¸è®¾ç½®ä¸º 10 æˆ– 100ï¼Œå¦åˆ™å±å¹•ä¼šè¢«æ—¥å¿—åˆ·å±ï¼Œä¸”è½»å¾®å½±å“æ€§èƒ½ã€‚\n",
    "\n",
    "# num_train_epochs=3ï¼Œè®­ç»ƒè½®æ¬¡ï¼Œä¸€ä¸ªepochæ„å‘³ç€æ¨¡å‹æŠŠæ•´ä¸ªè®­ç»ƒé›†ä»å¤´åˆ°å°¾è·‘ä¸€é\n",
    "# è®¾ç½®ä¸º3æ˜¯å¾®è°ƒé¢†åŸŸçš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œé€šå¸¸1-3ä¸ªepochå°±èƒ½å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚å†å¤šç»ƒå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¤±å»æ³›åèƒ½åŠ›\n",
    "\n",
    "# save_strategy=\"epoch\", æ¨¡å‹ä¿å­˜çš„ç­–ç•¥ã€‚\n",
    "# å†³å®šäº†ä»€ä¹ˆæ—¶å€™ä¿å­˜ä¸€ä¸ªCheckjpoint(å­˜æ¡£)\n",
    "# epoch, è¡¨ç¤ºæ¯è·‘å®Œä¸€è½®ï¼Œå°±è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "# stepsï¼Œ æ¯éš”Næ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆé…åˆsave_stepså‚æ•°ï¼‰\n",
    "# no, è®­ç»ƒç»“é€Ÿå‰ä¸ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœï¼ˆçœç¡¬ç›˜ï¼‰\n",
    "\n",
    "# report_to=\"none\"ï¼Œå¯è§†åŒ–æŠ¥å‘Šå·¥å…·é›†æˆã€‚\n",
    "# transformeråº“æ”¯æŒè‡ªåŠ¨æŠŠè®­ç»ƒæ•°æ®ä¸Šä¼ åˆ°WandBï¼ˆWeights & Biases), TensorBoard, MLflowç­‰å¹³å°\n",
    "# noneï¼Œ è¡¨ç¤ºå…³é—­ä¸Šä¼ åŠŸèƒ½ã€‚\n",
    "# åœ¨æœ¬åœ°å¼€å‘ã€æµ‹è¯•æˆ–è€…ä¸æƒ³æ³¨å†ŒWandBè´¦å·æ—¶ï¼Œè®¾ç½®ä¸ºnoneå¯ä»¥é¿å…å¾ˆå¤šç™»å½•æŠ¥é”™å’Œç½‘ç»œè¿æ¥é—®é¢˜\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3, \n",
    "    save_strategy=\"epoch\", \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "# SFTTrainerï¼ˆSupervised Fine-Tuning Trainerï¼‰æ˜¯ trl åº“æä¾›çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒçš„ä½œç”¨æ˜¯å°†ä¹‹å‰å‡†å¤‡å¥½çš„æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ã€å¾®è°ƒç­–ç•¥å…¨éƒ¨ç»„è£…åœ¨ä¸€èµ·ï¼Œæ„å»ºä¸€ä¸ªå¯ä»¥æ‰§è¡Œè®­ç»ƒä»»åŠ¡çš„å¯¹è±¡ã€‚\n",
    "# ç›¸æ¯”äº Hugging Face åŸç”Ÿçš„ Trainerï¼ŒSFTTrainer åšäº†å¾ˆå¤šé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰çš„è‡ªåŠ¨åŒ–å·¥ä½œï¼ˆå¦‚è‡ªåŠ¨åˆ†è¯ã€è‡ªåŠ¨åº”ç”¨ LoRAã€è‡ªåŠ¨å¤„ç† Dataset æ ¼å¼ç­‰ï¼‰ã€‚\n",
    "print(\"ğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 8. ä¿å­˜ä½ çš„æˆæœ\n",
    "print(f\"ğŸ‰ è®­ç»ƒå®Œæˆï¼ä¿å­˜é€‚é…å™¨åˆ° {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051d3037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== âš”ï¸ æ•ˆæœå¯¹å†³ âš”ï¸ ===\n",
      "\n",
      "ğŸ”´ åŸå§‹æ¨¡å‹å›ç­”:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\n",
      "assistant\n",
      "ä¸‰è§’å½¢çš„é¢ç§¯å¯ä»¥é€šè¿‡å…¬å¼è®¡ç®—å¾—å‡ºã€‚å¯¹äºä¸€ä¸ªç­‰è…°ä¸‰è§’å½¢ï¼ˆå³æœ‰ä¸¤ä¸ªç›¸ç­‰çš„è¾¹ï¼‰ï¼Œå…¶é¢ç§¯å…¬å¼ä¸ºï¼š\n",
      "\n",
      "\\[ \\text{é¢ç§¯} = \\frac{1}{2} \\times \\text{åº•} \\times \\text{é«˜} \\]\n",
      "\n",
      "ç»™å®šçš„ä¸‰è§’å½¢åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œä»£å…¥ä¸Šè¿°å…¬å¼å¾—ï¼š\n",
      "\n",
      "\\[ \\text{é¢ç§¯} = \\frac{1}{2} \\times 8 \\times 3 = 4 \\times 3 = 12 \\]\n",
      "\n",
      "æ‰€ä»¥ï¼Œè¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯æ˜¯12å¹³æ–¹å•ä½ã€‚\n",
      "\n",
      "Loading LoRA adapters...\n",
      "ğŸŸ¢ è’¸é¦æ¨¡å‹å›ç­”:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\n",
      "assistant\n",
      "[æ€è€ƒè¿‡ç¨‹]\n",
      "ä¸‰è§’å½¢çš„é¢ç§¯è®¡ç®—å…¬å¼æ˜¯åº•ä¹˜ä»¥é«˜å†é™¤ä»¥2ã€‚é¦–å…ˆï¼Œç¡®å®šåº•è¾¹é•¿åº¦ä¸º8å˜ç±³ï¼Œé«˜ä¸º3å˜ç±³ã€‚ç„¶åï¼Œå°†åº•å’Œé«˜ç›¸ä¹˜å¾—åˆ°8ä¹˜ä»¥3ç­‰äº24ã€‚æœ€åï¼Œå°†ç»“æœé™¤ä»¥2ï¼Œå¾—åˆ°é¢ç§¯ä¸º12å¹³æ–¹å˜ç±³ã€‚\n",
      "\n",
      "[æœ€ç»ˆç­”æ¡ˆ]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 1. å‡†å¤‡æµ‹è¯•é—®é¢˜ (ç”¨è®­ç»ƒè¿‡çš„é—®é¢˜æµ‹è¯•è¿‡æ‹Ÿåˆï¼Œç”¨æ–°é—®é¢˜æµ‹è¯•æ³›åŒ–)\n",
    "#test_question = \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\" \n",
    "test_question = \"ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\"\n",
    "# 2. å®šä¹‰æ¨ç†å‡½æ•°\n",
    "def run_inference(model, tokenizer, question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1) # ä½æ¸©ä»¥ä¾¿å¤ç°\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== âš”ï¸ æ•ˆæœå¯¹å†³ âš”ï¸ ===\\n\")\n",
    "\n",
    "# --- å›åˆ 1: åŸå§‹å‚»ç“œæ¨¡å‹ (Base Student) ---\n",
    "# é‡æ–°åŠ è½½çº¯å‡€æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "print(f\"ğŸ”´ åŸå§‹æ¨¡å‹å›ç­”:\\n{run_inference(base_model, tokenizer, test_question)}\")\n",
    "\n",
    "# --- å›åˆ 2: è’¸é¦åçš„æ¨¡å‹ (Distilled Student) ---\n",
    "# åŠ è½½åˆšæ‰è®­ç»ƒå¥½çš„ LoRA\n",
    "print(\"\\nLoading LoRA adapters...\")\n",
    "distilled_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "print(f\"ğŸŸ¢ è’¸é¦æ¨¡å‹å›ç­”:\\n{run_inference(distilled_model, tokenizer, test_question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47cf10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
