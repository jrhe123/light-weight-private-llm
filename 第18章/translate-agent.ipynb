{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b690c81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”Œ è¿æ¥ MCP æœåŠ¡å™¨ä¸­...\n",
      "ğŸŒ å‘ç° MCP æœåŠ¡å™¨æä¾›äº† 14 ä¸ªå·¥å…·: ['read_file', 'read_text_file', 'read_media_file', 'read_multiple_files', 'write_file', 'edit_file', 'create_directory', 'list_directory', 'list_directory_with_sizes', 'directory_tree', 'move_file', 'search_files', 'get_file_info', 'list_allowed_directories']\n",
      "ğŸš€ Agent å¼€å§‹å·¥ä½œ...\n",
      "ğŸ‘‰ å†³ç­–: read_text_file,{'path': '/Users/lichao/Downloads/mcp_workspace/input.md'}\n",
      "read text file context:# Agentic Design Patterns\n",
      "\n",
      "*A Hands-On Guide to Building Intelligent Systems* *1* *, * *Antonio Gull, file len:861242\n",
      "ğŸ‘‰ å†³ç­–: split_text_to_memory,{}\n",
      "âš™ï¸ [æœ¬åœ°èƒ½åŠ›] æ­£åœ¨æ‹†åˆ†æ–‡æœ¬...\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 1/1469 æ®µï¼Œ# Agentic Design Patterns\n",
      "\n",
      "*ä¸€æœ¬å®è·µæŒ‡å—ï¼šæ„å»ºæ™ºèƒ½ç³»ç»Ÿ* *1* *, * *Antonio Gulli* * * $$ ç›®å½• - æ€»å…±424é¡µ   = 1+2+1+1+4\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 1 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 2/1469 æ®µï¼Œ1. ç¬¬1ç« ï¼šæç¤ºé“¾ï¼ˆä»£ç ï¼‰ï¼Œ12é¡µ [æœ€ç»ˆç‰ˆï¼Œæœ€åé˜…è¯»å®Œæˆï¼Œä»£ç æ— è¯¯]  \n",
      "2. ç¬¬2ç« ï¼šè·¯ç”±ï¼ˆä»£ç ï¼‰ï¼Œ13é¡µ [æœ€ç»ˆç‰ˆï¼Œæœ€åé˜…è¯»å®Œæˆï¼Œä»£ç æ— è¯¯]  \n",
      "3. ç¬¬3ç« ï¼šå¹¶è¡ŒåŒ–ï¼ˆä»£ç ï¼‰ï¼Œ15é¡µ [æœ€ç»ˆç‰ˆ\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 2 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 3/1469 æ®µï¼Œ8. ç¬¬8ç« ï¼šå†…å­˜ç®¡ç† (ä»£ç ), 21é¡µ [æœ€ç»ˆç‰ˆï¼Œæœ€åé˜…è¯»å®Œæˆï¼Œä»£ç é€šè¿‡]  \n",
      "9. ç¬¬9ç« ï¼šå­¦ä¹ ä¸é€‚åº” (ä»£ç ), 12é¡µ [æœ€ç»ˆç‰ˆï¼Œæœ€åé˜…è¯»å®Œæˆï¼Œä»£ç é€šè¿‡]  \n",
      "10. ç¬¬10ç« ï¼šæ¨¡å‹ä¸Šä¸‹æ–‡å\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 3 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 4/1469 æ®µï¼Œ15. ç¬¬15ç« ï¼šæ™ºèƒ½ä½“é—´é€šä¿¡ï¼ˆA2Aï¼‰ï¼ˆä»£ç ï¼‰ï¼Œ15é¡µ \\[æœ€ç»ˆï¼Œæœ€åé˜…è¯»å®Œæˆï¼Œä»£ç é€šè¿‡\\]  \n",
      "16. ç¬¬16ç« ï¼šèµ„æºæ„ŸçŸ¥ä¼˜åŒ–ï¼ˆä»£ç ï¼‰ï¼Œ15é¡µ  \n",
      "17. ç¬¬17ç« ï¼šæ¨ç†æŠ€æœ¯ï¼ˆä»£ç ï¼‰ï¼Œ24é¡µ \\[\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 4 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 5/1469 æ®µï¼Œ22. é™„å½• A: é«˜çº§æç¤ºæŠ€æœ¯, 28 é¡µ [æœ€ç»ˆç‰ˆ, æœ€åé˜…è¯»å®Œæˆ, ä»£ç æ— è¯¯]  \n",
      "23. é™„å½• B - AI ä»£ç†...: ä» GUI åˆ°ç°å®ä¸–ç•Œç¯å¢ƒ, 6 é¡µ [æœ€ç»ˆç‰ˆ, æœ€åé˜…è¯»å®Œæˆ, ä»£\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 5 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 6/1469 æ®µï¼Œ11 é¡µ  ( *ç”± Gemini ç”Ÿæˆã€‚åŒ…å«æ¨ç†æ­¥éª¤ä½œä¸ºä»£ç†ç¤ºä¾‹* ) \\[ æœ€ç»ˆç‰ˆ, lrd \\] åœ¨çº¿è´¡çŒ® - å¸¸è§é—®é¢˜è§£ç­”ï¼šä»£ç†è®¾è®¡æ¨¡å¼ **é¢„å°æœ¬** : *<https://www.ama\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 6 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 7/1469 æ®µï¼Œ1 æˆ‘æ‰€æœ‰çš„ç‰ˆç¨éƒ½å°†æèµ ç»™æ‹¯æ•‘å„¿ç«¥ä¼š  \n",
      "è‡´æˆ‘çš„å„¿å­ï¼Œå¸ƒé²è¯ºï¼Œ  \n",
      "\n",
      "ä½ åœ¨æˆ‘ä¸¤å²æ—¶ï¼Œä¸ºæˆ‘çš„ç”Ÿæ´»å¸¦æ¥äº†æ–°çš„ã€é—ªè€€çš„å…‰èŠ’ã€‚å½“æˆ‘æ¢ç´¢å°†å®šä¹‰æˆ‘ä»¬æœªæ¥çš„ç³»ç»Ÿæ—¶ï¼Œæˆ‘æœ€é¦–è¦æ€è€ƒçš„æ˜¯ä½ å°†ç»§æ‰¿çš„ä¸–ç•Œã€‚  \n",
      "\n",
      "è‡´æˆ‘çš„å„¿å­\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 7 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 8/1469 æ®µï¼Œæˆ‘è¯šæŒšåœ°æ„Ÿè°¢æ‰€æœ‰ä½¿æœ¬ä¹¦æˆä¸ºå¯èƒ½çš„ä¸ªäººå’Œå›¢é˜Ÿã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘è¦æ„Ÿè°¢Googleï¼Œæ„Ÿè°¢å…¶åšæŒä½¿å‘½ã€èµ‹èƒ½Googlersï¼ˆè°·æ­Œå‘˜å·¥ï¼‰å¹¶å°Šé‡åˆ›æ–°çš„æœºä¼šã€‚\n",
      "\n",
      "æˆ‘è¡·å¿ƒæ„Ÿè°¢CTOåŠå…¬å®¤ï¼Œæ„Ÿè°¢å…¶ç»™äºˆæˆ‘æ¢ç´¢æ–°é¢†åŸŸçš„æœºä¼šï¼Œ\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 8 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 9/1469 æ®µï¼Œæˆ‘è¡·å¿ƒæ„Ÿè°¢Will Grannisï¼Œæˆ‘ä»¬çš„å‰¯æ€»è£ï¼Œä»–å¯¹äººä»¬æ‰€ç»™äºˆçš„ä¿¡ä»»ä»¥åŠä½œä¸ºæœåŠ¡å‹é¢†å¯¼çš„å…¸èŒƒã€‚æ„Ÿè°¢John Abelï¼Œæˆ‘çš„ç»ç†ï¼Œé¼“åŠ±æˆ‘è¿½æ±‚ä¸ªäººæ´»åŠ¨ï¼Œå¹¶å§‹ç»ˆä»¥è‹±å›½å¼çš„æ´å¯ŸåŠ›ç»™äºˆæˆ‘å“è¶Šçš„æŒ‡å¯¼ã€‚æ„Ÿè°¢Ant\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 9 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 10/1469 æ®µï¼Œæˆ‘çš„èµèµä¹ŸçŒ®ç»™Marco Argentiï¼Œæ„Ÿè°¢ä»–æå‡ºå…·æœ‰æŒ‘æˆ˜æ€§å’Œæ¿€åŠ±æ€§çš„æ„¿æ™¯â€”â€”æ™ºèƒ½ä½“å¢å¼ºäººç±»åŠ³åŠ¨åŠ›ã€‚åŒæ—¶ä¹Ÿè¦æ„Ÿè°¢Jim Lanzoneå’ŒJordi Ribasï¼Œæ„Ÿè°¢ä»–ä»¬æ¨åŠ¨æœç´¢é¢†åŸŸä¸æ™ºèƒ½ä½“é¢†åŸŸä¹‹é—´å…³\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 10 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š 11/1469 æ®µï¼Œæˆ‘åŒæ ·æ„Ÿè°¢äº‘AIå›¢é˜Ÿï¼Œå°¤å…¶æ˜¯å…¶è´Ÿè´£äººè¨ä¹Œå¸ƒæ‹‰Â·è’‚ç“¦é‡Œï¼ˆSaurabh Tiwaryï¼‰ï¼Œæ¨åŠ¨AIç»„ç»‡å‘åŸåˆ™æ€§è¿›æ­¥è¿ˆè¿›ã€‚æ„Ÿè°¢æŠ€æœ¯é¢†åŸŸè´Ÿè´£äººè¨å‹’å§†Â·è¨å‹’å§†Â·æµ·å¡å°”ï¼ˆSalem Salem Haykalï¼‰ï¼Œä½œ\n",
      "ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± 11 æ®µ)\n",
      "ğŸ‘‰ å†³ç­–: get_chunk_translate_segment,{}\n",
      "ğŸ‘‰ å†³ç­–: write_file,{'content': '__USE_MEMORY_BUFFER__', 'path': '/Users/lichao/Downloads/mcp_workspace/output_zh.md'}\n",
      "âš¡ï¸ [ç³»ç»Ÿ] æ‹¦æˆªåˆ°å†™å…¥è¯·æ±‚ï¼Œæ­£åœ¨æ³¨å…¥å†…å­˜ Buffer...\n",
      "ğŸ’¬ Agent: (ChatCompletionMessage(content='', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None), '')\n",
      "âš ï¸å‡ºé”™äº†ï¼ï¼ï¼ï¼Œå¼ºåˆ¶é€€å‡ºAgent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import asyncio\n",
    "from typing import List, Dict, Any\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"NO_PROXY\"] = \"192.168.1.10,localhost,127.0.0.1\"\n",
    "\n",
    "# ================= é…ç½®éƒ¨åˆ† =================\n",
    "#API_KEY = os.environ.get(\"DASHSCOPE_API_KEY\", \"sk-\")\n",
    "#API_KEY = os.environ.get(\"GEMINI_API_KEY\", \"\")\n",
    "API_KEY=\"ollama\"\n",
    "#MODEL_URL=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "#MODEL_URL=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "MODEL_URL=\"http://192.168.1.10:11434/v1\" \n",
    "ALLOWED_PATH = \"/Users/lichao/Downloads/mcp_workspace\"\n",
    "INPUT_FILE = \"input.md\"\n",
    "OUTPUT_FILE = \"output_zh.md\"\n",
    "#MODEL_NAME = \"qwen-plus\"\n",
    "#MODEL_NAME = \"gemini-2.5-flash\"\n",
    "MODEL_NAME = \"qwen3:latest\"\n",
    "\n",
    "#client = OpenAI(api_key=DASHSCOPE_API_KEY, base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "# ================= 1. å†…å­˜ä¸Šä¸‹æ–‡ (Memory Context) =================\n",
    "class MemoryContext:\n",
    "    def __init__(self):\n",
    "        self.raw_content = \"\"\n",
    "        self.source_chunks: List[str] = []\n",
    "        self.read_cursor: int = 0\n",
    "        self.output_buffer: List[str] = []\n",
    "\n",
    "    def get_full_output(self) -> str:\n",
    "        return \"\\n\\n\".join(self.output_buffer)\n",
    "\n",
    "memory = MemoryContext()\n",
    "\n",
    "# ================= 2. æœ¬åœ°å®ç°çš„ 4 ä¸ªä¸­é—´å·¥å…· =================\n",
    "\n",
    "def local_tool_split():\n",
    "    \"\"\"[æœ¬åœ°] æ‹†åˆ†æ–‡æœ¬åˆ°å†…å­˜\"\"\"\n",
    "    print(f\"âš™ï¸ [æœ¬åœ°èƒ½åŠ›] æ­£åœ¨æ‹†åˆ†æ–‡æœ¬...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=0)\n",
    "    chunks = splitter.split_text(memory.raw_content)\n",
    "    memory.source_chunks = chunks\n",
    "    memory.read_cursor = 0\n",
    "    memory.output_buffer = []\n",
    "    #print(f\"æ‹†åˆ†å®Œæˆï¼Œå†…å­˜ä¸­å·²æœ‰ {len(chunks)} ä¸ªç‰‡æ®µã€‚\")\n",
    "    return f\"æ‹†åˆ†å®Œæˆï¼Œå†…å­˜ä¸­å·²æœ‰ {len(chunks)} ä¸ªç‰‡æ®µã€‚\"\n",
    "\n",
    "def local_tool_get_next_and_translate():\n",
    "    \"\"\"[æœ¬åœ°] å–ä¸‹ä¸€æ®µ, å¹¶è¿›è¡Œç¿»è¯‘\"\"\"\n",
    "    if memory.read_cursor >= len(memory.source_chunks) or memory.read_cursor > 10:\n",
    "        return \"EOF\"\n",
    "    chunk = memory.source_chunks[memory.read_cursor]\n",
    "    idx = memory.read_cursor\n",
    "    memory.read_cursor += 1\n",
    "    #print(f\"ğŸ“„ [æœ¬åœ°èƒ½åŠ›] å–å‡ºç¬¬ {idx+1}/{len(memory.source_chunks)} æ®µ\")\n",
    "\n",
    "    work_client = OpenAI(api_key=API_KEY, base_url=MODEL_URL)\n",
    "    resp = work_client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"ç›´è¯‘ Markdown ä¸ºä¸­æ–‡ï¼Œä¿ç•™æ ¼å¼ã€‚\"},\n",
    "            {\"role\": \"user\", \"content\": chunk}\n",
    "        ],\n",
    "        temperature=0.1 # ç¿»è¯‘è¦æ±‚ä½æ¸©åº¦\n",
    "    )\n",
    "    print(f\"ğŸ“„ [æœ¬åœ°èƒ½åŠ›] ç¿»è¯‘ï¼š {idx+1}/{len(memory.source_chunks)} æ®µï¼Œ{resp.choices[0].message.content[:100]}\")\n",
    "    return resp.choices[0].message.content\n",
    "\n",
    "def local_tool_save(content: str):\n",
    "    \"\"\"[æœ¬åœ°] ä¿å­˜ç‰‡æ®µåˆ°å†…å­˜\"\"\"\n",
    "    memory.output_buffer.append(content)\n",
    "    print(f\"ğŸ’¾ [æœ¬åœ°èƒ½åŠ›] ç»“æœå·²å­˜å…¥å†…å­˜ (å…± {len(memory.output_buffer)} æ®µ)\")\n",
    "    return \"å·²ä¿å­˜åˆ°å†…å­˜ bufferã€‚\"\n",
    "\n",
    "# å®šä¹‰è¿™ 4 ä¸ªæœ¬åœ°å·¥å…·çš„ Schema (OpenAI æ ¼å¼)\n",
    "LOCAL_TOOLS_SCHEMA = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"split_text_to_memory\",\n",
    "            \"description\": \"å°†åˆšæ‰è¯»å…¥åˆ°å†…å­˜ä¸­çš„é•¿æ–‡æœ¬æ‹†åˆ†å¹¶å­˜å…¥å†…å­˜é˜Ÿåˆ—ã€‚æ— å‚æ•°\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}, \"required\": []}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_chunk_translate_segment\",\n",
    "            \"description\": \"ä»å†…å­˜é˜Ÿåˆ—ä¸­å–å‡ºä¸‹ä¸€ä¸ªæ–‡æœ¬ç‰‡æ®µè¿›è¡Œç¿»è¯‘ã€‚å¦‚æœè¿”å› 'EOF' è¡¨ç¤ºå–å®Œäº†ã€‚\",\n",
    "            \"parameters\": {\"type\": \"object\", \"properties\": {}}\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ================= 3. ä¸»é€»è¾‘ =================\n",
    "\n",
    "async def run_dynamic_agent():\n",
    "    # 1. è¿æ¥ MCP æœåŠ¡å™¨\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"npx\",\n",
    "        args=[\"-y\", \"@modelcontextprotocol/server-filesystem\", ALLOWED_PATH],\n",
    "    )\n",
    "    \n",
    "    print(\"ğŸ”Œ è¿æ¥ MCP æœåŠ¡å™¨ä¸­...\")\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            \n",
    "            # ==========================================\n",
    "            # å…³é”®æ­¥éª¤ A: åŠ¨æ€è·å– MCP å·¥å…·åˆ—è¡¨\n",
    "            # ==========================================\n",
    "            mcp_list_result = await session.list_tools()\n",
    "            mcp_tools_map = {t.name: t for t in mcp_list_result.tools} # ç”¨äºåç»­æŸ¥æ‰¾\n",
    "            \n",
    "            print(f\"ğŸŒ å‘ç° MCP æœåŠ¡å™¨æä¾›äº† {len(mcp_tools_map)} ä¸ªå·¥å…·: {list(mcp_tools_map.keys())}\")\n",
    "            \n",
    "            # å°† MCP å·¥å…·è½¬æ¢ä¸º OpenAI æ ¼å¼\n",
    "            openai_mcp_tools = []\n",
    "            for tool in mcp_list_result.tools:\n",
    "                openai_mcp_tools.append({\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\n",
    "                        \"name\": tool.name,\n",
    "                        \"description\": tool.description,\n",
    "                        \"parameters\": tool.inputSchema # MCP Schema å…¼å®¹ OpenAI parameters\n",
    "                    }\n",
    "                })\n",
    "\n",
    "            # ==========================================\n",
    "            # å…³é”®æ­¥éª¤ B: åˆå¹¶å·¥å…·åˆ—è¡¨ (4ä¸ªæœ¬åœ° + Nä¸ªè¿œç¨‹)\n",
    "            # ==========================================\n",
    "            all_tools = openai_mcp_tools + LOCAL_TOOLS_SCHEMA\n",
    "\n",
    "            # ==========================================\n",
    "            # å…³é”®æ­¥éª¤ C: å¯åŠ¨ Agent å¾ªç¯\n",
    "            # ==========================================\n",
    "            client = OpenAI(api_key=API_KEY, base_url = MODEL_URL)\n",
    "\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": (\n",
    "                            \"ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„ Markdown ç¿»è¯‘ Agentã€‚**å°†è‹±æ–‡Markdonwè½¬æˆä¸­æ–‡Markdown**\"\n",
    "                            f\"ä½ çš„å·¥ä½œç›®å½•æ ¹è·¯å¾„æ˜¯ï¼š{ALLOWED_PATH}\\n\"  # <--- åŠ ä¸Šè¿™ä¸€å¥\n",
    "                            \"è¯·æ³¨æ„ï¼šè°ƒç”¨ read_file å’Œ write_file æ—¶ï¼Œ**å¿…é¡»ä½¿ç”¨å®Œæ•´çš„ç»å¯¹è·¯å¾„**ã€‚\\n\"\n",
    "                            f\"ä¾‹å¦‚ï¼šè¯»å†™æ–‡ä»¶æ—¶è¯·ä½¿ç”¨ {os.path.join(ALLOWED_PATH, INPUT_FILE)}\\n\\n\"\n",
    "                            \"è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹é¡ºåºæ‰§è¡Œï¼Œ**ç¦æ­¢è·³æ­¥**ï¼Œ**ç¦æ­¢å¹¶å‘è°ƒç”¨å·¥å…·**ï¼š\\n\\n\"\n",
    "                            \"1. ç¬¬ä¸€æ­¥ï¼šè°ƒç”¨ `read_text_file` è¯»å–æ–‡ä»¶ã€‚å½“å‰å…è®¸æ“ä½œçš„æ ¹ç›®å½•æ˜¯: {ALLOWED_PATH}\\n\"\n",
    "                            \"   ğŸ›‘ **æ‰§è¡Œå®Œè¿™ä¸€æ­¥åï¼Œå¿…é¡»åœæ­¢æ€è€ƒï¼Œç­‰å¾…å·¥å…·è¿”å›çœŸå®çš„æ–‡ä»¶å†…å®¹ã€‚**\\n\"\n",
    "                            \"   ğŸš« **ç»å¯¹ä¸è¦**åœ¨ä¸çŸ¥é“æ–‡ä»¶çœŸå®å†…å®¹çš„æƒ…å†µä¸‹ï¼Œå°±å»è°ƒç”¨ `split_text_to_memory`ï¼Œä¸è¦è‡†é€ æ–‡æœ¬å†…å®¹ï¼\\n\\n\"\n",
    "                            \"2. ç¬¬äºŒæ­¥ï¼šå½“çœ‹åˆ°å·¥å…·è¿”å›çš„æ–‡ä»¶å†…å®¹åï¼Œè°ƒç”¨ `split_text_to_memory` (æœ¬åœ°) æ‹†åˆ†å†…å®¹ã€‚\\n\"\n",
    "                            \"3. ç¬¬ä¸‰æ­¥ï¼šå¾ªç¯è°ƒç”¨ `get_chunk_translate_segment`ã€‚\\n\"\n",
    "                            \"4. ç¬¬å››æ­¥ï¼šæ”¶åˆ°**EOF**æ¶ˆæ¯åï¼Œä½¿ç”¨ `write_file` å†™å…¥æ–‡ä»¶ï¼Œcontent å‚æ•°å¡« '__USE_MEMORY_BUFFER__'ã€‚å½“å‰å…è®¸æ“ä½œçš„æ ¹ç›®å½•æ˜¯: {ALLOWED_PATH}\"\n",
    "                            \"5. ç‰¹åˆ«æ³¨æ„ï¼š**å›å¤æ—¶ä¸è¦å¸¦æ€ç»´é“¾**\"\n",
    "                    )\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": f\"è¯·è¯»å– {INPUT_FILE}ï¼Œç¿»è¯‘åä¿å­˜åˆ° {OUTPUT_FILE}\"}\n",
    "            ]\n",
    "\n",
    "            print(\"ğŸš€ Agent å¼€å§‹å·¥ä½œ...\")\n",
    "\n",
    "            should_prune_history = False\n",
    "            while True:\n",
    "                # æ€è€ƒ\n",
    "                #print(f\"send LLM message:{messages}\")\n",
    "                response = client.chat.completions.create(\n",
    "                    model=MODEL_NAME, messages=messages, tools=all_tools, tool_choice=\"auto\"\n",
    "                )\n",
    "                msg = response.choices[0].message\n",
    "                messages.append(msg)\n",
    "\n",
    "                if not msg.tool_calls:\n",
    "\n",
    "                    print(f\"ğŸ’¬ Agent: {msg, msg.content}\")\n",
    "                    print(f\"âš ï¸å‡ºé”™äº†ï¼ï¼ï¼ï¼Œå¼ºåˆ¶é€€å‡ºAgent\")\n",
    "                    break\n",
    "\n",
    "                # ã€å¼ºåˆ¶ä¸²è¡Œã€‘å¦‚æœå‘ç° Agent ä¸€æ¬¡æ€§æƒ³åšä¸¤ä»¶äº‹ï¼ˆæ¯”å¦‚æ—¢è¯»åˆæ‹†ï¼‰ï¼Œåªå…è®¸å®ƒåšç¬¬ä¸€ä»¶\n",
    "                if len(msg.tool_calls) > 1:\n",
    "                    print(f\"âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ°å¹¶å‘è°ƒç”¨ {len(msg.tool_calls)} ä¸ªå·¥å…·ï¼Œå¼ºåˆ¶æˆªæ–­ï¼Œåªæ‰§è¡Œç¬¬ä¸€ä¸ªã€‚\")\n",
    "                    # åªä¿ç•™ç¬¬ä¸€ä¸ªå·¥å…·è°ƒç”¨ï¼Œåˆ é™¤å…¶ä»–çš„ï¼Œé˜²æ­¢å®ƒç”¨å¹»è§‰å‚æ•°æ‰§è¡Œç¬¬äºŒä¸ªå·¥å…·\n",
    "                    msg.tool_calls = [msg.tool_calls[0]]\n",
    "\n",
    "                # æ‰§è¡Œ\n",
    "                for tool_call in msg.tool_calls:\n",
    "                    name = tool_call.function.name\n",
    "                    #print(f\"DEBUG - Raw Arguments: {tool_call.function.arguments!r}\")\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "                    tool_result = \"\"\n",
    "                    \n",
    "                    print(f\"ğŸ‘‰ å†³ç­–: {name},{args}\")\n",
    "\n",
    "                    # --- è·¯ç”±åˆ¤æ–­é€»è¾‘ ---\n",
    "                    \n",
    "                    # æƒ…å†µ 1: å¦‚æœæ˜¯ MCP æä¾›çš„å·¥å…· (åŠ¨æ€å‘ç°çš„)\n",
    "                    if name in mcp_tools_map:\n",
    "                        # ç‰¹æ®Šæ‹¦æˆªï¼šå¦‚æœæ˜¯å†™å…¥æ“ä½œï¼Œä¸”ä½¿ç”¨äº†å ä½ç¬¦ï¼Œåˆ™æ³¨å…¥å†…å­˜æ•°æ®\n",
    "                        if name == \"write_file\" and args.get(\"content\") == \"__USE_MEMORY_BUFFER__\":\n",
    "                            print(\"âš¡ï¸ [ç³»ç»Ÿ] æ‹¦æˆªåˆ°å†™å…¥è¯·æ±‚ï¼Œæ­£åœ¨æ³¨å…¥å†…å­˜ Buffer...\")\n",
    "                            args[\"content\"] = memory.get_full_output()\n",
    "                        \n",
    "                        # è°ƒç”¨è¿œç¨‹ MCP\n",
    "                        try:\n",
    "                            res = await session.call_tool(name, args)\n",
    "                            # ç®€åŒ–è¿”å›ç»™ LLM çš„å†…å®¹ï¼ŒèŠ‚çœ Token (ç‰¹åˆ«æ˜¯ read_file è¿”å›å¤§æ–‡æœ¬æ—¶)\n",
    "                            if name == \"read_file\" or name == \"read_text_file\":\n",
    "                                tool_result = res.content[0].text # å¿…é¡»è¿”å›çœŸå®æ–‡æœ¬ç»™ LLMï¼Œå› ä¸ºå®ƒè¦ä¼ ç»™ split\n",
    "                                if(tool_result is not None):\n",
    "                                    print(f\"read text file context:{tool_result[:100]}, file len:{len(tool_result)}\")\n",
    "                                    # 2. ã€æ ¸å¿ƒã€‘å­˜å…¥å†…å­˜ï¼Œä¸ç»™ LLM çœ‹ï¼\n",
    "                                    memory.raw_content = tool_result\n",
    "                                \n",
    "                                    # 3. æ„é€ ç»™ LLM çœ‹çš„â€œå‡â€ç»“æœ (ç®€æŠ¥)\n",
    "                                    tool_result = f\"æ–‡ä»¶è¯»å–æˆåŠŸï¼å†…å®¹å·²å­˜å…¥ç³»ç»Ÿå†…å­˜ã€‚æ€»é•¿åº¦: {len(tool_result)} å­—ç¬¦ã€‚è¯·ç»§ç»­è°ƒç”¨ split_text_to_memoryã€‚\"\n",
    "                                \n",
    "                                    #print(f\"âœ… [ç³»ç»Ÿæ‹¦æˆª] æ–‡ä»¶å†…å®¹å·²æˆªè·å¹¶å­˜å…¥å†…å­˜ï¼Œæœªæ³„éœ²ç»™ LLMã€‚\")\n",
    "                                    \n",
    "                                else:\n",
    "                                    print(f\"ğŸš«ä¸¥é‡é”™è¯¯ï¼Œæœªè¯»åˆ°ä»»ä½•å†…å®¹...\")\n",
    "                            else:\n",
    "                                tool_result = \"æ‰§è¡ŒæˆåŠŸ (MCP Tool)\"\n",
    "                                break\n",
    "                        except Exception as e:\n",
    "                            tool_result = f\"MCP Error: {e}\"\n",
    "                            break\n",
    "\n",
    "                    # æƒ…å†µ 2: å¦‚æœæ˜¯æœ¬åœ°å·¥å…·\n",
    "                    elif name == \"split_text_to_memory\":\n",
    "                        tool_result = local_tool_split()\n",
    "                        # ä¸ºäº†çœ Tokenï¼Œè¿™é‡Œå¯ä»¥æ¬ºéª— LLM è¯´æ–‡æœ¬å·²åŠ è½½ï¼Œä¸è¦æŠŠç»“æœè¿”ç»™å®ƒ\n",
    "                        tool_result = \"[å·²çœç•¥çš„å¤§æ–‡æœ¬]\" \n",
    "\n",
    "                        # æ‹†åˆ†å®Œä¹Ÿå¯ä»¥æ¸…ç†ä¸€æ¬¡\n",
    "                        should_prune_history = True\n",
    "                        status_summary = (\n",
    "                            f\"æ–‡ä»¶å·²æ‹†åˆ†ä¸º {len(memory.source_chunks)} æ®µã€‚\\n\"\n",
    "                            f\"ğŸ‘‰ è¯·å¼€å§‹å¾ªç¯ï¼šè°ƒç”¨ `get_next_chunk_from_memory` è·å–ç¬¬ {memory.read_cursor} æ®µã€‚\"\n",
    "                        )\n",
    "\n",
    "                    elif name == \"get_chunk_translate_segment\":\n",
    "                        tool_result = local_tool_get_next_and_translate()\n",
    "                        #if(tool_result is not None):\n",
    "                            #print(f\"get next chunk:{tool_result[:100]}...\")\n",
    "                        if tool_result != \"EOF\":\n",
    "                            tool_result = local_tool_save(tool_result)\n",
    "                            # æ ‡è®°ï¼šè¿™ä¸€è½®é—­ç¯ç»“æŸäº†ï¼Œè¯¥æ¸…ç†åƒåœ¾äº†\n",
    "                            should_prune_history = True\n",
    "                            \n",
    "                            # æ„é€ ä¸€ä¸ªçŠ¶æ€ç®€æŠ¥ï¼Œå‘Šè¯‰ LLM ä¸‹ä¸€æ­¥è¯¥å¹²å˜›ï¼Œé˜²æ­¢å®ƒå¤±å¿†\n",
    "                            # æˆ‘ä»¬å¯ä»¥ä» memory å¯¹è±¡é‡Œè·å–å½“å‰è¿›åº¦\n",
    "                            current = memory.read_cursor\n",
    "                            total = len(memory.source_chunks)\n",
    "                            status_summary = (\n",
    "                                f\"âœ… ç¬¬ {current} æ®µç¿»è¯‘å·²ä¿å­˜ (è¿›åº¦: {current}/{total})ã€‚\\n\"\n",
    "                                \"å†å²ä¸Šä¸‹æ–‡å·²æ¸…ç†ä»¥é‡Šæ”¾å†…å­˜ã€‚\\n\"\n",
    "                                \"ğŸ‘‰ è¯·ç«‹å³è°ƒç”¨ `get_chunk_translate_segment` ç»§ç»­ä¸‹ä¸€æ®µã€‚\"\n",
    "                            )\n",
    "                    else:\n",
    "                        tool_result = f\"Error: æœªçŸ¥å·¥å…· {name}\"\n",
    "\n",
    "                    # åé¦ˆç»“æœ\n",
    "                    messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": str(tool_result)})\n",
    "\n",
    "                    if should_prune_history:\n",
    "                        #print(f\"ğŸ§¹ [ç³»ç»Ÿ] æ£€æµ‹åˆ°é—­ç¯å®Œæˆï¼Œæ­£åœ¨æ¸…ç†ä¸Šä¸‹æ–‡ (ä¿ç•™ System Prompt)...\")\n",
    "                        \n",
    "                        # 1. ä¿ç•™ System Prompt (messages[0])\n",
    "                        system_prompt = messages[0]\n",
    "                        first_user_prompt = messages[1]\n",
    "                        \n",
    "                        # 2. æ„é€ ä¸€ä¸ªæ–°çš„ User æ¶ˆæ¯ï¼Œä½œä¸ºâ€œæ¥åŠ›æ£’â€\n",
    "                        # è¿™éå¸¸é‡è¦ï¼å› ä¸ºæ¸…ç©ºå†å²åï¼ŒLLM éœ€è¦çŸ¥é“è‡ªå·±ç°åœ¨çš„çŠ¶æ€\n",
    "                        new_user_message = {\n",
    "                            \"role\": \"user\", \n",
    "                            \"content\": f\"ç³»ç»Ÿé€šçŸ¥ï¼š{status_summary}\"\n",
    "                        }\n",
    "                        \n",
    "                        # 3. é‡ç½® messages åˆ—è¡¨\n",
    "                        messages = [system_prompt, first_user_prompt, new_user_message]\n",
    "                        \n",
    "                        # è¿™æ ·ï¼Œä¸‹ä¸€æ¬¡å¾ªç¯æ—¶ï¼Œå‘é€ç»™ LLM çš„åªæœ‰ 2 æ¡æ¶ˆæ¯ï¼Œ\n",
    "                        # Token æ¶ˆè€—ç¬é—´å›åˆ°æœ€ä½æ°´å¹³ï¼\n",
    "                        should_prune_history = False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Jupyter ç¯å¢ƒå…¼å®¹\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        await run_dynamic_agent()\n",
    "    else:\n",
    "        asyncio.run(run_dynamic_agent())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
