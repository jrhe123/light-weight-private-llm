{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b31544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä»æœ¬åœ°åŠ è½½è€å¸ˆæ¨¡å‹: Qwen/Qwen3-4B-Instruct-2507\n",
      "æ­£åœ¨ä»æœ¬åœ°åŠ è½½å­¦ç”Ÿæ¨¡å‹: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6c9b9b683f472c9968923853250d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--Qwen--Qwen3-4B-Instruct-2507. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b771ef988fdf45afb0a4b660c63de4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2695692eb2774a6ea4ecdb2fef6f9d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfde859835124088b93649102d4bfbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> éªŒè¯é€šè¿‡ï¼šè€å¸ˆå’Œå­¦ç”Ÿè¯è¡¨ä¸€è‡´ã€‚\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ================= ä¿®æ”¹é…ç½® =================\n",
    "#TEACHER_MODEL_PATH = os.path.join(BASE_DIR, \"teacher_7b\")\n",
    "#STUDENT_MODEL_PATH = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "#TEACHER_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "TEACHER_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "STUDENT_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"æ­£åœ¨ä»æœ¬åœ°åŠ è½½è€å¸ˆæ¨¡å‹: {TEACHER_MODEL_PATH}\")\n",
    "print(f\"æ­£åœ¨ä»æœ¬åœ°åŠ è½½å­¦ç”Ÿæ¨¡å‹: {STUDENT_MODEL_PATH}\")\n",
    "\n",
    "# Tokenizer åŠ è½½ (ç›´æ¥è¯»æœ¬åœ°æ–‡ä»¶å¤¹)\n",
    "tokenizer_s = AutoTokenizer.from_pretrained(STUDENT_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer_t = AutoTokenizer.from_pretrained(TEACHER_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# ç¡®ä¿ pad_token å­˜åœ¨ (Qwen æœ‰æ—¶é»˜è®¤ pad_token ä¸º None)\n",
    "if tokenizer_s.pad_token is None: tokenizer_s.pad_token = tokenizer_s.eos_token\n",
    "if tokenizer_t.pad_token is None: tokenizer_t.pad_token = tokenizer_t.eos_token\n",
    "\n",
    "# æ ¸å¿ƒæ£€æŸ¥ï¼šç™½ç›’è’¸é¦è¦æ±‚è€å¸ˆå’Œå­¦ç”Ÿçš„ Logits å¯¹åº”åŒä¸€ä¸ªè¯è¡¨\n",
    "if tokenizer_s.vocab_size != tokenizer_t.vocab_size:\n",
    "    print(f\"[é”™è¯¯] è¯è¡¨ä¸åŒ¹é…ï¼å­¦ç”Ÿ: {tokenizer_s.vocab_size}, è€å¸ˆ: {tokenizer_t.vocab_size}\")\n",
    "    print(\"Logits æ— æ³•å¯¹é½ï¼Œæ— æ³•è¿›è¡Œç™½ç›’è’¸é¦ã€‚ç¨‹åºé€€å‡ºã€‚\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\">>> éªŒè¯é€šè¿‡ï¼šè€å¸ˆå’Œå­¦ç”Ÿè¯è¡¨ä¸€è‡´ã€‚\")\n",
    "    tokenizer = tokenizer_s # åç»­ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ª tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a914de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡ä¸‹è½½æ¨¡å‹åˆ°: ./local_models\n",
      "æ­£åœ¨ä¸‹è½½è€å¸ˆæ¨¡å‹: Qwen/Qwen3-4B-Instruct-2507 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff0bec11e22439c9ec5cd32ccdfc072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa7724f3b5d449d939e1e06eb312e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a1f26a86f524fdbaf0ea63f143a39d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c26b6f9cc1b4de283215fc2765889e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a835de832c140e69783dd3b3d7c254b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f05e6018fcc4300b70aa08a60fb9227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a002efa5f49548fd917809245ed66055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de2b4a81d724fd98896a78b8b84d80c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f10ef724b22431cbf614320f6297ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e102a141a9a642d88bccb0a30b3a7d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è€å¸ˆæ¨¡å‹ä¸‹è½½å®Œæˆï¼\n",
      "\n",
      "æ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ªï¼\n",
      "è€å¸ˆæ¨¡å‹è·¯å¾„: d:\\llm\\local_models\\teacher_3_4b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ================= é…ç½®ä¸‹è½½è·¯å¾„ =================\n",
    "# å»ºè®®åœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ª models æ–‡ä»¶å¤¹ï¼Œæ–¹ä¾¿ç®¡ç†\n",
    "BASE_DIR = \"./local_models\"\n",
    "\n",
    "# 1. è€å¸ˆæ¨¡å‹ ID (å‡è®¾æ˜¯ GPTQ é‡åŒ–ç‰ˆ)\n",
    "TEACHER_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "#TEACHER_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# è€å¸ˆæ¨¡å‹æœ¬åœ°å­˜å‚¨è·¯å¾„\n",
    "TEACHER_DIR = os.path.join(BASE_DIR, \"teacher_3_4b\")\n",
    "\n",
    "# 2. å­¦ç”Ÿæ¨¡å‹ ID\n",
    "STUDENT_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# å­¦ç”Ÿæ¨¡å‹æœ¬åœ°å­˜å‚¨è·¯å¾„\n",
    "STUDENT_DIR = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "\n",
    "# ================= å¼€å§‹ä¸‹è½½ =================\n",
    "print(f\"å‡†å¤‡ä¸‹è½½æ¨¡å‹åˆ°: {BASE_DIR}\")\n",
    "\n",
    "# ä¸‹è½½è€å¸ˆæ¨¡å‹\n",
    "if not os.path.exists(TEACHER_DIR):\n",
    "    print(f\"æ­£åœ¨ä¸‹è½½è€å¸ˆæ¨¡å‹: {TEACHER_ID} ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=TEACHER_ID,\n",
    "        local_dir=TEACHER_DIR,\n",
    "        local_dir_use_symlinks=False, # è®¾ä¸º False ç¡®ä¿ä¸‹è½½çš„æ˜¯å®é™…æ–‡ä»¶è€Œä¸æ˜¯å¿«æ·æ–¹å¼\n",
    "        resume_download=True          # æ”¯æŒæ–­ç‚¹ç»­ä¼ \n",
    "    )\n",
    "    print(\"è€å¸ˆæ¨¡å‹ä¸‹è½½å®Œæˆï¼\")\n",
    "else:\n",
    "    print(f\"è€å¸ˆæ¨¡å‹ç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {TEACHER_DIR}\")\n",
    "\n",
    "# ä¸‹è½½å­¦ç”Ÿæ¨¡å‹\n",
    "if not os.path.exists(STUDENT_DIR):\n",
    "    print(f\"æ­£åœ¨ä¸‹è½½å­¦ç”Ÿæ¨¡å‹: {STUDENT_ID} ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=STUDENT_ID,\n",
    "        local_dir=STUDENT_DIR,\n",
    "        local_dir_use_symlinks=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "    print(\"å­¦ç”Ÿæ¨¡å‹ä¸‹è½½å®Œæˆï¼\")\n",
    "else:\n",
    "    print(f\"å­¦ç”Ÿæ¨¡å‹ç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {STUDENT_DIR}\")\n",
    "\n",
    "print(\"\\næ‰€æœ‰æ¨¡å‹å‡†å¤‡å°±ç»ªï¼\")\n",
    "print(f\"è€å¸ˆæ¨¡å‹è·¯å¾„: {os.path.abspath(TEACHER_DIR)}\")\n",
    "print(f\"å­¦ç”Ÿæ¨¡å‹è·¯å¾„: {os.path.abspath(STUDENT_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85e73cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> æ­£åœ¨æ ¡éªŒ Tokenizer...\n",
      ">>> éªŒè¯é€šè¿‡ï¼šé€»è¾‘è¯è¡¨å¤§å°ä¸€è‡´ (151643)ã€‚\n",
      ">>> æ­£åœ¨è¯»å–æ•°æ®: math_dataset_with_answers_new.jsonl\n",
      ">>> å¤„ç†æ•°æ®ä¸­ (Tokenizing)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9158098c69748c98e49a82e4a070d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> åŠ è½½ Teacher (Int4)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e867b7791b44f6aca375a56df7a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> åŠ è½½ Student (FP32)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21288\\2757161983.py:115: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `KDTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> å¼€å§‹è®­ç»ƒ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:96: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='381' max='381' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [381/381 2:49:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1241.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>865.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>578.632100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>543.920700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>432.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>398.923500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>396.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>378.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>363.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>389.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>364.531800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>361.167600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>301.415400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>326.928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>306.838000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>299.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>256.433400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>266.394800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>306.044400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>262.173300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>287.414400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>246.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>245.167100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>227.174000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>265.948900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>210.304100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>252.944500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>241.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>201.551200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>215.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>223.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>213.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>229.709600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>233.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>201.361100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>202.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>206.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>215.341800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> ä¿å­˜æ¨¡å‹...\n",
      "å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. é…ç½®å‚æ•°\n",
    "# ==============================================================================\n",
    "BASE_DIR = \"./local_models\"\n",
    "\n",
    "TEACHER_MODEL_PATH = os.path.join(BASE_DIR, \"teacher_7b\")\n",
    "STUDENT_MODEL_PATH = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "#TEACHER_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\" \n",
    "#STUDENT_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATA_FILE = \"math_dataset_with_answers_new.jsonl\" \n",
    "\n",
    "# è®­ç»ƒè¶…å‚æ•°\n",
    "MAX_LENGTH = 1024  \n",
    "\n",
    "# TEMPERATURE (æ¸©åº¦ T)çš„ä½œç”¨ï¼š\n",
    "# åŸå§‹æƒ…å†µï¼šè¾“å‡ºçš„ Logits å¯èƒ½æ˜¯ [çŒ«: 10, ç‹—: 2, æ±½è½¦: -5]ã€‚\n",
    "# ç»è¿‡ Softmax åï¼Œå˜æˆ [0.999, 0.001, 0.0]ã€‚è¿™æ ·â€œç‹—â€å’Œâ€œæ±½è½¦â€çš„åŒºåˆ«è¢«æŠ¹å¹³äº†\n",
    "# ä½†ç™½ç›’è’¸é¦ï¼Œå°±æ˜¯æƒ³æŠŠè€å¸ˆæ¨¡å‹ä¸­ï¼Œè¿™äº›å·®åˆ«å­¦åˆ°ã€‚\n",
    "# æ‰€ä»¥æŠŠ Logits é™¤ä»¥ä¸€ä¸ªæ¸©åº¦ï¼ˆæ¯”å¦‚ T=2.0ï¼‰ã€‚æ•°å€¼å˜æˆ [5, 1, -2.5]ã€‚Softmax åå¯èƒ½å˜æˆ [0.8, 0.15, 0.05]ã€‚\n",
    "# æ•ˆæœï¼šæ¦‚ç‡åˆ†å¸ƒå˜â€œå¹³ç¼“â€äº†ã€‚ è¿™æ ·â€œç‹—(0.15)â€å’Œâ€œæ±½è½¦(0.05)â€çš„åŒºåˆ«å°±æ˜¾ç°å‡ºæ¥äº†ï¼Œå­¦ç”Ÿå°±èƒ½å­¦åˆ°è¿™äº›ç»†èŠ‚ã€‚\n",
    "TEMPERATURE = 2.0  \n",
    " \n",
    "# ALPHA (æ¯”å¦‚ 0.5)ï¼š\n",
    "#   0.5 ä»½çš„åŠªåŠ›ï¼šå»å¯¹é½æ ‡å‡†ç­”æ¡ˆï¼ˆåšå¯¹é¢˜ï¼‰ã€‚\n",
    "#   0.5 ä»½çš„åŠªåŠ›ï¼šå»æ¨¡ä»¿è€å¸ˆçš„æ€è€ƒæ–¹å¼ï¼ˆå­¦æ°”è´¨ï¼‰ã€‚\n",
    "#  Loss Total=Î±â‹…Loss_CE+(1âˆ’Î±)â‹…Loss_KD\n",
    "ALPHA = 0.5 \n",
    "\n",
    "BATCH_SIZE = 2           \n",
    "GRAD_ACCUMULATION = 4    \n",
    "EPOCHS = 3               \n",
    "\n",
    "# ==============================================================================\n",
    "# 2. åŠ è½½ä¸æ ¡éªŒ Tokenizer\n",
    "# ==============================================================================\n",
    "print(\">>> æ­£åœ¨æ ¡éªŒ Tokenizer...\")\n",
    "# è¿™é‡Œçš„ trust_remote_code=True æ˜¯å¿…é¡»çš„ï¼Œé˜²æ­¢æŸäº›è‡ªå®šä¹‰æ¨¡å‹æŠ¥é”™\n",
    "tokenizer_s = AutoTokenizer.from_pretrained(STUDENT_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer_t = AutoTokenizer.from_pretrained(TEACHER_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "if tokenizer_s.pad_token is None: tokenizer_s.pad_token = tokenizer_s.eos_token\n",
    "if tokenizer_t.pad_token is None: tokenizer_t.pad_token = tokenizer_t.eos_token\n",
    "\n",
    "# æ ¡éªŒé€»è¾‘\n",
    "if tokenizer_s.vocab_size != tokenizer_t.vocab_size:\n",
    "    print(f\"[é”™è¯¯] è¯è¡¨ä¸åŒ¹é…ï¼å­¦ç”Ÿ: {tokenizer_s.vocab_size}, è€å¸ˆ: {tokenizer_t.vocab_size}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(f\">>> éªŒè¯é€šè¿‡ï¼šé€»è¾‘è¯è¡¨å¤§å°ä¸€è‡´ ({tokenizer_s.vocab_size})ã€‚\")\n",
    "    tokenizer = tokenizer_s \n",
    "\n",
    "# ==============================================================================\n",
    "# 3. æ•°æ®å¤„ç†\n",
    "# ==============================================================================\n",
    "print(f\">>> æ­£åœ¨è¯»å–æ•°æ®: {DATA_FILE}\")\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "\n",
    "def process_func(example):\n",
    "    # 1. æ‹¼æ¥æ–‡æœ¬\n",
    "    combined_content = f\"[æ€è€ƒè¿‡ç¨‹]\\n{example['thinking']}\\n\\n[æœ€ç»ˆç­”æ¡ˆ]\\n{example['output']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example['instruction'] + (\"\\n\" + example['input'] if example.get('input') else \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": combined_content}\n",
    "    ]\n",
    "    # 2. è½¬å­—ç¬¦ä¸²\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    # 3. è½¬æ•°å­—\n",
    "    tokenized = tokenizer(text, max_length=MAX_LENGTH, truncation=True)\n",
    "    return tokenized\n",
    "\n",
    "print(\">>> å¤„ç†æ•°æ®ä¸­ (Tokenizing)...\")\n",
    "tokenized_dataset = dataset.map(process_func, remove_columns=dataset.column_names)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Collator è®¾ç½®\n",
    "# ==============================================================================\n",
    "response_template = \"<|im_start|>assistant\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False \n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. åŠ è½½æ¨¡å‹ (æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†)\n",
    "# ==============================================================================\n",
    "print(\">>> åŠ è½½ Teacher (Int4)...\")\n",
    "# è€å¸ˆä¸è®­ç»ƒï¼Œå¯ä»¥æ˜¯ FP16 ç”šè‡³ Int4ï¼Œæ²¡é—®é¢˜\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TEACHER_MODEL_PATH,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "teacher_model.eval()\n",
    "for param in teacher_model.parameters(): param.requires_grad = False\n",
    "\n",
    "print(\">>> åŠ è½½ Student (FP32)...\")\n",
    "# ã€æ ¸å¿ƒä¿®å¤ï¼ï¼ï¼ã€‘\n",
    "# å­¦ç”Ÿæ¨¡å‹å¿…é¡»ç”¨ float32 åŠ è½½ï¼Œæˆ–è€…ä¸å†™ torch_dtype (é»˜è®¤å°±æ˜¯ float32)\n",
    "# è¿™æ · Trainer(fp16=True) æ‰èƒ½æ­£å¸¸å»ºç«‹æ··åˆç²¾åº¦çš„ Master Weights\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    STUDENT_MODEL_PATH,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float32,  # <--- æ”¹ä¸º float32ï¼Œè§£å†³ unscale æŠ¥é”™\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. è‡ªå®šä¹‰ Trainer (å¸¦ç»´åº¦å¯¹é½)\n",
    "# ==============================================================================\n",
    "class KDTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. å­¦ç”Ÿå‰å‘\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # 2. è€å¸ˆå‰å‘\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # 3. ã€ç»´åº¦å¯¹é½ã€‘è§£å†³ 152064 vs 151936 æŠ¥é”™\n",
    "        if student_logits.shape[-1] != teacher_logits.shape[-1]:\n",
    "            min_dim = min(student_logits.shape[-1], teacher_logits.shape[-1])\n",
    "            student_logits = student_logits[..., :min_dim]\n",
    "            teacher_logits = teacher_logits[..., :min_dim]\n",
    "        \n",
    "        # 4. è®¡ç®— KD Loss\n",
    "\n",
    "        # KL æ•£åº¦ï¼šè¿™æ˜¯ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼ˆå­¦ç”Ÿé¢„æµ‹çš„åˆ†å¸ƒ vs è€å¸ˆé¢„æµ‹çš„åˆ†å¸ƒï¼‰æœ‰å¤šå¤§å·®å¼‚çš„æ•°å­¦å·¥å…·ã€‚\n",
    "        # ç›®æ ‡ï¼šè®©å­¦ç”Ÿçš„æ¦‚ç‡åˆ†å¸ƒå°½å¯èƒ½è¶‹è¿‘äºè€å¸ˆçš„åˆ†å¸ƒã€‚\n",
    "        loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "        # é—®é¢˜1ï¼šä¸ºä»€ä¹ˆå­¦ç”Ÿæ˜¯ log_softmax è€Œè€å¸ˆæ˜¯ softmaxï¼Ÿ\n",
    "        # è¿™æ˜¯ PyTorch nn.KLDivLoss æ¥å£çš„ç‰¹æ®Šè¦æ±‚ï¼š\n",
    "        #   è¾“å…¥ï¼ˆInputï¼‰ï¼šè¦æ±‚æ˜¯ Log æ¦‚ç‡ï¼ˆæ‰€ä»¥å­¦ç”Ÿç”¨ log_softmaxï¼‰ã€‚\n",
    "        #   ç›®æ ‡ï¼ˆTargetï¼‰ï¼šè¦æ±‚æ˜¯æ™®é€šæ¦‚ç‡ï¼ˆæ‰€ä»¥è€å¸ˆç”¨ softmaxï¼‰ã€‚\n",
    "        \n",
    "        # é—®é¢˜2ï¼šä¸ºä»€ä¹ˆè¦ä¹˜ä»¥ (TEMPERATURE ** 2)ï¼Ÿ\n",
    "        # è¿™æ˜¯ä¸€ä¸ªæ¢¯åº¦è¡¥å¿æœºåˆ¶ã€‚å½“ä½ æŠŠ Logits é™¤ä»¥Tæ—¶ï¼Œåå‘ä¼ æ’­è®¡ç®—å‡ºæ¥çš„æ¢¯åº¦æ•°å€¼ä¼šç¼©å° 1/T^2 å€ã€‚\n",
    "        # ä¸ºäº†è®© KD Loss çš„æ¢¯åº¦å¤§å°å’Œ CE Loss åœ¨åŒä¸€ä¸ªæ•°é‡çº§ï¼Œä¸è®©å®ƒå˜å¾—å¤ªå°è€Œè¢«å¿½ç•¥ï¼Œæ‰€ä»¥äººä¸ºä¹˜å›T^2\n",
    "        loss_kd = loss_fct(\n",
    "            F.log_softmax(student_logits / TEMPERATURE, dim=-1),\n",
    "            F.softmax(teacher_logits / TEMPERATURE, dim=-1)\n",
    "        ) * (TEMPERATURE ** 2)\n",
    "        \n",
    "        # CE Lossè®©å­¦ç”Ÿå­¦ä¼šï¼šè¿™é“é¢˜ç­”æ¡ˆæ˜¯ \"12\"ã€‚\n",
    "        # KD Lossè®©å­¦ç”Ÿå­¦ä¼šï¼šä¸ºä»€ä¹ˆè€å¸ˆåœ¨ç®—å‡º \"12\" ä¹‹å‰ï¼Œå¯¹ \"10\" æˆ– \"15\" ä¹Ÿæœ‰è¿‡ä¸€ç‚¹ç‚¹æ¦‚ç‡çš„ä¾§é‡ï¼ˆå¯èƒ½æ˜¯ä¸­é—´æ­¥éª¤çš„è¿‘ä¼¼ï¼‰ã€‚\n",
    "        loss_ce = student_outputs.loss #æ™®é€šçš„äº¤å‰ç†µæŸå¤±ï¼Œå³ç­”æ¡ˆçš„æŸå¤±\n",
    "        total_loss = ALPHA * loss_ce + (1 - ALPHA) * loss_kd\n",
    "        \n",
    "        return (total_loss, student_outputs) if return_outputs else total_loss\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. è®­ç»ƒ\n",
    "# ==============================================================================\n",
    "#æ‰€è°“æ··åˆç²¾åº¦å°±æ˜¯è®¡ç®—æ—¶ç”¨é«˜ç²¾åº¦ï¼Œå­˜æ”¾åˆ°æ˜¾å­˜ä¸­çš„ç”¨ä½ç²¾åº¦\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_math_distilled\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,                # å¼€å¯æ··åˆç²¾åº¦ï¼Œæ­¤æ—¶éœ€è¦ Student æ˜¯ FP32ï¼Œ çœæ˜¾å­˜ã€æé€Ÿåº¦ã€é˜²æº¢å‡ºã€‚\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True\n",
    ")\n",
    "\n",
    "trainer = KDTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\">>> å¼€å§‹è®­ç»ƒ...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "print(\">>> ä¿å­˜æ¨¡å‹...\")\n",
    "trainer.save_model(\"./final_math_student_model\")\n",
    "print(\"å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e70a9474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åŠ è½½æ¨¡å‹: ./final_math_student_model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer you are loading from './final_math_student_model' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> å¼€å§‹æµ‹è¯• (è¯·è§‚å¯Ÿæ˜¯å¦åŒ…å« [æ€è€ƒè¿‡ç¨‹])\n",
      "\n",
      "é—®é¢˜: è®¡ç®—ï¼š24.6 Ã· 0.6\n",
      "------------------------------\n",
      "è¦è®¡ç®—24.6é™¤ä»¥0.6ï¼Œå¯ä»¥å°†è¿™ä¸ªé™¤æ³•é—®é¢˜ç®€åŒ–ä¸ºä¸¤ä¸ªæ•°ç›¸é™¤çš„é—®é¢˜ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. å°†24.6å’Œ0.6éƒ½è½¬æ¢æˆç›¸åŒçš„å°æ•°ä½æ•°æ¥æ–¹ä¾¿è®¡ç®—ã€‚\n",
      "2. è®¡ç®—24.6é™¤ä»¥0.6ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è¿›è¡Œè®¡ç®—ï¼Œä¹Ÿå¯ä»¥å…ˆå°†24.6å’Œ0.6è½¬æ¢ä¸ºæ›´ç®€å•çš„å½¢å¼ï¼Œä»¥ä¾¿äºè®¡ç®—ã€‚ä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨24.6é™¤ä»¥0.6æ¥è¿›è¡Œè®¡ç®—ã€‚\n",
      "\n",
      "\\[24.6 Ã· 0.6 = 41\\]\n",
      "\n",
      "å› æ­¤ï¼Œ24.6é™¤ä»¥0.6çš„ç»“æœæ˜¯41ã€‚\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆæ˜¯41ã€‚\n",
      "==================================================\n",
      "\n",
      "é—®é¢˜: å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\n",
      "------------------------------\n",
      "å°æ˜æœ‰çš„è‹¹æœæ•°ä¸º5ä¸ªã€‚\n",
      "\n",
      "æ ¹æ®é¢˜ç›®ä¿¡æ¯ï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„3å€å°‘2ä¸ªã€‚å¯ä»¥å°†è¿™ä¸ªå…³ç³»è¡¨è¾¾ä¸ºï¼š\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 3 * å°æ˜çš„è‹¹æœæ•° - 2 \\]\n",
      "\n",
      "ä»£å…¥å°æ˜æœ‰çš„è‹¹æœæ•°5ä¸ªè¿›è¡Œè®¡ç®—ï¼š\n",
      "\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 3 * 5 - 2 \\]\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 15 - 2 \\]\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 13 \\]\n",
      "\n",
      "å› æ­¤ï¼Œå°çº¢æœ‰13ä¸ªè‹¹æœã€‚\n",
      "==================================================\n",
      "\n",
      "é—®é¢˜: å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\n",
      "------------------------------\n",
      "è¦è§£è¿™ä¸ªæ–¹ç¨‹ \\(3x + 5 = 20\\)ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†ç­‰å¼ä¸¤è¾¹çš„å¸¸æ•°é¡¹ç§»åˆ°ä¸€è¾¹ï¼Œä½¿å¾—ç­‰å¼çš„ä¸¤è¾¹éƒ½ä¸º0ã€‚è¿™æ ·å¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€å•çš„æ–¹ç¨‹ï¼Œä¾¿äºæ±‚è§£ã€‚\n",
      "\n",
      "1. å°†ç­‰å¼ä¸¤è¾¹åŒæ—¶å‡å»5ï¼š\n",
      "\\[3x = 15\\]\n",
      "\n",
      "2. å†å°†ç­‰å¼ä¸¤è¾¹åŒæ—¶é™¤ä»¥3ï¼ˆå› ä¸ºç­‰å¼ä¸¤è¾¹éƒ½ä¹˜ä»¥ç›¸åŒçš„æ•°ï¼Œæ‰€ä»¥ç­‰å¼ä¸¤è¾¹åŒæ—¶é™¤ä»¥åŒä¸€ä¸ªæ•°ä¼šä¿æŒä¸å˜ï¼‰:\n",
      "\\[x = \\frac{15}{3}\\]\n",
      "\n",
      "3. è¿›è¡Œè®¡ç®—å¾—åˆ°ï¼š\n",
      "\\[x = 5\\]\n",
      "\n",
      "å› æ­¤ï¼Œx çš„å€¼æ˜¯ 5ã€‚\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š\\(x = 5\\)ã€‚\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ================= é…ç½® =================\n",
    "# æŒ‡å‘åˆšæ‰ä¿å­˜çš„å¾®è°ƒåçš„æ¨¡å‹ç›®å½•\n",
    "MODEL_PATH = \"./final_math_student_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"æ­£åœ¨åŠ è½½æ¨¡å‹: {MODEL_PATH} ...\")\n",
    "\n",
    "# 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "# æ³¨æ„ï¼šè¿™é‡ŒåŠ è½½çš„æ˜¯ä½ è®­ç»ƒå¥½çš„ Studentï¼Œä¸éœ€è¦å†åŠ è½½ Teacher äº†\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval() # åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼\n",
    "\n",
    "# 2. å®šä¹‰æ¨ç†å‡½æ•°\n",
    "def generate_answer(instruction, input_text=\"\"):\n",
    "    # æ„é€ ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„ Prompt æ ¼å¼\n",
    "    # æ³¨æ„ï¼šè®­ç»ƒæ—¶æˆ‘ä»¬æŠŠ input æ‹¼åœ¨äº† instruction åé¢\n",
    "    user_content = instruction + (\"\\n\" + input_text if input_text else \"\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    # åº”ç”¨èŠå¤©æ¨¡æ¿ (åªç”Ÿæˆåˆ° user ç»“æŸï¼Œåé¢è®©æ¨¡å‹ç»­å†™ assistant)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨åŠ ä¸Š <|im_start|>assistant\\n\n",
    "    )\n",
    "    \n",
    "    # è½¬ä¸ºæ•°å­—\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # å¼€å§‹ç”Ÿæˆ\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,      # å…è®¸ç”Ÿæˆçš„æœ€å¤§é•¿åº¦\n",
    "            do_sample=False,          # é‡‡æ ·æ¨¡å¼ï¼Œç¨å¾®æœ‰ç‚¹éšæœºæ€§ï¼Œæ›´åƒäººè¯´è¯\n",
    "            temperature=0.2,         # æ§åˆ¶åˆ›é€ æ€§\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # è§£ç  (åªçœ‹ç”Ÿæˆçš„ Assistant éƒ¨åˆ†)\n",
    "    # outputs[0] åŒ…å«äº†è¾“å…¥çš„ prompt + ç”Ÿæˆçš„ responseï¼Œæˆ‘ä»¬éœ€è¦åˆ‡ç‰‡\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    final_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# ================= æµ‹è¯•æ¡ˆä¾‹ =================\n",
    "print(\"\\n>>> å¼€å§‹æµ‹è¯• (è¯·è§‚å¯Ÿæ˜¯å¦åŒ…å« [æ€è€ƒè¿‡ç¨‹])\\n\")\n",
    "\n",
    "# æµ‹è¯•é¢˜ 1ï¼šè®­ç»ƒé›†ä¸­ç±»ä¼¼çš„é¢˜ç›®ï¼ˆçœ‹è¿‡æ‹Ÿåˆ/è®°å¿†èƒ½åŠ›ï¼‰\n",
    "q1 = \"è®¡ç®—ï¼š24.6 Ã· 0.6\"\n",
    "print(f\"é—®é¢˜: {q1}\")\n",
    "print(\"-\" * 30)\n",
    "ans1 = generate_answer(q1)\n",
    "print(ans1)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æµ‹è¯•é¢˜ 2ï¼šä¸€é“æ–°é¢˜ç›®ï¼ˆçœ‹æ³›åŒ–èƒ½åŠ›ï¼‰\n",
    "q2 = \"å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\"\n",
    "print(f\"\\né—®é¢˜: {q2}\")\n",
    "print(\"-\" * 30)\n",
    "ans2 = generate_answer(q2)\n",
    "print(ans2)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# æµ‹è¯•é¢˜ 3ï¼šæ›´éš¾ä¸€ç‚¹çš„é€»è¾‘\n",
    "q3 = \"å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\"\n",
    "print(f\"\\né—®é¢˜: {q3}\")\n",
    "print(\"-\" * 30)\n",
    "ans3 = generate_answer(q3)\n",
    "print(ans3)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0c6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒï¼Œå‡†å¤‡åŠ è½½ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œ PK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0b0feb24-58fa-41b1-8a90-292cb30dfe59)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model: Qwen/Qwen2.5-0.5B-Instruct ...\n",
      "Loading Distilled Model: ./final_math_student_model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> æ¨¡å‹åŠ è½½å®Œæ¯•ï¼å¼€å§‹å¯¹æ¯”æµ‹è¯•...\n",
      "\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 1: è®¡ç®—ï¼š24.6 Ã· 0.6\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è®¡ç®— \\(24.6 \\div 0.6\\)ï¼Œæˆ‘ä»¬å¯ä»¥å°†é™¤æ³•è½¬æ¢ä¸ºä¹˜æ³•ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n",
      "\n",
      "\\[24.6 \\div 0.6 = 24.6 \\times \\frac{1}{0.6}\\]\n",
      "\n",
      "ç”±äº \\(0.6 = \\frac{3}{5}\\)ï¼Œæˆ‘ä»¬æœ‰ï¼š\n",
      "\n",
      "\\[24.6 \\times \\frac{1}{\\frac{3}{5}} = 24.6 \\times \\frac{5}{3}\\]\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ†æ•°ç›¸ä¹˜ï¼š\n",
      "\n",
      "\\[= 8.2 \\times 5\\]\n",
      "\n",
      "\\[= 41\\]\n",
      "\n",
      "å› æ­¤ï¼Œ\\(24.6 \\div 0.6 = 41\\)ã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è®¡ç®—24.6é™¤ä»¥0.6ï¼Œå¯ä»¥å°†è¿™ä¸ªé™¤æ³•é—®é¢˜ç®€åŒ–ä¸ºä¸¤ä¸ªæ•°ç›¸é™¤çš„é—®é¢˜ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n",
      "\n",
      "1. å°†24.6å’Œ0.6éƒ½è½¬æ¢æˆç›¸åŒçš„å°æ•°ä½æ•°æ¥æ–¹ä¾¿è®¡ç®—ã€‚\n",
      "2. è®¡ç®—24.6é™¤ä»¥0.6ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥è¿›è¡Œè®¡ç®—ï¼Œä¹Ÿå¯ä»¥å…ˆå°†24.6å’Œ0.6è½¬æ¢ä¸ºæ›´ç®€å•çš„å½¢å¼ï¼Œä»¥ä¾¿äºè®¡ç®—ã€‚ä½†åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ç›´æ¥ä½¿ç”¨24.6é™¤ä»¥0.6æ¥è¿›è¡Œè®¡ç®—ã€‚\n",
      "\n",
      "\\[24.6 Ã· 0.6 = 41\\]\n",
      "\n",
      "å› æ­¤ï¼Œ24.6é™¤ä»¥0.6çš„ç»“æœæ˜¯41ã€‚\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆæ˜¯41ã€‚\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 2: å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "å°çº¢çš„æ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¡ç®—ï¼š\n",
      "\n",
      "å°çº¢çš„æ•°é‡ = å°æ˜çš„æ•°é‡ - (å°æ˜çš„æ•°é‡çš„ 3 å€) + 2\n",
      "\n",
      "å°†å°æ˜çš„æ•°é‡ä»£å…¥å…¬å¼ä¸­å¾—åˆ°ï¼š\n",
      "å°çº¢çš„æ•°é‡ = 5 - (5 * 3) + 2\n",
      "å°çº¢çš„æ•°é‡ = 5 - 15 + 2\n",
      "å°çº¢çš„æ•°é‡ = -10 + 2\n",
      "å°çº¢çš„æ•°é‡ = -8\n",
      "\n",
      "ä½†æ˜¯è¿™ä¸ªç»“æœæ˜¾ç„¶æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºæ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºé¢˜ç›®ä¸­çš„æ¡ä»¶æ²¡æœ‰å®Œå…¨æ»¡è¶³ï¼Œæˆ–è€…æˆ‘ä»¬å¯¹é—®é¢˜çš„ç†è§£å­˜åœ¨åå·®ã€‚\n",
      "\n",
      "å¦‚æœé—®é¢˜æ˜¯é—®çš„æ˜¯å°çº¢å®é™…æ‹¥æœ‰çš„è‹¹æœæ•°é‡ï¼Œé‚£ä¹ˆç­”æ¡ˆåº”è¯¥æ˜¯å°çº¢æ¯”å°æ˜å¤šå‡ºçš„è‹¹æœæ•°é‡åŠ ä¸Šå°æ˜åŸæœ‰çš„è‹¹æœæ•°é‡ã€‚å³ï¼š\n",
      "- å°çº¢çš„æ•°é‡ = å°æ˜çš„æ•°é‡ + (å°æ˜çš„æ•°é‡çš„ 3 å€)\n",
      "- å°çº¢çš„æ•°é‡ = 5 + (5 * 3)\n",
      "- å°çº¢çš„æ•°é‡ = 5 + 15\n",
      "- å°çº¢çš„æ•°é‡ = 20\n",
      "\n",
      "å› æ­¤ï¼Œå°çº¢å®é™…ä¸Šæœ‰ 20 ä¸ªè‹¹æœã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "å°æ˜æœ‰çš„è‹¹æœæ•°ä¸º5ä¸ªã€‚\n",
      "\n",
      "æ ¹æ®é¢˜ç›®ä¿¡æ¯ï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„3å€å°‘2ä¸ªã€‚å¯ä»¥å°†è¿™ä¸ªå…³ç³»è¡¨è¾¾ä¸ºï¼š\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 3 * å°æ˜çš„è‹¹æœæ•° - 2 \\]\n",
      "\n",
      "ä»£å…¥å°æ˜æœ‰çš„è‹¹æœæ•°5ä¸ªè¿›è¡Œè®¡ç®—ï¼š\n",
      "\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 3 * 5 - 2 \\]\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 15 - 2 \\]\n",
      "\\[ å°çº¢çš„è‹¹æœæ•° = 13 \\]\n",
      "\n",
      "å› æ­¤ï¼Œå°çº¢æœ‰13ä¸ªè‹¹æœã€‚\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 3: å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è§£è¿™ä¸ªæ–¹ç¨‹ \\(3x + 5 = 20\\)ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†ç­‰å¼ä¸¤è¾¹çš„å¸¸æ•°é¡¹ç§»åˆ°ç­‰å¼çš„å¦ä¸€è¾¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸¤è¾¹å‡å»5ï¼š\n",
      "\n",
      "\\[3x + 5 - 5 = 20 - 5\\]\n",
      "\n",
      "è¿™ç®€åŒ–ä¸ºï¼š\n",
      "\n",
      "\\[3x = 15\\]\n",
      "\n",
      "ç„¶åï¼Œä¸ºäº†æ±‚å‡º \\(x\\) çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦å°†ç­‰å¼ä¸¤è¾¹éƒ½é™¤ä»¥3ï¼š\n",
      "\n",
      "\\[\\frac{3x}{3} = \\frac{15}{3}\\]\n",
      "\n",
      "è¿™ç®€åŒ–ä¸ºï¼š\n",
      "\n",
      "\\[x = 5\\]\n",
      "\n",
      "æ‰€ä»¥ï¼Œ\\(x\\) ç­‰äº5ã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è§£è¿™ä¸ªæ–¹ç¨‹ \\(3x + 5 = 20\\)ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†ç­‰å¼ä¸¤è¾¹çš„å¸¸æ•°é¡¹ç§»åˆ°ä¸€è¾¹ï¼Œä½¿å¾—ç­‰å¼çš„ä¸¤è¾¹éƒ½ä¸º0ã€‚è¿™æ ·å¯ä»¥å¾—åˆ°ä¸€ä¸ªç®€å•çš„æ–¹ç¨‹ï¼Œä¾¿äºæ±‚è§£ã€‚\n",
      "\n",
      "1. å°†ç­‰å¼ä¸¤è¾¹åŒæ—¶å‡å»5ï¼š\n",
      "\\[3x = 15\\]\n",
      "\n",
      "2. å†å°†ç­‰å¼ä¸¤è¾¹åŒæ—¶é™¤ä»¥3ï¼ˆå› ä¸ºç­‰å¼ä¸¤è¾¹éƒ½ä¹˜ä»¥ç›¸åŒçš„æ•°ï¼Œæ‰€ä»¥ç­‰å¼ä¸¤è¾¹åŒæ—¶é™¤ä»¥åŒä¸€ä¸ªæ•°ä¼šä¿æŒä¸å˜ï¼‰:\n",
      "\\[x = \\frac{15}{3}\\]\n",
      "\n",
      "3. è¿›è¡Œè®¡ç®—å¾—åˆ°ï¼š\n",
      "\\[x = 5\\]\n",
      "\n",
      "å› æ­¤ï¼Œx çš„å€¼æ˜¯ 5ã€‚\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆæ˜¯ï¼š\\(x = 5\\)ã€‚\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 4: ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "æ‚¨å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç”±é˜¿é‡Œäº‘å¼€å‘çš„æ¨¡å‹ã€‚æˆ‘æ˜¯æ ¹æ®å¤§é‡çš„æ•°æ®è®­ç»ƒå‡ºæ¥çš„è¯­è¨€æ¨¡å‹ï¼Œå¯ä»¥å›ç­”å„ç§é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œå¸®åŠ©ç”¨æˆ·è§£å†³é—®é¢˜ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›æ”¯æŒã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "æˆ‘æ˜¯ä¸€ä¸ªAIï¼Œå¯ä»¥è¢«æè¿°ä¸ºä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯å’Œè¿›è¡Œå¯¹è¯ã€‚æˆ‘çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·æ‰¾åˆ°æ‰€éœ€çš„ä¿¡æ¯æˆ–è§£å†³é—®é¢˜ã€‚\n",
      "\n",
      "è¯·å‘Šè¯‰æˆ‘ï¼Œä½ æœ€è¿‘åœ¨åšä»€ä¹ˆï¼Ÿ\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# ================= é…ç½® =================\n",
    "# åŸå§‹åŸºåº§æ¨¡å‹ ID\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„\n",
    "DISTILLED_MODEL_PATH = \"./final_math_student_model\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\">>> æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒï¼Œå‡†å¤‡åŠ è½½ä¸¤ä¸ªæ¨¡å‹è¿›è¡Œ PK...\")\n",
    "\n",
    "# ================= 1. åŠ è½½åˆ†è¯å™¨ =================\n",
    "# ä¸¤ä¸ªæ¨¡å‹ç”¨çš„åˆ†è¯å™¨æ˜¯ä¸€æ ·çš„ï¼ŒåŠ è½½å…¶ä¸­ä¸€ä¸ªå³å¯\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "# ================= 2. åŠ è½½ åŸå§‹åŸºåº§æ¨¡å‹ (Old) =================\n",
    "print(f\"Loading Base Model: {BASE_MODEL_ID} ...\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True # åŠ é€ŸåŠ è½½ï¼Œå‡å°‘å†…å­˜å ç”¨\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "# ================= 3. åŠ è½½ è’¸é¦åæ¨¡å‹ (New) =================\n",
    "print(f\"Loading Distilled Model: {DISTILLED_MODEL_PATH} ...\")\n",
    "model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model_distilled.eval()\n",
    "\n",
    "print(\"\\n>>> æ¨¡å‹åŠ è½½å®Œæ¯•ï¼å¼€å§‹å¯¹æ¯”æµ‹è¯•...\\n\")\n",
    "\n",
    "# ================= 4. å®šä¹‰é€šç”¨çš„æ¨ç†å‡½æ•° =================\n",
    "def get_response(model, question):\n",
    "    \"\"\"\n",
    "    è¾“å…¥æ¨¡å‹å’Œé—®é¢˜ï¼Œè¿”å›å›ç­”ã€‚\n",
    "    ä½¿ç”¨è´ªå©ªæœç´¢ (do_sample=False)ï¼Œç¡®ä¿è¾“å‡ºæ¨¡å‹è®¤ä¸ºæ¦‚ç‡æœ€å¤§çš„ç­”æ¡ˆï¼Œå‡å°‘éšæœºæ€§ã€‚\n",
    "    \"\"\"\n",
    "    # æ„é€  Prompt\n",
    "    # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨æ ‡å‡†çš„ User/Assistant æ ¼å¼\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, # ã€é‡è¦ã€‘å…³é—­é‡‡æ ·ï¼Œçœ‹æ¨¡å‹çš„çœŸå®å®åŠ›ï¼Œä¸è®©å®ƒççŒœ\n",
    "            temperature=None, # å…³é—­é‡‡æ ·åï¼Œtemperature æ— æ•ˆï¼Œè®¾ä¸º None\n",
    "            top_p=None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # è§£ç \n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# ================= 5. æµ‹è¯•æ¡ˆä¾‹ä¸å¯¹æ¯”å¾ªç¯ =================\n",
    "\n",
    "questions = [\n",
    "    # 1. è®­ç»ƒé›†ç±»ä¼¼çš„é¢˜ç›®ï¼ˆçœ‹æ˜¯å¦å­¦ä¼šäº†æ ¼å¼ï¼‰\n",
    "    \"è®¡ç®—ï¼š24.6 Ã· 0.6\",\n",
    "    \n",
    "    # 2. ç®€å•çš„åº”ç”¨é¢˜ï¼ˆçœ‹é€»è¾‘æ˜¯å¦å˜å¼ºï¼‰\n",
    "    \"å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\",\n",
    "    \n",
    "    # 3. é€»è¾‘æ¨ç†é¢˜ï¼ˆçœ‹æ˜¯å¦èƒ½åˆ†æ­¥æ€è€ƒï¼‰\n",
    "    \"å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\",\n",
    "    \n",
    "    # 4. å¸¸è¯†/éæ•°å­¦é¢˜ï¼ˆçœ‹æ˜¯å¦ç¾éš¾æ€§é—å¿˜ï¼Œå³å­¦äº†æ•°å­¦å¿˜äº†æ€ä¹ˆè¯´è¯ï¼‰\n",
    "    \"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚\"\n",
    "]\n",
    "\n",
    "separator = \"=\" * 80\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\nğŸ“ æµ‹è¯•é¢˜ç›® {i+1}: {q}\")\n",
    "    print(separator)\n",
    "    \n",
    "    # --- åŸå§‹æ¨¡å‹å›ç­” ---\n",
    "    print(\"ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\")\n",
    "    try:\n",
    "        ans_base = get_response(model_base, q)\n",
    "        print(ans_base.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    print(\"-\" * 40) # åˆ†å‰²çº¿\n",
    "    \n",
    "    # --- è’¸é¦æ¨¡å‹å›ç­” ---\n",
    "    print(\"ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] å›ç­”ï¼š\")\n",
    "    try:\n",
    "        ans_distilled = get_response(model_distilled, q)\n",
    "        print(ans_distilled.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    print(separator)\n",
    "    # ç¨å¾®åœé¡¿ä¸€ä¸‹ï¼Œé˜²æ­¢åˆ·æ–°å¤ªå¿«\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad9e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...\n",
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Distilled Model...\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 1: è®¡ç®—ï¼š24.6 Ã· 0.6\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è®¡ç®— \\(24.6 \\div 0.6\\)ï¼Œæˆ‘ä»¬å¯ä»¥å°†é™¤æ³•è½¬æ¢ä¸ºä¹˜æ³•ã€‚å…·ä½“æ­¥éª¤å¦‚ä¸‹ï¼š\n",
      "\n",
      "\\[24.6 \\div 0.6 = 24.6 \\times \\frac{1}{0.6}\\]\n",
      "\n",
      "ç”±äº \\(0.6 = \\frac{3}{5}\\)ï¼Œæˆ‘ä»¬æœ‰ï¼š\n",
      "\n",
      "\\[24.6 \\times \\frac{1}{\\frac{3}{5}} = 24.6 \\times \\frac{5}{3}\\]\n",
      "\n",
      "ç°åœ¨ï¼Œæˆ‘ä»¬å°†åˆ†æ•°ç›¸ä¹˜ï¼š\n",
      "\n",
      "\\[= 8.2 \\times 5\\]\n",
      "\n",
      "\\[= 41\\]\n",
      "\n",
      "å› æ­¤ï¼Œ\\(24.6 \\div 0.6 = 41\\)ã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] (è§¦å‘æ€è€ƒæ¨¡å¼)ï¼š\n",
      "thinking[æ€è€ƒè¿‡ç¨‹]\n",
      "è¦è®¡ç®—24.6é™¤ä»¥0.6ï¼Œå¯ä»¥å°†è¿™ä¸ªé™¤æ³•é—®é¢˜è½¬åŒ–ä¸ºä¹˜æ³•é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯æ±‚è§£24.6ä¹˜ä»¥0.6çš„ç»“æœã€‚\n",
      "\n",
      "\\[24.6 Ã· 0.6 = 24.6 Ã— 0.6\\]\n",
      "\n",
      "æˆ‘ä»¬æ¥é€æ­¥è¿›è¡Œè®¡ç®—ï¼š\n",
      "\n",
      "1. é¦–å…ˆï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ç›¸ä¹˜ï¼š\n",
      "\\[24.6 Ã— 0.6 = 14.16\\]\n",
      "\n",
      "æˆ–è€…ï¼Œä¹Ÿå¯ä»¥å°†24.6å’Œ0.6åˆ†åˆ«è½¬æ¢ä¸ºåˆ†æ•°å½¢å¼ï¼Œç„¶åè¿›è¡Œä¹˜æ³•è¿ç®—ï¼š\n",
      "\n",
      "\\[24.6 = \\frac{246}{10}ï¼Œ0.6 = \\frac{6}{10}\\]\n",
      "\n",
      "å› æ­¤ï¼Œ\\( \\frac{246}{10} Ã— \\frac{6}{10} = \\frac{246Ã—6}{10Ã—10} = \\frac{1416}{100} = 14.16 \\)\n",
      "\n",
      "æ‰€ä»¥ï¼Œ24.6é™¤ä»¥0.6çš„æœ€ç»ˆç»“æœæ˜¯14.16ã€‚\n",
      "\n",
      "\\[24.6 Ã· 0.6 = 14.16\\]\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 2: å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "å°çº¢çš„æ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·è®¡ç®—ï¼š\n",
      "\n",
      "å°çº¢çš„æ•°é‡ = å°æ˜çš„æ•°é‡ - (å°æ˜çš„æ•°é‡çš„ 3 å€) + 2\n",
      "\n",
      "å°†å°æ˜çš„æ•°é‡ä»£å…¥å…¬å¼ä¸­å¾—åˆ°ï¼š\n",
      "å°çº¢çš„æ•°é‡ = 5 - (5 * 3) + 2\n",
      "å°çº¢çš„æ•°é‡ = 5 - 15 + 2\n",
      "å°çº¢çš„æ•°é‡ = -10 + 2\n",
      "å°çº¢çš„æ•°é‡ = -8\n",
      "\n",
      "ä½†æ˜¯è¿™ä¸ªç»“æœæ˜¾ç„¶æ˜¯ä¸å¯èƒ½çš„ï¼Œå› ä¸ºæ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºé¢˜ç›®ä¸­çš„æ¡ä»¶æ²¡æœ‰å®Œå…¨æ»¡è¶³ï¼Œæˆ–è€…æˆ‘ä»¬å¯¹é—®é¢˜çš„ç†è§£å­˜åœ¨åå·®ã€‚\n",
      "\n",
      "å¦‚æœé—®é¢˜æ˜¯é—®çš„æ˜¯å°çº¢å®é™…æ‹¥æœ‰çš„è‹¹æœæ•°é‡ï¼Œé‚£ä¹ˆç­”æ¡ˆåº”è¯¥æ˜¯å°çº¢æ¯”å°æ˜å¤šå‡ºçš„è‹¹æœæ•°é‡åŠ ä¸Šå°æ˜åŸæœ‰çš„è‹¹æœæ•°é‡ã€‚å³ï¼š\n",
      "- å°çº¢çš„æ•°é‡ = å°æ˜çš„æ•°é‡ + (å°æ˜çš„æ•°é‡çš„ 3 å€)\n",
      "- å°çº¢çš„æ•°é‡ = 5 + (5 * 3)\n",
      "- å°çº¢çš„æ•°é‡ = 5 + 15\n",
      "- å°çº¢çš„æ•°é‡ = 20\n",
      "\n",
      "å› æ­¤ï¼Œå°çº¢å®é™…ä¸Šæœ‰ 20 ä¸ªè‹¹æœã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] (è§¦å‘æ€è€ƒæ¨¡å¼)ï¼š\n",
      "thinking[æ€è€ƒè¿‡ç¨‹]\n",
      "1. å°æ˜æœ‰çš„è‹¹æœæ•°ä¸º 5 ä¸ªã€‚\n",
      "2. å°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚\n",
      "\n",
      "æ ¹æ®é¢˜ç›®ä¿¡æ¯ï¼Œæˆ‘ä»¬å¯ä»¥å°†å°çº¢çš„è‹¹æœæ•°é‡è¡¨ç¤ºä¸º \\(5 \\times 3 - 2\\)ã€‚\n",
      "\n",
      "\\[5 \\times 3 = 15\\]  \n",
      "\\(15 - 2 = 13\\)\n",
      "\n",
      "å› æ­¤ï¼Œå°çº¢æœ‰ 13 ä¸ªè‹¹æœã€‚\n",
      "\n",
      "[æœ€ç»ˆç­”æ¡ˆ]\n",
      "å°çº¢æœ‰ 13 ä¸ªè‹¹æœã€‚\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•é¢˜ç›® 3: å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\n",
      "================================================================================\n",
      "ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\n",
      "è¦è§£è¿™ä¸ªæ–¹ç¨‹ \\(3x + 5 = 20\\)ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦å°†ç­‰å¼ä¸¤è¾¹çš„å¸¸æ•°é¡¹ç§»åˆ°ç­‰å¼çš„å¦ä¸€è¾¹ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä»ä¸¤è¾¹å‡å»5ï¼š\n",
      "\n",
      "\\[3x + 5 - 5 = 20 - 5\\]\n",
      "\n",
      "è¿™ç®€åŒ–ä¸ºï¼š\n",
      "\n",
      "\\[3x = 15\\]\n",
      "\n",
      "ç„¶åï¼Œä¸ºäº†æ±‚å‡º \\(x\\) çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦å°†ç­‰å¼ä¸¤è¾¹éƒ½é™¤ä»¥3ï¼š\n",
      "\n",
      "\\[\\frac{3x}{3} = \\frac{15}{3}\\]\n",
      "\n",
      "è¿™ç®€åŒ–ä¸ºï¼š\n",
      "\n",
      "\\[x = 5\\]\n",
      "\n",
      "æ‰€ä»¥ï¼Œ\\(x\\) ç­‰äº5ã€‚\n",
      "----------------------------------------\n",
      "ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] (è§¦å‘æ€è€ƒæ¨¡å¼)ï¼š\n",
      "thinking[æ€è€ƒè¿‡ç¨‹]\n",
      "ç»™å®šçš„æ–¹ç¨‹æ˜¯ \\(3x + 5 = 20\\). æˆ‘ä»¬éœ€è¦è§£è¿™ä¸ªæ–¹ç¨‹æ‰¾å‡ºæœªçŸ¥æ•° \\(x\\) çš„å€¼ã€‚\n",
      "\n",
      "1. é¦–å…ˆå°†ç­‰å¼ä¸¤è¾¹åŒæ—¶å‡å»5ï¼Œä»¥ä½¿æ–¹ç¨‹ç®€åŒ–ï¼š\n",
      "\\[3x = 15\\]\n",
      "\n",
      "2. æ¥ç€ï¼Œä¸ºäº†æ±‚å‡º \\(x\\) çš„å€¼ï¼Œæˆ‘ä»¬éœ€è¦å°†ç­‰å¼ä¸¤è¾¹åŒæ—¶é™¤ä»¥3ï¼ˆå› ä¸ºç­‰å¼ä¸¤è¾¹éƒ½ä¹˜ä»¥ç›¸åŒçš„æ•°ï¼Œå³3ï¼Œå¯ä»¥ä¿æŒä¸å˜ï¼‰:\n",
      "\\[x = \\frac{15}{3}\\]\n",
      "\n",
      "3. è¿›è¡Œè®¡ç®—å¾—åˆ°ï¼š\n",
      "\\[x = 5\\]\n",
      "\n",
      "å› æ­¤ï¼Œ\\(x = 5\\)ã€‚\n",
      "\n",
      "æœ€ç»ˆç­”æ¡ˆæ˜¯ \\(x = 5\\)ã€‚\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# ================= é…ç½® =================\n",
    "BASE_MODEL_ID = \"local_models/student_0.5b/\"\n",
    "DISTILLED_MODEL_PATH = \"./final_math_student_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\">>> æ­£åœ¨åˆå§‹åŒ–ç¯å¢ƒ...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "print(f\"Loading Base Model...\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID, device_map=device, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "print(f\"Loading Distilled Model...\")\n",
    "model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH, device_map=device, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")\n",
    "model_distilled.eval()\n",
    "\n",
    "# ================= æ”¹è¿›åçš„æ¨ç†å‡½æ•° =================\n",
    "def get_response(model, question, force_thinking=False):\n",
    "    \"\"\"\n",
    "    force_thinking: å¦‚æœä¸º Trueï¼Œå¼ºåˆ¶ç»™æ¨¡å‹â€œå–‚â€ä¸€ä¸ªå¼€å¤´ï¼Œé€¼å®ƒè¿›å…¥æ€è€ƒæ¨¡å¼\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # 1. ç”ŸæˆåŸºç¡€ Prompt\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 2. ã€æ ¸å¿ƒæŠ€å·§ã€‘Prompt æ³¨å…¥\n",
    "    # å¦‚æœæ˜¯è’¸é¦æ¨¡å‹ï¼Œæˆ‘ä»¬è¦æ£€æŸ¥å®ƒæ˜¯å¦åŒ…å«æ ‡ç­¾ã€‚\n",
    "    # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨æŠŠ [æ€è€ƒè¿‡ç¨‹] æ‹¼åœ¨ Assistant çš„å¼€å¤´\n",
    "    if force_thinking:\n",
    "        text += \"[æ€è€ƒè¿‡ç¨‹]\\n\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, \n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # è§£ç \n",
    "    # æ³¨æ„ï¼šå¦‚æœä½¿ç”¨äº†æ³¨å…¥ï¼Œç”Ÿæˆçš„ tokens é‡Œä¸åŒ…å«æ³¨å…¥çš„é‚£éƒ¨åˆ†ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æ‹¼å›å»æ˜¾ç¤º\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    if force_thinking:\n",
    "        \n",
    "        return \"thinking[æ€è€ƒè¿‡ç¨‹]\\n\" + response\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# ================= å¯¹æ¯”æµ‹è¯• =================\n",
    "\n",
    "questions = [\n",
    "    \"è®¡ç®—ï¼š24.6 Ã· 0.6\",\n",
    "    \"å°æ˜æœ‰ 5 ä¸ªè‹¹æœï¼Œå°çº¢çš„è‹¹æœæ•°é‡æ˜¯å°æ˜çš„ 3 å€å°‘ 2 ä¸ªã€‚è¯·é—®å°çº¢æœ‰å¤šå°‘ä¸ªè‹¹æœï¼Ÿ\",\n",
    "    \"å¦‚æœ 3x + 5 = 20ï¼Œé‚£ä¹ˆ x ç­‰äºå¤šå°‘ï¼Ÿ\"\n",
    "]\n",
    "\n",
    "separator = \"=\" * 80\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\nğŸ“ æµ‹è¯•é¢˜ç›® {i+1}: {q}\")\n",
    "    print(separator)\n",
    "    \n",
    "    # --- åŸå§‹æ¨¡å‹ (ä¸å¼ºåˆ¶) ---\n",
    "    print(\"ğŸ”´ [åŸå§‹ 0.5B æ¨¡å‹] å›ç­”ï¼š\")\n",
    "    try:\n",
    "        # åŸå§‹æ¨¡å‹æ²¡å­¦è¿‡è¿™ä¸ªæ ¼å¼ï¼Œå¼ºåˆ¶åŠ åè€Œä¼šä¹±ï¼Œæ‰€ä»¥ä¸åŠ \n",
    "        ans_base = get_response(model_base, q, force_thinking=False)\n",
    "        print(ans_base.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # --- è’¸é¦æ¨¡å‹ (å¼ºåˆ¶æ³¨å…¥æ ‡ç­¾) ---\n",
    "    print(\"ğŸŸ¢ [è’¸é¦å 0.5B æ¨¡å‹] (è§¦å‘æ€è€ƒæ¨¡å¼)ï¼š\")\n",
    "    try:\n",
    "        # æˆ‘ä»¬æ¥çœ‹çœ‹ï¼Œä¸€æ—¦å¼€äº†ä¸ªå¤´ï¼Œå®ƒèƒ½ä¸èƒ½æ¥ä¸‹å»ï¼\n",
    "        ans_distilled = get_response(model_distilled, q, force_thinking=True)\n",
    "        print(ans_distilled.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    print(separator)\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
