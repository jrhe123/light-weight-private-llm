{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6da19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "INPUT_FILE = \"math_dataset_with_answers.jsonl\"\n",
    "OUTPUT_FILE = \"incorrect_analysis.json\"\n",
    "MODEL_NAME = \"gemini-2.5-pro\" \n",
    "BATCH_SIZE = 100  # æ¯æ‰¹å¤„ç†çš„æ•°é‡\n",
    "\n",
    "#aistudio.google.com\n",
    "API_KEY=\"ä½ è‡ªå·±çš„KEY\"\n",
    "\n",
    "# ================= æ•°æ®ç»“æ„å®šä¹‰ =================\n",
    "class IncorrectItem(BaseModel):\n",
    "    id: int = Field(description=\"é¢˜ç›®å¯¹åº”çš„ ID ç¼–å·\")\n",
    "    reason: str = Field(description=\"ç®€çŸ­è§£é‡Šä¸ºä»€ä¹ˆè¿™é“é¢˜æ˜¯é”™çš„ï¼ˆä¾‹å¦‚ï¼š'è®¡ç®—é”™è¯¯ï¼Œ3+5åº”ä¸º8' æˆ– 'é€»è¾‘æ¨å¯¼æœ‰è¯¯'ï¼‰\")\n",
    "\n",
    "class ReviewResult(BaseModel):\n",
    "    incorrect_items: list[IncorrectItem]\n",
    "\n",
    "# ================= æ ¸å¿ƒåŠŸèƒ½å‡½æ•° =================\n",
    "\n",
    "def process_batch_with_retry(client, batch_data, batch_index, max_retries=3):\n",
    "    \"\"\"\n",
    "    å¤„ç†å•ä¸ªæ‰¹æ¬¡ï¼ŒåŒ…å«é‡è¯•é€»è¾‘\n",
    "    \"\"\"\n",
    "    # 1. æ„é€ å½“å‰æ‰¹æ¬¡çš„ Prompt\n",
    "    batch_content_str = \"\"\n",
    "    for item in batch_data:\n",
    "        record_str = (\n",
    "            f\"ID: {item['original_id']}\\n\"\n",
    "            f\"Question: {item['data'].get('instruction')}\\n\"\n",
    "            f\"Thinking: {item['data'].get('thinking')}\\n\"\n",
    "            f\"Answer: {item['data'].get('output')}\\n\"\n",
    "            f\"-----------------------------\\n\"\n",
    "        )\n",
    "        batch_content_str += record_str\n",
    "\n",
    "    prompt = (\n",
    "        \"ä½ æ˜¯ä¸€ä½èµ„æ·±æ•°å­¦è€å¸ˆã€‚è¯·ä»”ç»†æ£€æŸ¥ä»¥ä¸‹æ•°å­¦é¢˜çš„ã€æ¨ç†è¿‡ç¨‹ã€‘å’Œã€æœ€ç»ˆç­”æ¡ˆã€‘ã€‚\\n\"\n",
    "        \"ä»»åŠ¡è¦æ±‚ï¼š\\n\"\n",
    "        \"1. å¦‚æœé¢˜ç›®å®Œå…¨æ­£ç¡®ï¼Œå¿½ç•¥å®ƒã€‚\\n\"\n",
    "        \"2. å¦‚æœæœ‰é”™ï¼Œè¯·è®°å½•å…¶ IDï¼Œå¹¶ç”¨ç®€çŸ­çš„ä¸­æ–‡è¯´æ˜é”™è¯¯åŸå› ã€‚\\n\"\n",
    "        \"3. ä¸¥æ ¼æŒ‰ç…§å®šä¹‰çš„ JSON æ ¼å¼è¿”å›ç»“æœã€‚\\n\"\n",
    "        \"\\n\"\n",
    "        \"ä»¥ä¸‹æ˜¯æœ¬æ‰¹æ¬¡å¾…æ£€æŸ¥çš„é¢˜ç›®ï¼š\\n\"\n",
    "        f\"{batch_content_str}\"\n",
    "    )\n",
    "\n",
    "    # 2. é‡è¯•å¾ªç¯\n",
    "    wait_time = 5 # åˆå§‹ç­‰å¾… 5 ç§’\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=MODEL_NAME,\n",
    "                contents=prompt,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    response_mime_type=\"application/json\",\n",
    "                    response_schema=ReviewResult,\n",
    "                    temperature=0.1,\n",
    "                ),\n",
    "            )\n",
    "            \n",
    "            if response.parsed:\n",
    "                return response.parsed.incorrect_items\n",
    "            else:\n",
    "                print(f\"   âš ï¸ ç¬¬ {batch_index} æ‰¹è¿”å›æ— æ³•è§£æï¼Œå°è¯•é‡è¯•...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            # æ•è·é”™è¯¯ï¼ˆåŒ…æ‹¬ 503 Overloadedï¼‰\n",
    "            print(f\"   âš ï¸ ç¬¬ {batch_index} æ‰¹å‘ç”Ÿé”™è¯¯ (å°è¯• {attempt+1}/{max_retries}): {e}\")\n",
    "            if \"503\" in str(e) or \"429\" in str(e):\n",
    "                print(f\"      æœåŠ¡å™¨ç¹å¿™ï¼Œä¼‘æ¯ {wait_time} ç§’...\")\n",
    "                time.sleep(wait_time)\n",
    "                wait_time *= 2 # æŒ‡æ•°é€€é¿ï¼š5s -> 10s -> 20s\n",
    "            else:\n",
    "                # å¦‚æœæ˜¯å…¶ä»–ä¸¥é‡é”™è¯¯ï¼Œå¯èƒ½ä¸éœ€è¦é‡è¯•ï¼Œè§†æƒ…å†µè€Œå®š\n",
    "                time.sleep(2)\n",
    "\n",
    "    print(f\"   âŒ ç¬¬ {batch_index} æ‰¹å½»åº•å¤±è´¥ï¼Œè·³è¿‡æ­¤æ‰¹æ•°æ®ã€‚\")\n",
    "    return [] # è¿”å›ç©ºåˆ—è¡¨ï¼Œé¿å…ç¨‹åºä¸­æ–­\n",
    "\n",
    "# ================= ä¸»ç¨‹åº =================\n",
    "def main():\n",
    "    client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "    # 1. è¯»å–æ‰€æœ‰æ•°æ®åˆ°å†…å­˜\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"æ‰¾ä¸åˆ°æ–‡ä»¶: {INPUT_FILE}\")\n",
    "        return\n",
    "\n",
    "    print(f\"æ­£åœ¨è¯»å– {INPUT_FILE} ...\")\n",
    "    all_records = [] # å­˜å‚¨ç»“æ„ï¼š{'original_id': 1, 'data': {...}}\n",
    "\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            try:\n",
    "                # é¢„å…ˆè§£æ JSONï¼Œå¹¶ç»‘å®šåŸå§‹ ID\n",
    "                json_obj = json.loads(line)\n",
    "                all_records.append({\n",
    "                    \"original_id\": index,\n",
    "                    \"data\": json_obj\n",
    "                })\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    total_count = len(all_records)\n",
    "    print(f\"ç»„è£…å®Œæˆï¼Œå…± {total_count} æ¡æ•°æ®ã€‚\")\n",
    "    print(f\"å‡†å¤‡å¼€å§‹å¤„ç†ï¼Œæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼Œå…±éœ€å¤„ç† {(total_count + BATCH_SIZE - 1) // BATCH_SIZE} æ‰¹ã€‚\")\n",
    "\n",
    "    # 2. æ‰¹é‡å¤„ç†å¾ªç¯\n",
    "    all_incorrect_items = []\n",
    "\n",
    "    for i in range(0, total_count, BATCH_SIZE):\n",
    "        batch = all_records[i : i + BATCH_SIZE]\n",
    "        batch_num = (i // BATCH_SIZE) + 1\n",
    "        \n",
    "        print(f\"\\nğŸš€ æ­£åœ¨å¤„ç†ç¬¬ {batch_num} æ‰¹ (ID: {batch[0]['original_id']} - {batch[-1]['original_id']})...\")\n",
    "        \n",
    "        # è°ƒç”¨å¤„ç†å‡½æ•°\n",
    "        batch_errors = process_batch_with_retry(client, batch, batch_num)\n",
    "        \n",
    "        # å°†ç»“æœåŠ å…¥æ€»è¡¨\n",
    "        if batch_errors:\n",
    "            print(f\"   âœ… æœ¬æ‰¹æ¬¡å‘ç° {len(batch_errors)} ä¸ªé”™è¯¯ã€‚\")\n",
    "            all_incorrect_items.extend(batch_errors)\n",
    "        else:\n",
    "            print(\"   âœ… æœ¬æ‰¹æ¬¡æœªå‘ç°é”™è¯¯ï¼ˆæˆ–å¤„ç†å¤±è´¥ï¼‰ã€‚\")\n",
    "\n",
    "        # æ‰¹æ¬¡é—´å¼ºåˆ¶ä¼‘æ¯ï¼Œä¿æŠ¤ API é…é¢\n",
    "        time.sleep(2) \n",
    "\n",
    "    # 3. æœ€ç»ˆæ±‡æ€»ä¸ä¿å­˜\n",
    "    print(\"-\" * 50)\n",
    "    all_incorrect_items.sort(key=lambda x: x.id) # æŒ‰ ID æ’åº\n",
    "\n",
    "    if not all_incorrect_items:\n",
    "        print(\"ğŸ‰ æ£€æŸ¥å…¨éƒ¨å®Œæˆï¼Œæœªå‘ç°ä»»ä½•é”™è¯¯ï¼\")\n",
    "    else:\n",
    "        print(f\"æ£€æŸ¥å…¨éƒ¨å®Œæˆï¼å…±å‘ç° {len(all_incorrect_items)} ä¸ªé”™è¯¯ã€‚\")\n",
    "        \n",
    "        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨\n",
    "        save_data = [item.model_dump() for item in all_incorrect_items]\n",
    "\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(save_data, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"å®Œæ•´é”™è¯¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {OUTPUT_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
